{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7B8Dkg2fgwb"
   },
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "## Final report\n",
    "\n",
    "## Study of the impact on the ratio of unlabeled to labeled data on top-1 accuracy on the CIFAR-10 dataset and House Numbers dataset\n",
    "\n",
    "Pierre Andurand (pa2570)\n",
    "Tzu Yi Chuang (tc3075)\n",
    "Kuan Yu Ko (kk3376)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agw8HXRPf5ur"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GjKlhPBmc6ko",
    "outputId": "3d974040-c041-4043-d16f-1765a73722b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Resnet for CIFAR10 taken from Keras https://keras.io/examples/cifar10_resnet/\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# limit GPU memory to stop kernel crash\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "layers = tf.keras.layers\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def prepare_cifar10_data(subtract_pixel_mean = True):\n",
    "    \n",
    "    # The data, split between train and test sets:\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    # Normalize data.\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    num_classes=10\n",
    "    # If subtract pixel mean is enabled (Subtracting pixel mean improves accuracy)\n",
    "    if subtract_pixel_mean:\n",
    "        x_train_mean = np.mean(x_train, axis=0)\n",
    "        x_train -= x_train_mean\n",
    "        x_test -= x_train_mean\n",
    "        \n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    return (x_train, y_train),(x_test,y_test) \n",
    "    \n",
    "\n",
    "    \n",
    "def prepare_house_numbers_data(subtract_pixel_mean = True):\n",
    "    ## We import housenumber dataset from http://ufldl.stanford.edu/housenumbers/\n",
    "\n",
    "    #Loading dataset\n",
    "    num_classes=10\n",
    "    from scipy.io import loadmat\n",
    "\n",
    "    train = loadmat('train_32x32.mat')\n",
    "    test = loadmat('test_32x32.mat')\n",
    "\n",
    "    # train and test are python dictionaries\n",
    "    # keys are ['__header__', '__version__', '__globals__', 'X', 'y']\n",
    "\n",
    "    x_train = train['X']\n",
    "    y_train = train['y']\n",
    "    x_test = test['X']\n",
    "    y_test = test['y']\n",
    "\n",
    "    # x and y are numpy ndarrays\n",
    "\n",
    "    print(\"Shape of x_train is:\", x_train.shape)\n",
    "    print(\"Shape of y_train is:\", y_train.shape)\n",
    "    print(\"Shape of x_test is:\", x_test.shape)\n",
    "    print(\"Shape of y_test is:\", y_test.shape)\n",
    "\n",
    "    x_train = x_train[:,:,:,:]\n",
    "    x_test = x_test[:,:,:,:]\n",
    "\n",
    "    x_train = np.rollaxis(x_train, 3)\n",
    "    x_test = np.rollaxis(x_test, 3)\n",
    "\n",
    "    print(\"Shape of x_train is now:\", x_train.shape)\n",
    "    print(\"Shape of x_test is now:\", x_test.shape)\n",
    "\n",
    "    y_train = y_train[:,0]\n",
    "    y_test = y_test[:,0]\n",
    "\n",
    "    print(\"Shape of y_train is now:\", y_train.shape)\n",
    "    print(\"Shape of y_test is now:\", y_test.shape)\n",
    "\n",
    "    y_train[y_train==10] = 0\n",
    "    y_test[y_test==10] = 0\n",
    "\n",
    "    print(\"labels of y_train are\", np.unique(y_train[:]))\n",
    "    print(\"labels of y_test are\", np.unique(y_test[:]))\n",
    "    \n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "    # If subtract pixel mean is enabled (Subtracting pixel mean improves accuracy)\n",
    "    if subtract_pixel_mean:\n",
    "        x_train_mean = np.mean(x_train, axis=0)\n",
    "        x_train -= x_train_mean\n",
    "        x_test -= x_train_mean\n",
    "        \n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    return (x_train, y_train),(x_test,y_test) \n",
    "    \n",
    "\n",
    "#Defining Resnet20 model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def choose_resnet_model(input_shape, n=3, version=1):\n",
    "    \n",
    "    # Model version\n",
    "    # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "\n",
    "    # Computed depth from supplied model parameter n\n",
    "    if version == 1:\n",
    "        depth = n * 6 + 2\n",
    "    elif version == 2:\n",
    "        depth = n * 9 + 2\n",
    "\n",
    "    # Model name, depth and version\n",
    "    model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "\n",
    "    # Input image dimensions.\n",
    "    #input_shape = x_train.shape[1:]\n",
    "\n",
    "    if version == 2:\n",
    "        model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "    else:\n",
    "        model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    init = model.get_weights()\n",
    "\n",
    "    return model, init, model_type\n",
    "    \n",
    "\n",
    "def train_model(init, model, x_train, y_train, x_test, y_test, batch_size=32, num_classes=10, epochs=100, data_augmentation=True, \n",
    "                model_name= 'keras_cifar10_trained_teacher.h5'):\n",
    "    \n",
    "    # Prepare model model saving directory.\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    filepath = os.path.join(save_dir, model_name)\n",
    "    \n",
    "    model.set_weights(init)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "    callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "    if not data_augmentation:\n",
    "        print(\"Not using data augmentation.\")\n",
    "        model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test,y_test),\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "    else:\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        print(\"Using real-time data augmentation.\")\n",
    "        datagen = ImageDataGenerator(\n",
    "                # set input mean to 0 over the dataset\n",
    "                featurewise_center=False,\n",
    "                # set each sample mean to 0\n",
    "                samplewise_center=False,\n",
    "                # divide inputs by std of dataset\n",
    "                featurewise_std_normalization=False,\n",
    "                # divide each input by its std\n",
    "                samplewise_std_normalization=False,\n",
    "                # apply ZCA whitening\n",
    "                zca_whitening=False,\n",
    "                # epsilon for ZCA whitening\n",
    "                zca_epsilon=1e-06,\n",
    "                # randomly rotate images in the range (deg 0 to 180)\n",
    "                rotation_range=0,\n",
    "                # randomly shift images horizontally\n",
    "                width_shift_range=0.1,\n",
    "                # randomly shift images vertically\n",
    "                height_shift_range=0.1,\n",
    "                # set range for random shear\n",
    "                shear_range=0.,\n",
    "                # set range for random zoom\n",
    "                zoom_range=0.,\n",
    "                # set range for random channel shifts\n",
    "                channel_shift_range=0.,\n",
    "                # set mode for filling points outside the input boundaries\n",
    "                fill_mode='nearest',\n",
    "                # value used for fill_mode = \"constant\"\n",
    "                cval=0.,\n",
    "                # randomly flip images\n",
    "                horizontal_flip=True,\n",
    "                # randomly flip images\n",
    "                vertical_flip=False,\n",
    "                # set rescaling factor (applied before any other transformation)\n",
    "                rescale=None,\n",
    "                # set function that will be applied on each input\n",
    "                preprocessing_function=None,\n",
    "                # image data format, either \"channels_first\" or \"channels_last\"\n",
    "                data_format=None,\n",
    "                # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "                validation_split=0.0)\n",
    "\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # Fit the noised student model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                epochs=epochs, verbose=1, workers=4,validation_data=(x_test,y_test),\n",
    "                                callbacks=callbacks)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Supervised learning model with '+str(epochs)+'epochs \\n')\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "\n",
    "    \n",
    "    # Save model and weights\n",
    "    model.save(filepath)\n",
    "    print('Saved trained model at %s ' % filepath)\n",
    "\n",
    "    return scores[1], filepath\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYB9MNWtgC9w"
   },
   "source": [
    " \n",
    "# Test 1\n",
    "Below we would like to check if starting from the weights of the fully supervised model trained above, we would get an increase in accuracy if we run STNS on the full dataset with different ratios of unlabeled to labeled data. And if the accuracy goes up, we would like to see what is the optimal ratio. \n",
    "The STNS algorithm used is as follows:\n",
    "We do a loop over different ratios of label to unlabelled data (rate). And each loop does the following:\n",
    "1) take the weights from the fully supervised teacher model trained in box above \n",
    "2) Ten cycles of: un-noised model (teacher)->predict hard pseudolabel->training 10 epochs for noised model (student=teacher+dropout noise) on labeled+pseudo labeled->new weights. \n",
    "\n",
    "The student model will be the teacher model noised by a Dropout(0.5) before the last layer. And we will use data augmentation to add to the noise for the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "le4eN1EQigsM",
    "outputId": "3f93edf3-4aaa-46b0-d3ca-9ed38306d42d"
   },
   "outputs": [],
   "source": [
    "#Defining noised Student with a Dropout(0.5) just before the output layer\n",
    "\n",
    "\n",
    "def noised_resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def noised_resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    y = Dropout(0.5)(y)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def choose_noised_resnet_model(input_shape, n=3, version=1):\n",
    "\n",
    "    # Model version\n",
    "    # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "\n",
    "    # Computed depth from supplied model parameter n\n",
    "    if version == 1:\n",
    "        depth = n * 6 + 2\n",
    "    elif version == 2:\n",
    "        depth = n * 9 + 2\n",
    "\n",
    "    # Model name, depth and version\n",
    "    model_type = 'ResNet%dv%d' % (depth, version)\n",
    "    if version == 2:\n",
    "        model = noised_resnet_v2(input_shape=input_shape, depth=depth)\n",
    "    else:\n",
    "        model = noised_resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model_type)\n",
    "    \n",
    "    return model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "txpUcvn7d_Es",
    "outputId": "7a483a3a-a3a3-455a-92c1-097dcb9a5ec1"
   },
   "outputs": [],
   "source": [
    "def stns_full_dataset(x_train,y_train,x_test,y_test,rate,teacher, student,teacher_path,accuracy_supervised, data_augmentation):\n",
    "    \n",
    "    # Prepare model model saving directory.\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    teacher_loop_name = 'keras_teacher_loop.h5'\n",
    "    teacher_loop_path = os.path.join(save_dir, teacher_loop_name)\n",
    "    num_classes=10\n",
    "\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "                # set input mean to 0 over the dataset\n",
    "                featurewise_center=False,\n",
    "                # set each sample mean to 0\n",
    "                samplewise_center=False,\n",
    "                # divide inputs by std of dataset\n",
    "                featurewise_std_normalization=False,\n",
    "                # divide each input by its std\n",
    "                samplewise_std_normalization=False,\n",
    "                # apply ZCA whitening\n",
    "                zca_whitening=False,\n",
    "                # epsilon for ZCA whitening\n",
    "                zca_epsilon=1e-06,\n",
    "                # randomly rotate images in the range (deg 0 to 180)\n",
    "                rotation_range=0,\n",
    "                # randomly shift images horizontally\n",
    "                width_shift_range=0.1,\n",
    "                # randomly shift images vertically\n",
    "                height_shift_range=0.1,\n",
    "                # set range for random shear\n",
    "                shear_range=0.,\n",
    "                # set range for random zoom\n",
    "                zoom_range=0.,\n",
    "                # set range for random channel shifts\n",
    "                channel_shift_range=0.,\n",
    "                # set mode for filling points outside the input boundaries\n",
    "                fill_mode='nearest',\n",
    "                # value used for fill_mode = \"constant\"\n",
    "                cval=0.,\n",
    "                # randomly flip images\n",
    "                horizontal_flip=True,\n",
    "                # randomly flip images\n",
    "                vertical_flip=False,\n",
    "                # set rescaling factor (applied before any other transformation)\n",
    "                rescale=None,\n",
    "                # set function that will be applied on each input\n",
    "                preprocessing_function=None,\n",
    "                # image data format, either \"channels_first\" or \"channels_last\"\n",
    "                data_format=None,\n",
    "                # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "                validation_split=0.0)\n",
    "    \n",
    "    # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "    checkpoint = ModelCheckpoint(filepath=teacher_path,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "    callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "    \n",
    "    # rate= n_true/n_pseudo\n",
    "    \n",
    "    n_total=x_train.shape[0]\n",
    "\n",
    "    # total number of train images (n_total) = number of true label images (n_true) + number of pseudo label images (n_pseudo)\n",
    "    #                                        = n_pseudo(1/rate+1)\n",
    "    # n_pseudo = n_total/(1/rate+1); n_true=n_total-n_pseudo\n",
    "\n",
    "    #loop over rate values in order to find the optimal rate value for the self-learning semi supervised learning, \n",
    "    #ie one that will maximize accuracy\n",
    "    j=0\n",
    "    accuracy=np.zeros(rate.shape[0])\n",
    "    for r in rate:\n",
    "    \n",
    "        print(\"rate=\"+str(r)+\":\\n\")\n",
    "        n_pseudo=n_total/(1+(1/r))\n",
    "        n_true=n_total-n_pseudo\n",
    "        mask_true=np.random.choice(int(n_total),int(n_true),replace=False) #generating n_true integers between 0 and n_total-1\n",
    "        mask_pseudo=[item for item in range(n_total) if item not in mask_true] #all the other numbers between 0 and n_total-1 which are not in mask_true\n",
    "        mask_pseudo=np.array(mask_pseudo)\n",
    "        x_true=x_train[mask_true[:]] #x for the labeled data\n",
    "        y_true=y_train[mask_true[:]] #y for the labeled data\n",
    "        x_pseudo=x_train[mask_pseudo[:]] #x for the unlabeled data (pseudo)\n",
    "        #Taking weights from supervised model on full data (box above) \n",
    "        teacher.load_weights(teacher_path)\n",
    "        teacher.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "                  metrics=['accuracy'])\n",
    "        #evaluating teacher model on test data    \n",
    "        scores=teacher.evaluate(x_test,y_test,verbose=0)\n",
    "        print(\"Original model with labelled data only predicting on test data: \",scores[1])\n",
    "\n",
    "        x_true_pseudo=np.concatenate([x_true,x_pseudo]) #concatenating x for labeled and unlabeled data\n",
    "        print('x_true_pseudo.shape: ',x_true_pseudo.shape)\n",
    "        y_pseudo=teacher.predict(x_pseudo) #predicting labels on unlabeled data\n",
    "        y_pseudo = y_pseudo.argmax(axis=-1)\n",
    "        y_pseudo=keras.utils.to_categorical(y_pseudo, num_classes)\n",
    "        print('Shape y_pseudo',y_pseudo.shape)\n",
    "        print('Shape y_true',y_true.shape)\n",
    "        y_true_pseudo=np.concatenate([y_true,y_pseudo]) #concatenating y for labeled and pseudo labeled\n",
    "        print('y_true_pseudo.shape: ', y_true_pseudo.shape)\n",
    "        student.load_weights(teacher_path)\n",
    "        for i in range(10): \n",
    "            # 10 loops of 10 epochs of noised student training for labeled and pseudo labeled data (step 3 in article)\n",
    "            # followed by generating predictions on unlabeled data with the teacher model (=un-noised student)\n",
    "            # which uses the weights of the trained noised student (noise does not change the weights structure of models) (step 2 in article)\n",
    "            print(i)\n",
    "            # Compute quantities required for featurewise normalization\n",
    "            # (std, mean, and principal components if ZCA whitening is applied).\n",
    "            datagen.fit(x_true_pseudo)\n",
    "\n",
    "            # Fit the noised student model on the batches generated by datagen.flow().\n",
    "            training=student.fit_generator(datagen.flow(x_true_pseudo, y_true_pseudo, batch_size=32),\n",
    "                            epochs=10, verbose=1, workers=4,validation_data=(x_test,y_test),\n",
    "                            callbacks=callbacks)\n",
    "\n",
    "            # Save weights\n",
    "            student.save_weights(teacher_loop_path)\n",
    "            # Load weights for teacher model (un-noised)\n",
    "            teacher.load_weights(teacher_loop_path)\n",
    "            y_pseudo=teacher.predict(x_pseudo)\n",
    "            y_pseudo = y_pseudo.argmax(axis=-1)\n",
    "            scores=teacher.evaluate(x_test,y_test,verbose=0) #evaluating model on test data\n",
    "            print('iteration: ',i)\n",
    "            print('Test loss:', scores[0])\n",
    "            print('Test accuracy:', scores[1])\n",
    "\n",
    "            y_pseudo=keras.utils.to_categorical(y_pseudo, num_classes)\n",
    "            y_true_pseudo=np.concatenate([y_true,y_pseudo]) #new y_true_pseudo to be used in next loop\n",
    "        accuracy[j]=scores[1]\n",
    "        j+=1\n",
    "\n",
    "    supervised=[accuracy_supervised for i in range(len(rate))]\n",
    "    plt.plot(rate,accuracy)\n",
    "    plt.plot(rate,supervised,'r')\n",
    "    plt.xlabel('Rate')\n",
    "    plt.ylabel('Accuracy top 1')\n",
    "    if data_augmentation==False:\n",
    "        plt.title('Resnet - Test1 - Accuracy as a function of rate - No data augmentation')\n",
    "    else:\n",
    "        plt.title('Resnet - Test1 - Accuracy as a function of rate - with data augmentation')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZgdMxhPpA3D"
   },
   "source": [
    "# Test 2\n",
    "Now, we take a subset of 5,000 pictures that we use as labeled, and use part of the rest of the dataset as unlabeled. We run the teacher model on that labeled dataset. And then run STNS. We do that for different rate levels, and compare accuracies\n",
    "And then we try for a subset of 1000 pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hctlwQX5eNxf",
    "outputId": "a2bd5e21-e1e2-4985-ad29-cd941e64f88d"
   },
   "outputs": [],
   "source": [
    "# Varying the ratio for 5000 or 1000 labeled images. Rest of training dataset unlabeled with ratio determining size of total dataset.\n",
    "# Testing on full test dataset\n",
    "\n",
    "\n",
    "\n",
    "def prepare_smaller_labeled_training_dataset(x_train, y_train, sample_size=5000, num_classes=10):\n",
    "\n",
    "    total_size_cifar10=x_train.shape[0]   \n",
    "    print(total_size_cifar10)\n",
    "    print(y_train.shape)\n",
    "    y_train=y_train.argmax(axis=-1)\n",
    "    print(y_train.shape)\n",
    "    #checking that classes are balanced\n",
    "    for i in range(num_classes):\n",
    "        print(str(i)+\":\",sum(y_train==i))\n",
    "\n",
    "    #Selecting 500 images of each class\n",
    "    k=0\n",
    "    x_true=np.zeros((sample_size,32,32,3))\n",
    "    y_true=np.full((sample_size,),-1)\n",
    "    mask2=[]\n",
    "    for i in range(total_size_cifar10):\n",
    "        #print(i)\n",
    "        for j in range(num_classes):\n",
    "            if sum(y_true==j)<sample_size/num_classes:\n",
    "                if y_train[i]==j:\n",
    "                    x_true[k,:]=x_train[i,:]\n",
    "                    y_true[k]=y_train[i]\n",
    "                    k+=1\n",
    "                    mask2.append(i)\n",
    "                    break\n",
    "        #print('k=',k)\n",
    "        if k==sample_size:\n",
    "            break\n",
    "        \n",
    "    #print(y_small_train[0:40])\n",
    "    print(x_true.shape)\n",
    "    print(y_true.shape)\n",
    "\n",
    "\n",
    "    #verifying that there are 500 images in each class\n",
    "    for i in range(num_classes):\n",
    "        print(str(i)+\":\",sum(y_true==i))\n",
    "    \n",
    "    y_true=keras.utils.to_categorical(y_true, 10)\n",
    "    \n",
    "    return x_true, y_true, mask2\n",
    " \n",
    "def stns_5000(init, model_name, x_train,y_train,x_test,y_test,rate,teacher, student, sample_size=5000, epochs=100, \n",
    "              data_augmentation=True):\n",
    "    \n",
    "    total_size_cifar10=x_train.shape[0]\n",
    "    num_classes=10\n",
    "    x_true, y_true, mask2 = prepare_smaller_labeled_training_dataset(x_train, y_train, sample_size, num_classes=num_classes)\n",
    "    \n",
    "    accuracy2_supervised, teacher2_path = train_model(init, teacher, x_true, y_true, x_test, y_test, batch_size=32, \n",
    "                                                      num_classes=num_classes, \n",
    "                                                       epochs=epochs, data_augmentation=data_augmentation,\n",
    "                                                       model_name= model_name)\n",
    "\n",
    "    print('Small sample of 5000 training images, Supervised learning model with '+str(epochs)+'epochs \\n')\n",
    "    print('Test accuracy:', accuracy2_supervised)\n",
    "    \n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    teacher_loop2_name = 'teacher_loop2.h5'\n",
    "    teacher_loop2_path = os.path.join(save_dir, teacher_loop2_name)\n",
    "\n",
    "    \n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "            # set input mean to 0 over the dataset\n",
    "            featurewise_center=False,\n",
    "            # set each sample mean to 0\n",
    "            samplewise_center=False,\n",
    "            # divide inputs by std of dataset\n",
    "            featurewise_std_normalization=False,\n",
    "            # divide each input by its std\n",
    "            samplewise_std_normalization=False,\n",
    "            # apply ZCA whitening\n",
    "            zca_whitening=False,\n",
    "            # epsilon for ZCA whitening\n",
    "            zca_epsilon=1e-06,\n",
    "            # randomly rotate images in the range (deg 0 to 180)\n",
    "            rotation_range=0,\n",
    "            # randomly shift images horizontally\n",
    "            width_shift_range=0.1,\n",
    "            # randomly shift images vertically\n",
    "            height_shift_range=0.1,\n",
    "            # set range for random shear\n",
    "            shear_range=0.,\n",
    "            # set range for random zoom\n",
    "            zoom_range=0.,\n",
    "            # set range for random channel shifts\n",
    "            channel_shift_range=0.,\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            # value used for fill_mode = \"constant\"\n",
    "            cval=0.,\n",
    "            # randomly flip images\n",
    "            horizontal_flip=True,\n",
    "            # randomly flip images\n",
    "            vertical_flip=False,\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split=0.0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    mask3=[i for i in range(total_size_cifar10) if i not in mask2]\n",
    "    x_total_unlabeled=x_train[mask3,:]\n",
    "    print(\"size x_total_unlabeled (should be 45,000): \", x_total_unlabeled.shape[0])\n",
    "    \n",
    "    # rate= n_true/n_pseudo, n_true=sample_size\n",
    "\n",
    "    number_rate=len(rate)\n",
    "    \n",
    "    #n_total=sample_size*(1+rate)\n",
    "\n",
    "    # total number of train images (n_total) = number of true label images (sample_size) + number of pseudo label images (n_pseudo)\n",
    "    #                                        = sample_size(rate+1)\n",
    "    # n_pseudo = sample_size*rate\n",
    "\n",
    "    #loop over rate values in order to find the optimal rate value for the self-learning semi supervised learning, \n",
    "    #ie one that will maximize accuracy\n",
    "    \n",
    "    accuracy2=np.zeros(number_rate)\n",
    "    j=0\n",
    "    for r in rate:\n",
    "    \n",
    "        teacher.load_weights(teacher2_path)\n",
    "        student.load_weights(teacher2_path)\n",
    "    \n",
    "        print(\"rate=\"+str(r)+\":\\n\")\n",
    "        n_pseudo=int(sample_size*r)\n",
    "        n_true=sample_size\n",
    "        n_total=n_true+n_pseudo\n",
    "        print(\"n_total, n_true, n_pseudo: \", n_total, n_true, n_pseudo)\n",
    "        x_pseudo=x_total_unlabeled[0:n_pseudo,:] #x for the unlabeled data (pseudo)\n",
    "        x_true_pseudo=np.concatenate([x_true,x_pseudo]) #concatenating x for labeled and unlabeled data\n",
    "        print('x_true_pseudo.shape: ',x_true_pseudo.shape)\n",
    "        y_pseudo=teacher.predict(x_pseudo) #predicting labels on unlabeled data\n",
    "        y_pseudo = y_pseudo.argmax(axis=-1)\n",
    "        y_pseudo=keras.utils.to_categorical(y_pseudo, num_classes)\n",
    "        y_true_pseudo=np.concatenate([y_true,y_pseudo]) #concatenating y for labeled and pseudo labeled\n",
    "        print('y_true_pseudo.shape: ', y_true_pseudo.shape)\n",
    "        for i in range(10): \n",
    "            # 10 loops of 10 epochs of noised student training for labeled and pseudo labeled data (step 3 in article)\n",
    "            # followed by generating predictions on unlabeled data with the teacher model (=un-noised student)\n",
    "            # which uses the weights of the trained noised student (noise does not change the weights structure of models) (step 2 in article)\n",
    "            print(i)\n",
    "            training=student.fit(x_true_pseudo,y_true_pseudo,validation_data=(x_test,y_test),\n",
    "                                 epochs=10,batch_size=32,verbose=0)\n",
    "            # Save weights\n",
    "            student.save_weights(teacher_loop2_path)\n",
    "            # Load weights for teacher model (un-noised)\n",
    "            teacher.load_weights(teacher_loop2_path)\n",
    "            y_pseudo=teacher.predict(x_pseudo) #predicting labels on unlabeled data\n",
    "            y_pseudo = y_pseudo.argmax(axis=-1)\n",
    "            y_pseudo=keras.utils.to_categorical(y_pseudo, num_classes)\n",
    "            scores3=teacher.evaluate(x_test,y_test,verbose=0) #evaluating model on test data\n",
    "            print('iteration: ',i)\n",
    "            print('Test loss:', scores3[0])\n",
    "            print('Test accuracy:', scores3[1])\n",
    "            y_true_pseudo=np.concatenate([y_true,y_pseudo]) #new y_true_pseudo to be used in next loop\n",
    "    \n",
    "        accuracy2[j]=scores3[1]\n",
    "        j+=1\n",
    "\n",
    "    supervised2=[accuracy2_supervised for i in range(number_rate)]\n",
    "    plt.plot(rate, accuracy2)\n",
    "    plt.plot(rate, supervised2,'r')\n",
    "    plt.xlabel('Rate')\n",
    "    plt.ylabel('Accuracy top 1')\n",
    "    if data_augmentation==False:\n",
    "        plt.title('Resnet - Test2 - Accuracy as a function of rate - No data augmentation')\n",
    "    else:\n",
    "        plt.title('Resnet  - Test2 - Accuracy as a function of rate - with data augmentation')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy2\n",
    "    \n",
    "    \n",
    "    \n",
    "#from keras.utils import plot_model\n",
    "#plot_model(teacher,to_file='teacher.png')\n",
    "#plot_model(student,to_file='student.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run all the definitions above, and then run the calculations in following boxes\n",
    "\n",
    "#Running Test2 for CIFAR10\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_cifar10_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate2=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5])\n",
    "model_name= 'keras_cifar10_trained_teacher_5000_da.h5'\n",
    "accuracy_2 = stns_5000(init, model_name, x_train,y_train,x_test,y_test,rate2,teacher,\n",
    "                      student, sample_size=5000, epochs=100, \n",
    "                      data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "ResNet20v1\n",
      "50000\n",
      "(50000, 10)\n",
      "(50000,)\n",
      "0: 5000\n",
      "1: 5000\n",
      "2: 5000\n",
      "3: 5000\n",
      "4: 5000\n",
      "5: 5000\n",
      "6: 5000\n",
      "7: 5000\n",
      "8: 5000\n",
      "9: 5000\n",
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "0: 100\n",
      "1: 100\n",
      "2: 100\n",
      "3: 100\n",
      "4: 100\n",
      "5: 100\n",
      "6: 100\n",
      "7: 100\n",
      "8: 100\n",
      "9: 100\n",
      "Learning rate:  0.001\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 12s 390ms/step - loss: 2.6404 - accuracy: 0.1840 - val_loss: 2.4599 - val_accuracy: 0.1887\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      " 3/32 [=>............................] - ETA: 1s - loss: 2.1433 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 154ms/step - loss: 2.0267 - accuracy: 0.3230 - val_loss: 2.3648 - val_accuracy: 0.1519\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 1.9217 - accuracy: 0.3500 - val_loss: 2.4143 - val_accuracy: 0.2190\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 1.8670 - accuracy: 0.3580 - val_loss: 2.3642 - val_accuracy: 0.2183\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 1.7718 - accuracy: 0.4170 - val_loss: 2.8085 - val_accuracy: 0.1931\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.7042 - accuracy: 0.4290 - val_loss: 2.9160 - val_accuracy: 0.2171\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.6587 - accuracy: 0.4480 - val_loss: 2.3966 - val_accuracy: 0.2327\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.6138 - accuracy: 0.4830 - val_loss: 2.5052 - val_accuracy: 0.2907\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.5702 - accuracy: 0.4820 - val_loss: 2.2274 - val_accuracy: 0.2673\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 150ms/step - loss: 1.5109 - accuracy: 0.5000 - val_loss: 1.9424 - val_accuracy: 0.3571\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 150ms/step - loss: 1.5133 - accuracy: 0.5140 - val_loss: 2.5851 - val_accuracy: 0.2855\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.4756 - accuracy: 0.5200 - val_loss: 2.6461 - val_accuracy: 0.3043\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.4301 - accuracy: 0.5350 - val_loss: 2.0382 - val_accuracy: 0.3798\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 11s 330ms/step - loss: 1.3521 - accuracy: 0.5680 - val_loss: 1.9549 - val_accuracy: 0.3831\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 1.3421 - accuracy: 0.5740 - val_loss: 2.5202 - val_accuracy: 0.3276\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 11s 330ms/step - loss: 1.2723 - accuracy: 0.5920 - val_loss: 2.0873 - val_accuracy: 0.3964\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 1.2938 - accuracy: 0.5750 - val_loss: 2.2602 - val_accuracy: 0.3366\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 1.2526 - accuracy: 0.6030 - val_loss: 1.9472 - val_accuracy: 0.4247\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 1.1979 - accuracy: 0.6420 - val_loss: 2.3721 - val_accuracy: 0.3328\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 1.1886 - accuracy: 0.6270 - val_loss: 2.5745 - val_accuracy: 0.3478\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 1.1063 - accuracy: 0.6470 - val_loss: 2.0086 - val_accuracy: 0.4236\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.0893 - accuracy: 0.6670 - val_loss: 2.7018 - val_accuracy: 0.3472\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.1191 - accuracy: 0.6480 - val_loss: 2.5950 - val_accuracy: 0.3303\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 1.1375 - accuracy: 0.6490 - val_loss: 2.1973 - val_accuracy: 0.4056\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 1.1002 - accuracy: 0.6710 - val_loss: 2.7707 - val_accuracy: 0.3865\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 1.0141 - accuracy: 0.6890 - val_loss: 2.1924 - val_accuracy: 0.4183\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.9904 - accuracy: 0.7080 - val_loss: 2.3951 - val_accuracy: 0.4082\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 1.0048 - accuracy: 0.6880 - val_loss: 2.1714 - val_accuracy: 0.3932\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.9460 - accuracy: 0.7260 - val_loss: 1.8894 - val_accuracy: 0.4343\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 1.0693 - accuracy: 0.6690 - val_loss: 2.3082 - val_accuracy: 0.3862\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 1.0133 - accuracy: 0.6890 - val_loss: 1.8241 - val_accuracy: 0.4500\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.9005 - accuracy: 0.7550 - val_loss: 1.9713 - val_accuracy: 0.4419\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.9099 - accuracy: 0.7410 - val_loss: 2.0900 - val_accuracy: 0.4395\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.8790 - accuracy: 0.7420 - val_loss: 2.4416 - val_accuracy: 0.4225\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 150ms/step - loss: 0.8727 - accuracy: 0.7520 - val_loss: 2.1520 - val_accuracy: 0.4228\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.8384 - accuracy: 0.7620 - val_loss: 2.3036 - val_accuracy: 0.3973\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.8394 - accuracy: 0.7680 - val_loss: 2.4318 - val_accuracy: 0.4091\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.7993 - accuracy: 0.7820 - val_loss: 2.0454 - val_accuracy: 0.4428\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.7695 - accuracy: 0.7900 - val_loss: 2.2506 - val_accuracy: 0.4406\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.7508 - accuracy: 0.7910 - val_loss: 3.1514 - val_accuracy: 0.3598\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.7224 - accuracy: 0.8210 - val_loss: 2.1864 - val_accuracy: 0.4534\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.6959 - accuracy: 0.8050 - val_loss: 2.5945 - val_accuracy: 0.3819\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.6952 - accuracy: 0.8310 - val_loss: 2.8597 - val_accuracy: 0.4002\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.7129 - accuracy: 0.8150 - val_loss: 2.1566 - val_accuracy: 0.4640\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.7445 - accuracy: 0.8060 - val_loss: 3.0213 - val_accuracy: 0.3759\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.7213 - accuracy: 0.8140 - val_loss: 2.3403 - val_accuracy: 0.4597\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.7225 - accuracy: 0.8060 - val_loss: 2.2482 - val_accuracy: 0.4604\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 151ms/step - loss: 0.6415 - accuracy: 0.8400 - val_loss: 2.2769 - val_accuracy: 0.4674\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.6005 - accuracy: 0.8540 - val_loss: 2.5965 - val_accuracy: 0.4287\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.6301 - accuracy: 0.8410 - val_loss: 2.4387 - val_accuracy: 0.4412\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.6503 - accuracy: 0.8430 - val_loss: 2.4358 - val_accuracy: 0.4280\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.6507 - accuracy: 0.8360 - val_loss: 2.8825 - val_accuracy: 0.4102\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 0.6957 - accuracy: 0.8090 - val_loss: 2.8723 - val_accuracy: 0.4066\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.6111 - accuracy: 0.8490 - val_loss: 3.2263 - val_accuracy: 0.4061\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.6545 - accuracy: 0.8310 - val_loss: 3.3463 - val_accuracy: 0.3787\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.5743 - accuracy: 0.8730 - val_loss: 2.6078 - val_accuracy: 0.4433\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 11s 352ms/step - loss: 0.5191 - accuracy: 0.8910 - val_loss: 2.4545 - val_accuracy: 0.4859\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.5725 - accuracy: 0.8600 - val_loss: 2.3689 - val_accuracy: 0.4470\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.5181 - accuracy: 0.8890 - val_loss: 2.3106 - val_accuracy: 0.4662\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.5196 - accuracy: 0.8810 - val_loss: 2.8573 - val_accuracy: 0.4127\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.5105 - accuracy: 0.8850 - val_loss: 2.8499 - val_accuracy: 0.4273\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.5410 - accuracy: 0.8840 - val_loss: 3.1177 - val_accuracy: 0.3911\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.5515 - accuracy: 0.8770 - val_loss: 3.5589 - val_accuracy: 0.3972\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.5697 - accuracy: 0.8750 - val_loss: 2.1185 - val_accuracy: 0.4877\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.5284 - accuracy: 0.8830 - val_loss: 2.2682 - val_accuracy: 0.4646\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.4749 - accuracy: 0.9010 - val_loss: 2.3436 - val_accuracy: 0.4670\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.4869 - accuracy: 0.9000 - val_loss: 2.2648 - val_accuracy: 0.4800\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.4602 - accuracy: 0.9120 - val_loss: 2.8344 - val_accuracy: 0.4485\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.4265 - accuracy: 0.9190 - val_loss: 2.8906 - val_accuracy: 0.4484\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.4257 - accuracy: 0.9190 - val_loss: 2.2729 - val_accuracy: 0.4857\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.4911 - accuracy: 0.8980 - val_loss: 3.3165 - val_accuracy: 0.3902\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.4534 - accuracy: 0.9150 - val_loss: 2.4658 - val_accuracy: 0.4830\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 0.4410 - accuracy: 0.9100 - val_loss: 2.6875 - val_accuracy: 0.4339\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.4152 - accuracy: 0.9180 - val_loss: 3.0777 - val_accuracy: 0.4193\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.4286 - accuracy: 0.9280 - val_loss: 2.6455 - val_accuracy: 0.4416\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.4751 - accuracy: 0.9100 - val_loss: 3.2989 - val_accuracy: 0.4389\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.4505 - accuracy: 0.9070 - val_loss: 3.9278 - val_accuracy: 0.3847\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.4884 - accuracy: 0.8990 - val_loss: 3.4470 - val_accuracy: 0.3927\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.3821 - accuracy: 0.9340 - val_loss: 2.5938 - val_accuracy: 0.4505\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.4798 - accuracy: 0.9090 - val_loss: 2.8424 - val_accuracy: 0.4490\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.4772 - accuracy: 0.9100 - val_loss: 2.7176 - val_accuracy: 0.4300\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.3991 - accuracy: 0.9260 - val_loss: 2.2084 - val_accuracy: 0.4972\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.3253 - accuracy: 0.9540 - val_loss: 2.1193 - val_accuracy: 0.5180\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.3198 - accuracy: 0.9660 - val_loss: 2.1118 - val_accuracy: 0.5230\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.3000 - accuracy: 0.9750 - val_loss: 2.0792 - val_accuracy: 0.5241\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.2988 - accuracy: 0.9720 - val_loss: 2.0811 - val_accuracy: 0.5235\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.2894 - accuracy: 0.9780 - val_loss: 2.0709 - val_accuracy: 0.5284\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.3084 - accuracy: 0.9650 - val_loss: 2.1106 - val_accuracy: 0.5285\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.3045 - accuracy: 0.9750 - val_loss: 2.1003 - val_accuracy: 0.5261\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.2704 - accuracy: 0.9790 - val_loss: 2.0912 - val_accuracy: 0.5254\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 152ms/step - loss: 0.2699 - accuracy: 0.9840 - val_loss: 2.0920 - val_accuracy: 0.5263\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.2719 - accuracy: 0.9770 - val_loss: 2.1034 - val_accuracy: 0.5271\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.2820 - accuracy: 0.9730 - val_loss: 2.1099 - val_accuracy: 0.5288\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.2551 - accuracy: 0.9900 - val_loss: 2.1073 - val_accuracy: 0.5274\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.2816 - accuracy: 0.9790 - val_loss: 2.1123 - val_accuracy: 0.5269\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 154ms/step - loss: 0.2714 - accuracy: 0.9800 - val_loss: 2.1525 - val_accuracy: 0.5279\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 0.2710 - accuracy: 0.9760 - val_loss: 2.1020 - val_accuracy: 0.5317\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 0.2676 - accuracy: 0.9800 - val_loss: 2.0835 - val_accuracy: 0.5345\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 5s 153ms/step - loss: 0.2620 - accuracy: 0.9800 - val_loss: 2.1214 - val_accuracy: 0.5281\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      "32/32 [==============================] - 11s 352ms/step - loss: 0.2444 - accuracy: 0.9880 - val_loss: 2.1302 - val_accuracy: 0.5280\n",
      "10000/10000 [==============================] - 4s 357us/step\n",
      "Supervised learning model with 100epochs \n",
      "\n",
      "Test loss: 2.1301542472839357\n",
      "Test accuracy: 0.527999997138977\n",
      "Saved trained model at /home/ANDURAND/pandurand/saved_models/keras_cifar10_trained_teacher_1000_da.h5 \n",
      "Small sample of 5000 training images, Supervised learning model with 100epochs \n",
      "\n",
      "Test accuracy: 0.527999997138977\n",
      "size x_total_unlabeled (should be 45,000):  49000\n",
      "rate=0.05:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1050 1000 50\n",
      "x_true_pseudo.shape:  (1050, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1050, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.8892819705963135\n",
      "Test accuracy: 0.4577000141143799\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.26817804813385\n",
      "Test accuracy: 0.4884999990463257\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.7147017868041994\n",
      "Test accuracy: 0.4390999972820282\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.839760988998413\n",
      "Test accuracy: 0.48989999294281006\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.460305561065674\n",
      "Test accuracy: 0.3815999925136566\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.0359249786376954\n",
      "Test accuracy: 0.4652999937534332\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.6086373470306397\n",
      "Test accuracy: 0.4058000147342682\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.2983397079467776\n",
      "Test accuracy: 0.45730000734329224\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.118421015548706\n",
      "Test accuracy: 0.48730000853538513\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.245577632141114\n",
      "Test accuracy: 0.423799991607666\n",
      "rate=0.1:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1100 1000 100\n",
      "x_true_pseudo.shape:  (1100, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1100, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.7761242706298828\n",
      "Test accuracy: 0.45170000195503235\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.2535692504882814\n",
      "Test accuracy: 0.5027999877929688\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.617288795852661\n",
      "Test accuracy: 0.4526999890804291\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.6147464653015136\n",
      "Test accuracy: 0.414000004529953\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.908753305053711\n",
      "Test accuracy: 0.49000000953674316\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.142764298248291\n",
      "Test accuracy: 0.37290000915527344\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.1417218254089354\n",
      "Test accuracy: 0.4553000032901764\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 2.754025469970703\n",
      "Test accuracy: 0.47679999470710754\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.179382313156128\n",
      "Test accuracy: 0.45419999957084656\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.1121195083618165\n",
      "Test accuracy: 0.44699999690055847\n",
      "rate=0.2:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1200 1000 200\n",
      "x_true_pseudo.shape:  (1200, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1200, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.386734765625\n",
      "Test accuracy: 0.46459999680519104\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.449840747833252\n",
      "Test accuracy: 0.484499990940094\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.084280154800415\n",
      "Test accuracy: 0.4397999942302704\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.0167512214660643\n",
      "Test accuracy: 0.4602000117301941\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.8164052322387696\n",
      "Test accuracy: 0.43309998512268066\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 2.7354861000061037\n",
      "Test accuracy: 0.46639999747276306\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 4.184362241744995\n",
      "Test accuracy: 0.4262000024318695\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.4907069194793703\n",
      "Test accuracy: 0.4499000012874603\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.765496647644043\n",
      "Test accuracy: 0.4442000091075897\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.352682559013367\n",
      "Test accuracy: 0.445499986410141\n",
      "rate=0.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1500 1000 500\n",
      "x_true_pseudo.shape:  (1500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.8354077308654784\n",
      "Test accuracy: 0.43790000677108765\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.3220609970092774\n",
      "Test accuracy: 0.4596000015735626\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.6261104804992677\n",
      "Test accuracy: 0.4309999942779541\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.8724874717712403\n",
      "Test accuracy: 0.4821999967098236\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.9438353218078612\n",
      "Test accuracy: 0.42719998955726624\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.007378017425537\n",
      "Test accuracy: 0.4690999984741211\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.14093434715271\n",
      "Test accuracy: 0.4772999882698059\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 4.3019416107177735\n",
      "Test accuracy: 0.4415999948978424\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.155509124755859\n",
      "Test accuracy: 0.4223000109195709\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.384472805023193\n",
      "Test accuracy: 0.46630001068115234\n",
      "rate=0.75:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1750 1000 750\n",
      "x_true_pseudo.shape:  (1750, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1750, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.608844658660889\n",
      "Test accuracy: 0.4587000012397766\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.901020293426513\n",
      "Test accuracy: 0.33230000734329224\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.0285105308532714\n",
      "Test accuracy: 0.43779999017715454\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.6608316310882567\n",
      "Test accuracy: 0.4767000079154968\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.39914486579895\n",
      "Test accuracy: 0.4560000002384186\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.422307764816284\n",
      "Test accuracy: 0.41769999265670776\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 4.071157147979736\n",
      "Test accuracy: 0.38769999146461487\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.1889778007507323\n",
      "Test accuracy: 0.43630000948905945\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 9.847624601745606\n",
      "Test accuracy: 0.30390000343322754\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.5097222789764406\n",
      "Test accuracy: 0.391400009393692\n",
      "rate=1.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  2000 1000 1000\n",
      "x_true_pseudo.shape:  (2000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (2000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.0895423715591432\n",
      "Test accuracy: 0.4966000020503998\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.848318020248413\n",
      "Test accuracy: 0.46650001406669617\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.664232674407959\n",
      "Test accuracy: 0.4724999964237213\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.2760581783294676\n",
      "Test accuracy: 0.4537000060081482\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.5289967891693115\n",
      "Test accuracy: 0.4223000109195709\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.3408109279632567\n",
      "Test accuracy: 0.4309000074863434\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.8568243644714357\n",
      "Test accuracy: 0.4661000072956085\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.9695994663238525\n",
      "Test accuracy: 0.42309999465942383\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.3007731227874757\n",
      "Test accuracy: 0.43560001254081726\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 5.278859498214722\n",
      "Test accuracy: 0.37610000371932983\n",
      "rate=2.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  3500 1000 2500\n",
      "x_true_pseudo.shape:  (3500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (3500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.897322804260254\n",
      "Test accuracy: 0.46799999475479126\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.3100387035369874\n",
      "Test accuracy: 0.45829999446868896\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.87477057800293\n",
      "Test accuracy: 0.38679999113082886\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.1596219314575196\n",
      "Test accuracy: 0.4278999865055084\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.469293669891357\n",
      "Test accuracy: 0.38580000400543213\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.5969381969451906\n",
      "Test accuracy: 0.4269999861717224\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 4.1496482154846195\n",
      "Test accuracy: 0.4066999852657318\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.4384208335876463\n",
      "Test accuracy: 0.43070000410079956\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.766583506011963\n",
      "Test accuracy: 0.3862999975681305\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.9860696113586425\n",
      "Test accuracy: 0.4023999869823456\n",
      "rate=5.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  6000 1000 5000\n",
      "x_true_pseudo.shape:  (6000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (6000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.277988967514038\n",
      "Test accuracy: 0.4032000005245209\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.4809795028686525\n",
      "Test accuracy: 0.4115999937057495\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.6826818332672118\n",
      "Test accuracy: 0.41839998960494995\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.763972706604004\n",
      "Test accuracy: 0.42239999771118164\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 5.29803790435791\n",
      "Test accuracy: 0.36899998784065247\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.506512052154541\n",
      "Test accuracy: 0.4009999930858612\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 5.762349367523194\n",
      "Test accuracy: 0.35499998927116394\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 5.990585158538819\n",
      "Test accuracy: 0.33899998664855957\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.989396176147461\n",
      "Test accuracy: 0.375\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 6.921728421783447\n",
      "Test accuracy: 0.31139999628067017\n",
      "rate=7.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  8500 1000 7500\n",
      "x_true_pseudo.shape:  (8500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (8500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.2824546924591065\n",
      "Test accuracy: 0.5023000240325928\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.5848648990631102\n",
      "Test accuracy: 0.49720001220703125\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.4011064228057863\n",
      "Test accuracy: 0.48489999771118164\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.9939735038757322\n",
      "Test accuracy: 0.47769999504089355\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.314062834167481\n",
      "Test accuracy: 0.4410000145435333\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.919294115447998\n",
      "Test accuracy: 0.44350001215934753\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 5.061146203994751\n",
      "Test accuracy: 0.4065000116825104\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 4.098942037200928\n",
      "Test accuracy: 0.444599986076355\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 5.341108612823486\n",
      "Test accuracy: 0.4214000105857849\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.4315073204040525\n",
      "Test accuracy: 0.4284000098705292\n",
      "rate=10.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  11000 1000 10000\n",
      "x_true_pseudo.shape:  (11000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (11000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.164771518325806\n",
      "Test accuracy: 0.5329999923706055\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.8720379917144774\n",
      "Test accuracy: 0.48820000886917114\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.60157433052063\n",
      "Test accuracy: 0.4505000114440918\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.4998786178588865\n",
      "Test accuracy: 0.4684000015258789\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.109525590515137\n",
      "Test accuracy: 0.4251999855041504\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.532934372711182\n",
      "Test accuracy: 0.4147000014781952\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 6.651201003265381\n",
      "Test accuracy: 0.3849000036716461\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 4.768613976287842\n",
      "Test accuracy: 0.4020000100135803\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.778811540222168\n",
      "Test accuracy: 0.41260001063346863\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.9849585441589355\n",
      "Test accuracy: 0.3869999945163727\n",
      "rate=15.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  16000 1000 15000\n",
      "x_true_pseudo.shape:  (16000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (16000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.086513357543945\n",
      "Test accuracy: 0.45509999990463257\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.404883170700073\n",
      "Test accuracy: 0.46059998869895935\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.3664108154296875\n",
      "Test accuracy: 0.42289999127388\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.092649134063721\n",
      "Test accuracy: 0.3937000036239624\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 6.038397034454346\n",
      "Test accuracy: 0.3889000117778778\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 7.092181885528564\n",
      "Test accuracy: 0.362199991941452\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 4.771481267547608\n",
      "Test accuracy: 0.40799999237060547\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 8.167428671264648\n",
      "Test accuracy: 0.32519999146461487\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 5.911196309661865\n",
      "Test accuracy: 0.37470000982284546\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 5.93229736404419\n",
      "Test accuracy: 0.3799000084400177\n",
      "rate=20.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  21000 1000 20000\n",
      "x_true_pseudo.shape:  (21000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (21000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.2284860805511473\n",
      "Test accuracy: 0.5131000280380249\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.5938151893615724\n",
      "Test accuracy: 0.47929999232292175\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.375537422943115\n",
      "Test accuracy: 0.48989999294281006\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.515990002441406\n",
      "Test accuracy: 0.4652999937534332\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.7120470722198484\n",
      "Test accuracy: 0.4805000126361847\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.059543490600586\n",
      "Test accuracy: 0.44690001010894775\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.8132181789398194\n",
      "Test accuracy: 0.4675999879837036\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.838999429321289\n",
      "Test accuracy: 0.46790000796318054\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.294735181427002\n",
      "Test accuracy: 0.44589999318122864\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.140070071411133\n",
      "Test accuracy: 0.4577000141143799\n",
      "rate=30.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  31000 1000 30000\n",
      "x_true_pseudo.shape:  (31000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (31000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.6553955158233644\n",
      "Test accuracy: 0.4726000130176544\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.208248189163208\n",
      "Test accuracy: 0.4699999988079071\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.5163112930297853\n",
      "Test accuracy: 0.47279998660087585\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.476985664367676\n",
      "Test accuracy: 0.4781000018119812\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.6439938972473143\n",
      "Test accuracy: 0.4787999987602234\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.395321820831299\n",
      "Test accuracy: 0.4641999900341034\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.871654203414917\n",
      "Test accuracy: 0.48179998993873596\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 4.131107922363281\n",
      "Test accuracy: 0.46799999475479126\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.811727893829346\n",
      "Test accuracy: 0.4462999999523163\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.334255548095703\n",
      "Test accuracy: 0.4749999940395355\n",
      "rate=49.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  50000 1000 49000\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.388376324081421\n",
      "Test accuracy: 0.5095999836921692\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.503294016265869\n",
      "Test accuracy: 0.447299987077713\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.6470924884796143\n",
      "Test accuracy: 0.484499990940094\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 4.325317747879028\n",
      "Test accuracy: 0.45890000462532043\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.388334260559082\n",
      "Test accuracy: 0.4553000032901764\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.382046965789795\n",
      "Test accuracy: 0.460099995136261\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 4.239982390975952\n",
      "Test accuracy: 0.45509999990463257\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 5.371167018890381\n",
      "Test accuracy: 0.43959999084472656\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.466728690338135\n",
      "Test accuracy: 0.45419999957084656\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.679674021148681\n",
      "Test accuracy: 0.44920000433921814\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Running Test2 for CIFAR10\n",
    "\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_cifar10_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate10=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,10,15,20,30,49])\n",
    "model_name= 'keras_cifar10_trained_teacher_1000_da.h5'\n",
    "accuracy_10 = stns_5000(init,model_name,x_train,y_train,x_test,y_test,rate10,teacher,\n",
    "                       student, sample_size=1000, epochs=100,\n",
    "                       data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Test2 for CIFAR10\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_cifar10_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate3=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,9])\n",
    "model_name= 'keras_cifar10_trained_teacher_5000_nda.h5'\n",
    "accuracy_3 = stns_5000(init, model_name, x_train,y_train,x_test,y_test,rate3,teacher, student, sample_size=5000, epochs=100,\n",
    "                      data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "ResNet20v1\n",
      "50000\n",
      "(50000, 10)\n",
      "(50000,)\n",
      "0: 5000\n",
      "1: 5000\n",
      "2: 5000\n",
      "3: 5000\n",
      "4: 5000\n",
      "5: 5000\n",
      "6: 5000\n",
      "7: 5000\n",
      "8: 5000\n",
      "9: 5000\n",
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "0: 100\n",
      "1: 100\n",
      "2: 100\n",
      "3: 100\n",
      "4: 100\n",
      "5: 100\n",
      "6: 100\n",
      "7: 100\n",
      "8: 100\n",
      "9: 100\n",
      "Learning rate:  0.001\n",
      "Not using data augmentation.\n",
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.5307 - accuracy: 0.2100 - val_loss: 2.4702 - val_accuracy: 0.1903\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      " 160/1000 [===>..........................] - ETA: 1s - loss: 1.9660 - accuracy: 0.3187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.8598 - accuracy: 0.3670 - val_loss: 2.2779 - val_accuracy: 0.2193\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.6578 - accuracy: 0.4680 - val_loss: 2.8549 - val_accuracy: 0.1821\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.3970 - accuracy: 0.5670 - val_loss: 2.3334 - val_accuracy: 0.2442\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 1.1043 - accuracy: 0.7100 - val_loss: 2.9942 - val_accuracy: 0.2216\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.9137 - accuracy: 0.7480 - val_loss: 3.1029 - val_accuracy: 0.2133\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.7673 - accuracy: 0.8010 - val_loss: 3.9440 - val_accuracy: 0.2292\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.4938 - accuracy: 0.9070 - val_loss: 3.1842 - val_accuracy: 0.2751\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3682 - accuracy: 0.9580 - val_loss: 4.1506 - val_accuracy: 0.2170\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3485 - accuracy: 0.9520 - val_loss: 4.2171 - val_accuracy: 0.2463\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3913 - accuracy: 0.9370 - val_loss: 3.3420 - val_accuracy: 0.2634\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3508 - accuracy: 0.9500 - val_loss: 7.3257 - val_accuracy: 0.1627\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3680 - accuracy: 0.9390 - val_loss: 4.8986 - val_accuracy: 0.2109\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3060 - accuracy: 0.9640 - val_loss: 5.4862 - val_accuracy: 0.1736\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2589 - accuracy: 0.9820 - val_loss: 4.7457 - val_accuracy: 0.2474\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2376 - accuracy: 0.9820 - val_loss: 3.9639 - val_accuracy: 0.2690\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3254 - accuracy: 0.9590 - val_loss: 3.6880 - val_accuracy: 0.2523\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3282 - accuracy: 0.9530 - val_loss: 4.7860 - val_accuracy: 0.2343\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2503 - accuracy: 0.9790 - val_loss: 4.6772 - val_accuracy: 0.2303\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2353 - accuracy: 0.9790 - val_loss: 4.2151 - val_accuracy: 0.2589\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2126 - accuracy: 0.9870 - val_loss: 4.4066 - val_accuracy: 0.2875\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1929 - accuracy: 0.9960 - val_loss: 4.0093 - val_accuracy: 0.2767\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1852 - accuracy: 0.9970 - val_loss: 3.9636 - val_accuracy: 0.2783\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1752 - accuracy: 0.9990 - val_loss: 3.1760 - val_accuracy: 0.3255\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1741 - accuracy: 0.9990 - val_loss: 3.4415 - val_accuracy: 0.2942\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1719 - accuracy: 0.9990 - val_loss: 3.1289 - val_accuracy: 0.3302\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1910 - accuracy: 0.9930 - val_loss: 4.3332 - val_accuracy: 0.2830\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1879 - accuracy: 0.9930 - val_loss: 4.2411 - val_accuracy: 0.2867\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1992 - accuracy: 0.9920 - val_loss: 5.0562 - val_accuracy: 0.2450\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3175 - accuracy: 0.9540 - val_loss: 6.0900 - val_accuracy: 0.2592\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.8016 - accuracy: 0.7870 - val_loss: 18.6619 - val_accuracy: 0.1215\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5514 - accuracy: 0.8730 - val_loss: 4.7074 - val_accuracy: 0.2647\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3138 - accuracy: 0.9600 - val_loss: 5.9877 - val_accuracy: 0.2483\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2157 - accuracy: 0.9920 - val_loss: 3.6478 - val_accuracy: 0.3061\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1919 - accuracy: 0.9970 - val_loss: 3.1748 - val_accuracy: 0.3245\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1883 - accuracy: 0.9950 - val_loss: 3.2172 - val_accuracy: 0.3187\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1811 - accuracy: 0.9980 - val_loss: 3.4058 - val_accuracy: 0.3336\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1774 - accuracy: 0.9980 - val_loss: 3.4156 - val_accuracy: 0.3194\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1738 - accuracy: 0.9990 - val_loss: 3.3399 - val_accuracy: 0.3172\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1696 - accuracy: 0.9990 - val_loss: 3.0714 - val_accuracy: 0.3190\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1678 - accuracy: 1.0000 - val_loss: 3.1409 - val_accuracy: 0.3294\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1678 - accuracy: 0.9990 - val_loss: 3.0546 - val_accuracy: 0.3342\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1946 - accuracy: 0.9910 - val_loss: 4.8054 - val_accuracy: 0.2653\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1807 - accuracy: 0.9980 - val_loss: 4.8012 - val_accuracy: 0.2244\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1839 - accuracy: 0.9940 - val_loss: 4.1764 - val_accuracy: 0.2665\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1740 - accuracy: 1.0000 - val_loss: 3.5730 - val_accuracy: 0.3140\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1671 - accuracy: 0.9990 - val_loss: 3.6589 - val_accuracy: 0.2951\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1784 - accuracy: 0.9950 - val_loss: 4.2411 - val_accuracy: 0.2696\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2325 - accuracy: 0.9700 - val_loss: 5.7858 - val_accuracy: 0.2650\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2885 - accuracy: 0.9560 - val_loss: 7.5793 - val_accuracy: 0.2559\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2735 - accuracy: 0.9630 - val_loss: 6.8397 - val_accuracy: 0.2425\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2215 - accuracy: 0.9840 - val_loss: 5.3618 - val_accuracy: 0.2710\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2209 - accuracy: 0.9830 - val_loss: 7.8409 - val_accuracy: 0.2135\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2675 - accuracy: 0.9670 - val_loss: 7.8232 - val_accuracy: 0.1945\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2381 - accuracy: 0.9790 - val_loss: 7.0008 - val_accuracy: 0.2277\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3292 - accuracy: 0.9410 - val_loss: 9.4095 - val_accuracy: 0.2236\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.3524 - accuracy: 0.9480 - val_loss: 6.6710 - val_accuracy: 0.2601\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2644 - accuracy: 0.9710 - val_loss: 5.8326 - val_accuracy: 0.2792\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2171 - accuracy: 0.9850 - val_loss: 5.0475 - val_accuracy: 0.2715\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2003 - accuracy: 0.9870 - val_loss: 4.0715 - val_accuracy: 0.3222\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2046 - accuracy: 0.9910 - val_loss: 3.7924 - val_accuracy: 0.3095\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2377 - accuracy: 0.9780 - val_loss: 5.5376 - val_accuracy: 0.2021\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2046 - accuracy: 0.9880 - val_loss: 4.0817 - val_accuracy: 0.3202\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1797 - accuracy: 0.9960 - val_loss: 3.9985 - val_accuracy: 0.2963\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1819 - accuracy: 0.9970 - val_loss: 4.3058 - val_accuracy: 0.2810\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1861 - accuracy: 0.9950 - val_loss: 5.5099 - val_accuracy: 0.2802\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2161 - accuracy: 0.9850 - val_loss: 5.1968 - val_accuracy: 0.2739\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1889 - accuracy: 0.9930 - val_loss: 5.9112 - val_accuracy: 0.2764\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1818 - accuracy: 0.9960 - val_loss: 3.7317 - val_accuracy: 0.3265\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2645 - accuracy: 0.9630 - val_loss: 6.5742 - val_accuracy: 0.2209\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2379 - accuracy: 0.9750 - val_loss: 4.5252 - val_accuracy: 0.2682\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2150 - accuracy: 0.9860 - val_loss: 5.4670 - val_accuracy: 0.2580\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1992 - accuracy: 0.9870 - val_loss: 4.7648 - val_accuracy: 0.2594\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1875 - accuracy: 0.9930 - val_loss: 5.0615 - val_accuracy: 0.2221\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1682 - accuracy: 1.0000 - val_loss: 4.3976 - val_accuracy: 0.2490\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1682 - accuracy: 0.9980 - val_loss: 4.6638 - val_accuracy: 0.2642\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1672 - accuracy: 0.9980 - val_loss: 4.5352 - val_accuracy: 0.2829\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2103 - accuracy: 0.9840 - val_loss: 7.8430 - val_accuracy: 0.2053\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1879 - accuracy: 0.9930 - val_loss: 4.1662 - val_accuracy: 0.3219\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1906 - accuracy: 0.9900 - val_loss: 5.5113 - val_accuracy: 0.2565\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1872 - accuracy: 0.9950 - val_loss: 4.2332 - val_accuracy: 0.2986\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1774 - accuracy: 0.9960 - val_loss: 3.8008 - val_accuracy: 0.3180\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1691 - accuracy: 0.9990 - val_loss: 3.6669 - val_accuracy: 0.3297\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1689 - accuracy: 0.9990 - val_loss: 3.5351 - val_accuracy: 0.3317\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1639 - accuracy: 1.0000 - val_loss: 3.4498 - val_accuracy: 0.3374\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1667 - accuracy: 0.9990 - val_loss: 3.4444 - val_accuracy: 0.3374\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1630 - accuracy: 1.0000 - val_loss: 3.4372 - val_accuracy: 0.3385\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1631 - accuracy: 1.0000 - val_loss: 3.4267 - val_accuracy: 0.3371\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1618 - accuracy: 1.0000 - val_loss: 3.4207 - val_accuracy: 0.3362\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1629 - accuracy: 1.0000 - val_loss: 3.4174 - val_accuracy: 0.3361\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1636 - accuracy: 1.0000 - val_loss: 3.5339 - val_accuracy: 0.3368\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1627 - accuracy: 0.9990 - val_loss: 3.5019 - val_accuracy: 0.3365\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1638 - accuracy: 0.9990 - val_loss: 3.4505 - val_accuracy: 0.3413\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1635 - accuracy: 0.9990 - val_loss: 3.4550 - val_accuracy: 0.3372\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1639 - accuracy: 0.9990 - val_loss: 3.4207 - val_accuracy: 0.3390\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1619 - accuracy: 1.0000 - val_loss: 3.4325 - val_accuracy: 0.3321\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1620 - accuracy: 1.0000 - val_loss: 3.4109 - val_accuracy: 0.3370\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1657 - accuracy: 0.9980 - val_loss: 3.4061 - val_accuracy: 0.3392\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1621 - accuracy: 1.0000 - val_loss: 3.4634 - val_accuracy: 0.3292\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.1623 - accuracy: 1.0000 - val_loss: 3.4327 - val_accuracy: 0.3290\n",
      "10000/10000 [==============================] - 4s 357us/step\n",
      "Supervised learning model with 100epochs \n",
      "\n",
      "Test loss: 3.4327112979888916\n",
      "Test accuracy: 0.32899999618530273\n",
      "Saved trained model at /home/ANDURAND/pandurand/saved_models/keras_cifar10_trained_teacher_1000_nda.h5 \n",
      "Small sample of 5000 training images, Supervised learning model with 100epochs \n",
      "\n",
      "Test accuracy: 0.32899999618530273\n",
      "size x_total_unlabeled (should be 45,000):  49000\n",
      "rate=0.05:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1050 1000 50\n",
      "x_true_pseudo.shape:  (1050, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1050, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 4.727610631561279\n",
      "Test accuracy: 0.2648000121116638\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.7887344821929934\n",
      "Test accuracy: 0.305400013923645\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.907056542205811\n",
      "Test accuracy: 0.30250000953674316\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 6.847062505340576\n",
      "Test accuracy: 0.2305999994277954\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 5.152142262268066\n",
      "Test accuracy: 0.27090001106262207\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 5.86144831161499\n",
      "Test accuracy: 0.22370000183582306\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.389260958862305\n",
      "Test accuracy: 0.3310000002384186\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.715659861755371\n",
      "Test accuracy: 0.30309998989105225\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 9.425012149047852\n",
      "Test accuracy: 0.19850000739097595\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.353059980392456\n",
      "Test accuracy: 0.3224000036716461\n",
      "rate=0.1:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1100 1000 100\n",
      "x_true_pseudo.shape:  (1100, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1100, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 4.806207247924805\n",
      "Test accuracy: 0.2791000008583069\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.922525926208496\n",
      "Test accuracy: 0.2953000068664551\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.605714003753662\n",
      "Test accuracy: 0.26759999990463257\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.37667081413269\n",
      "Test accuracy: 0.2694000005722046\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.056266507720947\n",
      "Test accuracy: 0.29089999198913574\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 5.984348673248291\n",
      "Test accuracy: 0.2540999948978424\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.3614226123809816\n",
      "Test accuracy: 0.3253999948501587\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.639395381164551\n",
      "Test accuracy: 0.29490000009536743\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 7.526533556365967\n",
      "Test accuracy: 0.2587999999523163\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.5885510665893556\n",
      "Test accuracy: 0.3142000138759613\n",
      "rate=0.2:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1200 1000 200\n",
      "x_true_pseudo.shape:  (1200, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1200, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.250130242538452\n",
      "Test accuracy: 0.32589998841285706\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.319573656463623\n",
      "Test accuracy: 0.2615000009536743\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.1173495262146\n",
      "Test accuracy: 0.29179999232292175\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 4.768555037689209\n",
      "Test accuracy: 0.25369998812675476\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.396119678115845\n",
      "Test accuracy: 0.3118000030517578\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 6.693378981018067\n",
      "Test accuracy: 0.24220000207424164\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.3938470512390135\n",
      "Test accuracy: 0.32269999384880066\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.320986675643921\n",
      "Test accuracy: 0.3280999958515167\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 5.2767922386169435\n",
      "Test accuracy: 0.2897999882698059\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.379788388824463\n",
      "Test accuracy: 0.32089999318122864\n",
      "rate=0.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1500 1000 500\n",
      "x_true_pseudo.shape:  (1500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.740686022567749\n",
      "Test accuracy: 0.26589998602867126\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.932464900970459\n",
      "Test accuracy: 0.26820001006126404\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.563642373657227\n",
      "Test accuracy: 0.2874999940395355\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.8465150497436524\n",
      "Test accuracy: 0.2985999882221222\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 6.553613708496094\n",
      "Test accuracy: 0.23319999873638153\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.4787179141998292\n",
      "Test accuracy: 0.2962999939918518\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 4.4728533668518065\n",
      "Test accuracy: 0.29269999265670776\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 4.01906933517456\n",
      "Test accuracy: 0.2896000146865845\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.832976865386963\n",
      "Test accuracy: 0.3077000081539154\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.806534496307373\n",
      "Test accuracy: 0.2639000117778778\n",
      "rate=0.75:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1750 1000 750\n",
      "x_true_pseudo.shape:  (1750, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1750, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.951466192626953\n",
      "Test accuracy: 0.25999999046325684\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.168371824645996\n",
      "Test accuracy: 0.31679999828338623\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 5.206682781982422\n",
      "Test accuracy: 0.2492000013589859\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.6210004943847656\n",
      "Test accuracy: 0.2858999967575073\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 5.838728885650635\n",
      "Test accuracy: 0.2567000091075897\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.032825225067139\n",
      "Test accuracy: 0.2863999903202057\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 10.240605592346192\n",
      "Test accuracy: 0.1949000060558319\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.8457896423339846\n",
      "Test accuracy: 0.2678000032901764\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 5.069139729309082\n",
      "Test accuracy: 0.2290000021457672\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.7984097259521485\n",
      "Test accuracy: 0.298799991607666\n",
      "rate=1.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  2000 1000 1000\n",
      "x_true_pseudo.shape:  (2000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (2000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.4089793533325197\n",
      "Test accuracy: 0.3091999888420105\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.74771951713562\n",
      "Test accuracy: 0.3018999993801117\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.6550720947265627\n",
      "Test accuracy: 0.29120001196861267\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.183862635803223\n",
      "Test accuracy: 0.2806999981403351\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.181187109375\n",
      "Test accuracy: 0.301800012588501\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.481917869567871\n",
      "Test accuracy: 0.2842999994754791\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.865934935760498\n",
      "Test accuracy: 0.3107999861240387\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.9860462730407713\n",
      "Test accuracy: 0.30250000953674316\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.027270548248291\n",
      "Test accuracy: 0.30149999260902405\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.8239375480651856\n",
      "Test accuracy: 0.31209999322891235\n",
      "rate=2.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  3500 1000 2500\n",
      "x_true_pseudo.shape:  (3500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (3500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 4.287212406921387\n",
      "Test accuracy: 0.263700008392334\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.428827172088623\n",
      "Test accuracy: 0.2468000054359436\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.4571065338134765\n",
      "Test accuracy: 0.28060001134872437\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.080630583190918\n",
      "Test accuracy: 0.23839999735355377\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.581385438537597\n",
      "Test accuracy: 0.2599000036716461\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 5.570770803833008\n",
      "Test accuracy: 0.2231999933719635\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 5.201823124694824\n",
      "Test accuracy: 0.2526000142097473\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 4.52163994064331\n",
      "Test accuracy: 0.26510000228881836\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 5.382646649932862\n",
      "Test accuracy: 0.24740000069141388\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 6.102319649505615\n",
      "Test accuracy: 0.22779999673366547\n",
      "rate=5.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  6000 1000 5000\n",
      "x_true_pseudo.shape:  (6000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (6000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.308086640930176\n",
      "Test accuracy: 0.32100000977516174\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.538840838623047\n",
      "Test accuracy: 0.3028999865055084\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 5.37392663269043\n",
      "Test accuracy: 0.28700000047683716\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.313365345001221\n",
      "Test accuracy: 0.2883000075817108\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.881676551055908\n",
      "Test accuracy: 0.2842000126838684\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 5.378397882843018\n",
      "Test accuracy: 0.263700008392334\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 5.479086519622803\n",
      "Test accuracy: 0.26190000772476196\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 6.049109423828125\n",
      "Test accuracy: 0.2565999925136566\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 6.030221743774414\n",
      "Test accuracy: 0.25929999351501465\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 5.474244795227051\n",
      "Test accuracy: 0.26080000400543213\n",
      "rate=7.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  8500 1000 7500\n",
      "x_true_pseudo.shape:  (8500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (8500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.8056572265625\n",
      "Test accuracy: 0.28999999165534973\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.868926762390137\n",
      "Test accuracy: 0.28850001096725464\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 5.411183393096924\n",
      "Test accuracy: 0.2953000068664551\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.4327499954223635\n",
      "Test accuracy: 0.28540000319480896\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 5.81248081817627\n",
      "Test accuracy: 0.2786000072956085\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 8.364450415039062\n",
      "Test accuracy: 0.23839999735355377\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 5.744064689636231\n",
      "Test accuracy: 0.27459999918937683\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 7.246490068054199\n",
      "Test accuracy: 0.23880000412464142\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 5.849722660064697\n",
      "Test accuracy: 0.2736000120639801\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 5.425507955932617\n",
      "Test accuracy: 0.2872999906539917\n",
      "rate=10.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  11000 1000 10000\n",
      "x_true_pseudo.shape:  (11000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (11000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.5692345596313477\n",
      "Test accuracy: 0.3118000030517578\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.626393281936646\n",
      "Test accuracy: 0.2903999984264374\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 5.231984827423096\n",
      "Test accuracy: 0.28679999709129333\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.664307776260376\n",
      "Test accuracy: 0.27630001306533813\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 5.527053489685058\n",
      "Test accuracy: 0.2757999897003174\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 5.923850079345703\n",
      "Test accuracy: 0.27000001072883606\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 7.172089922332764\n",
      "Test accuracy: 0.25699999928474426\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 5.788966703796387\n",
      "Test accuracy: 0.28040000796318054\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 6.527112554931641\n",
      "Test accuracy: 0.25949999690055847\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 6.183628884124756\n",
      "Test accuracy: 0.2612000107765198\n",
      "rate=15.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  16000 1000 15000\n",
      "x_true_pseudo.shape:  (16000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (16000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.728297777557373\n",
      "Test accuracy: 0.3018999993801117\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.9552578971862795\n",
      "Test accuracy: 0.2879999876022339\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 5.750795424652099\n",
      "Test accuracy: 0.27459999918937683\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.110659852600097\n",
      "Test accuracy: 0.2937000095844269\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 5.259346477508545\n",
      "Test accuracy: 0.2969000041484833\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 5.4388263832092285\n",
      "Test accuracy: 0.28610000014305115\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 6.121986181640625\n",
      "Test accuracy: 0.2784999907016754\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 5.920564832687378\n",
      "Test accuracy: 0.2851000130176544\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 6.7217166694641115\n",
      "Test accuracy: 0.26600000262260437\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 6.351849284362793\n",
      "Test accuracy: 0.275299996137619\n",
      "rate=20.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  21000 1000 20000\n",
      "x_true_pseudo.shape:  (21000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (21000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.4248121070861814\n",
      "Test accuracy: 0.3140999972820282\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.470525995635986\n",
      "Test accuracy: 0.3165000081062317\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.812612549591065\n",
      "Test accuracy: 0.32429999113082886\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 5.122973820495606\n",
      "Test accuracy: 0.3215999901294708\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 5.053739402008056\n",
      "Test accuracy: 0.310699999332428\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 6.2779203750610355\n",
      "Test accuracy: 0.28200000524520874\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 6.4611283248901366\n",
      "Test accuracy: 0.2596000134944916\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 5.930280465698242\n",
      "Test accuracy: 0.29280000925064087\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 6.4833262901306155\n",
      "Test accuracy: 0.2849000096321106\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 6.178670478820801\n",
      "Test accuracy: 0.2842999994754791\n",
      "rate=30.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  31000 1000 30000\n",
      "x_true_pseudo.shape:  (31000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (31000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.1910296829223634\n",
      "Test accuracy: 0.32120001316070557\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 4.521927200317383\n",
      "Test accuracy: 0.2842999994754791\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 5.206434442138672\n",
      "Test accuracy: 0.3093999922275543\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 4.241616703033447\n",
      "Test accuracy: 0.32749998569488525\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.8611953170776365\n",
      "Test accuracy: 0.3174999952316284\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.440674334716797\n",
      "Test accuracy: 0.34049999713897705\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 5.044509173583984\n",
      "Test accuracy: 0.32440000772476196\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 5.350504523468017\n",
      "Test accuracy: 0.3084999918937683\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 6.035920384979248\n",
      "Test accuracy: 0.3073999881744385\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 5.572812176513672\n",
      "Test accuracy: 0.30160000920295715\n",
      "rate=49.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  50000 1000 49000\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.646610298919678\n",
      "Test accuracy: 0.3447999954223633\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.9952990676879883\n",
      "Test accuracy: 0.3212999999523163\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 4.732820283508301\n",
      "Test accuracy: 0.3109000027179718\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 4.5381894912719725\n",
      "Test accuracy: 0.334199994802475\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.983964590454102\n",
      "Test accuracy: 0.33160001039505005\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 4.670548073577881\n",
      "Test accuracy: 0.33660000562667847\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 5.320996913909912\n",
      "Test accuracy: 0.3190000057220459\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 5.063263104248047\n",
      "Test accuracy: 0.3427000045776367\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.824523351287842\n",
      "Test accuracy: 0.35010001063346863\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 5.339835684204101\n",
      "Test accuracy: 0.3379000127315521\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Running Test2 for CIFAR10\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_cifar10_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate11=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,10,15,20,30,49])\n",
    "model_name= 'keras_cifar10_trained_teacher_1000_nda.h5'\n",
    "accuracy_11 = stns_5000(init, model_name, x_train,y_train,x_test,y_test,rate11,teacher, student, sample_size=1000, epochs=100,\n",
    "                       data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Test2 for HouseNumbers\n",
    "\n",
    "(x_train_house,y_train_house),(x_test_house,y_test_house) = prepare_house_numbers_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate4=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,9])\n",
    "model_name= 'keras_HN_trained_teacher_5000_da.h5'\n",
    "accuracy_4 = stns_5000(init, model_name, x_train_house,y_train_house,x_test_house,y_test_house,rate4,teacher, student, sample_size=5000, \n",
    "                      epochs=100,\n",
    "                      data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Test2 for HouseNumbers\n",
    "\n",
    "(x_train_house,y_train_house),(x_test_house,y_test_house) = prepare_house_numbers_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate12=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,10,15,20,30,49])\n",
    "model_name= 'keras_HN_trained_teacher_1000_da.h5'\n",
    "accuracy_12 = stns_5000(init, model_name, x_train_house,y_train_house,x_test_house,y_test_house,rate12,teacher, student, sample_size=1000,\n",
    "                       epochs=100,\n",
    "                      data_augmentation=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train is: (32, 32, 3, 73257)\n",
      "Shape of y_train is: (73257, 1)\n",
      "Shape of x_test is: (32, 32, 3, 26032)\n",
      "Shape of y_test is: (26032, 1)\n",
      "Shape of x_train is now: (73257, 32, 32, 3)\n",
      "Shape of x_test is now: (26032, 32, 32, 3)\n",
      "Shape of y_train is now: (73257,)\n",
      "Shape of y_test is now: (26032,)\n",
      "labels of y_train are [0 1 2 3 4 5 6 7 8 9]\n",
      "labels of y_test are [0 1 2 3 4 5 6 7 8 9]\n",
      "Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "ResNet20v1\n",
      "73257\n",
      "(73257, 10)\n",
      "(73257,)\n",
      "0: 4948\n",
      "1: 13861\n",
      "2: 10585\n",
      "3: 8497\n",
      "4: 7458\n",
      "5: 6882\n",
      "6: 5727\n",
      "7: 5595\n",
      "8: 5045\n",
      "9: 4659\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n",
      "0: 500\n",
      "1: 500\n",
      "2: 500\n",
      "3: 500\n",
      "4: 500\n",
      "5: 500\n",
      "6: 500\n",
      "7: 500\n",
      "8: 500\n",
      "9: 500\n",
      "Learning rate:  0.001\n",
      "Not using data augmentation.\n",
      "Train on 5000 samples, validate on 26032 samples\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 22s 4ms/step - loss: 2.4486 - accuracy: 0.1850 - val_loss: 2.4188 - val_accuracy: 0.2140\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      "  96/5000 [..............................] - ETA: 6s - loss: 1.8497 - accuracy: 0.4271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.5969 - accuracy: 0.5212 - val_loss: 1.7052 - val_accuracy: 0.4892\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 1.0012 - accuracy: 0.7366 - val_loss: 1.3873 - val_accuracy: 0.6015\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.7325 - accuracy: 0.8242 - val_loss: 1.0914 - val_accuracy: 0.7124\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.5937 - accuracy: 0.8702 - val_loss: 1.2004 - val_accuracy: 0.6822\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.4911 - accuracy: 0.8990 - val_loss: 0.8001 - val_accuracy: 0.7956\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.4151 - accuracy: 0.9226 - val_loss: 1.0075 - val_accuracy: 0.7367\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.3818 - accuracy: 0.9350 - val_loss: 0.7036 - val_accuracy: 0.8360\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.3194 - accuracy: 0.9550 - val_loss: 1.3924 - val_accuracy: 0.6429\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2721 - accuracy: 0.9680 - val_loss: 1.4433 - val_accuracy: 0.6676\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2476 - accuracy: 0.9772 - val_loss: 0.9512 - val_accuracy: 0.7759\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2826 - accuracy: 0.9626 - val_loss: 0.7998 - val_accuracy: 0.8176\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2740 - accuracy: 0.9656 - val_loss: 1.5725 - val_accuracy: 0.6987\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2714 - accuracy: 0.9698 - val_loss: 1.0925 - val_accuracy: 0.7530\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2330 - accuracy: 0.9824 - val_loss: 1.2788 - val_accuracy: 0.7216\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2291 - accuracy: 0.9854 - val_loss: 0.7820 - val_accuracy: 0.8259\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2073 - accuracy: 0.9894 - val_loss: 1.4995 - val_accuracy: 0.6785\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1998 - accuracy: 0.9904 - val_loss: 1.0684 - val_accuracy: 0.7753\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2742 - accuracy: 0.9648 - val_loss: 2.5911 - val_accuracy: 0.5718\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2269 - accuracy: 0.9826 - val_loss: 1.3808 - val_accuracy: 0.7310\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2072 - accuracy: 0.9868 - val_loss: 0.8628 - val_accuracy: 0.8170\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2045 - accuracy: 0.9880 - val_loss: 1.6733 - val_accuracy: 0.6875\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2513 - accuracy: 0.9708 - val_loss: 1.0105 - val_accuracy: 0.7901\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2740 - accuracy: 0.9654 - val_loss: 1.5209 - val_accuracy: 0.7196\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2184 - accuracy: 0.9848 - val_loss: 0.9262 - val_accuracy: 0.8261\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1969 - accuracy: 0.9906 - val_loss: 1.1621 - val_accuracy: 0.7707\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1874 - accuracy: 0.9928 - val_loss: 0.7108 - val_accuracy: 0.8607\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1861 - accuracy: 0.9926 - val_loss: 0.9526 - val_accuracy: 0.8150\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2460 - accuracy: 0.9742 - val_loss: 1.1437 - val_accuracy: 0.7714\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2133 - accuracy: 0.9842 - val_loss: 1.1088 - val_accuracy: 0.7657\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1885 - accuracy: 0.9918 - val_loss: 0.8065 - val_accuracy: 0.8451\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1739 - accuracy: 0.9960 - val_loss: 1.0184 - val_accuracy: 0.8074\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2048 - accuracy: 0.9828 - val_loss: 2.0804 - val_accuracy: 0.6499\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2473 - accuracy: 0.9722 - val_loss: 0.8322 - val_accuracy: 0.8259\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2006 - accuracy: 0.9878 - val_loss: 0.9647 - val_accuracy: 0.8099\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1911 - accuracy: 0.9906 - val_loss: 0.9794 - val_accuracy: 0.8182\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2353 - accuracy: 0.9752 - val_loss: 1.1350 - val_accuracy: 0.7899\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1992 - accuracy: 0.9872 - val_loss: 0.8924 - val_accuracy: 0.8318\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1812 - accuracy: 0.9946 - val_loss: 0.9938 - val_accuracy: 0.8043\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1683 - accuracy: 0.9974 - val_loss: 0.9465 - val_accuracy: 0.8253\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1718 - accuracy: 0.9948 - val_loss: 0.8876 - val_accuracy: 0.8266\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2335 - accuracy: 0.9758 - val_loss: 1.8674 - val_accuracy: 0.6886\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2483 - accuracy: 0.9678 - val_loss: 0.8005 - val_accuracy: 0.8539\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2039 - accuracy: 0.9856 - val_loss: 1.0441 - val_accuracy: 0.7959\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1802 - accuracy: 0.9948 - val_loss: 0.7677 - val_accuracy: 0.8570\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1643 - accuracy: 0.9982 - val_loss: 0.7585 - val_accuracy: 0.8602\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1545 - accuracy: 1.0000 - val_loss: 0.6383 - val_accuracy: 0.8850\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1497 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 0.8868\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1451 - accuracy: 1.0000 - val_loss: 0.6441 - val_accuracy: 0.8825\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1403 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 0.8889\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1988 - accuracy: 0.9804 - val_loss: 6.9722 - val_accuracy: 0.4479\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.3188 - accuracy: 0.9450 - val_loss: 1.3934 - val_accuracy: 0.7399\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2126 - accuracy: 0.9798 - val_loss: 0.8648 - val_accuracy: 0.8205\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1927 - accuracy: 0.9874 - val_loss: 1.2014 - val_accuracy: 0.7604\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1743 - accuracy: 0.9934 - val_loss: 0.7728 - val_accuracy: 0.8555\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2117 - accuracy: 0.9824 - val_loss: 1.1123 - val_accuracy: 0.7917\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1854 - accuracy: 0.9890 - val_loss: 0.9851 - val_accuracy: 0.8170\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1811 - accuracy: 0.9916 - val_loss: 1.2643 - val_accuracy: 0.7675\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1777 - accuracy: 0.9908 - val_loss: 1.3000 - val_accuracy: 0.7546\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1782 - accuracy: 0.9896 - val_loss: 1.0007 - val_accuracy: 0.8074\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1809 - accuracy: 0.9892 - val_loss: 0.8844 - val_accuracy: 0.8437\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1695 - accuracy: 0.9930 - val_loss: 0.8795 - val_accuracy: 0.8341\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1802 - accuracy: 0.9896 - val_loss: 0.9197 - val_accuracy: 0.8334\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1896 - accuracy: 0.9870 - val_loss: 1.1932 - val_accuracy: 0.7929\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2097 - accuracy: 0.9792 - val_loss: 0.9639 - val_accuracy: 0.8087\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1716 - accuracy: 0.9942 - val_loss: 0.8103 - val_accuracy: 0.8483\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2008 - accuracy: 0.9856 - val_loss: 0.9069 - val_accuracy: 0.8375\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1912 - accuracy: 0.9868 - val_loss: 0.7894 - val_accuracy: 0.8588\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1757 - accuracy: 0.9924 - val_loss: 0.7893 - val_accuracy: 0.8625\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1614 - accuracy: 0.9972 - val_loss: 0.7350 - val_accuracy: 0.8677\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1517 - accuracy: 0.9988 - val_loss: 0.7208 - val_accuracy: 0.8698\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1525 - accuracy: 0.9970 - val_loss: 0.7950 - val_accuracy: 0.8565\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1664 - accuracy: 0.9926 - val_loss: 0.9922 - val_accuracy: 0.8259\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2087 - accuracy: 0.9800 - val_loss: 0.9837 - val_accuracy: 0.8248\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1653 - accuracy: 0.9934 - val_loss: 0.9373 - val_accuracy: 0.8304\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1701 - accuracy: 0.9910 - val_loss: 1.5800 - val_accuracy: 0.7253\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1655 - accuracy: 0.9934 - val_loss: 1.4107 - val_accuracy: 0.7508\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2345 - accuracy: 0.9728 - val_loss: 1.4212 - val_accuracy: 0.7590\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1766 - accuracy: 0.9908 - val_loss: 0.7639 - val_accuracy: 0.8617\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1566 - accuracy: 0.9974 - val_loss: 0.7759 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1548 - accuracy: 0.9970 - val_loss: 1.1902 - val_accuracy: 0.8023\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1484 - accuracy: 0.9982 - val_loss: 0.6654 - val_accuracy: 0.8826\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1448 - accuracy: 0.9992 - val_loss: 0.6346 - val_accuracy: 0.8893\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1426 - accuracy: 1.0000 - val_loss: 0.6225 - val_accuracy: 0.8919\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1418 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 0.8929\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1416 - accuracy: 0.9994 - val_loss: 0.6245 - val_accuracy: 0.8914\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1401 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 0.8932\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1394 - accuracy: 1.0000 - val_loss: 0.6197 - val_accuracy: 0.8934\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1388 - accuracy: 1.0000 - val_loss: 0.6114 - val_accuracy: 0.8951\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1382 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.8956\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1372 - accuracy: 1.0000 - val_loss: 0.6118 - val_accuracy: 0.8948\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1363 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.8947\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1355 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 0.8955\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1346 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 0.8959\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1336 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.8965\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1325 - accuracy: 1.0000 - val_loss: 0.6029 - val_accuracy: 0.8966\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1316 - accuracy: 0.9998 - val_loss: 0.6077 - val_accuracy: 0.8955\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1303 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 0.8975\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1290 - accuracy: 1.0000 - val_loss: 0.5952 - val_accuracy: 0.8979\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1277 - accuracy: 1.0000 - val_loss: 0.5957 - val_accuracy: 0.8979\n",
      "26032/26032 [==============================] - 9s 358us/step\n",
      "Supervised learning model with 100epochs \n",
      "\n",
      "Test loss: 0.5956640113477115\n",
      "Test accuracy: 0.8978949189186096\n",
      "Saved trained model at /home/ANDURAND/pandurand/saved_models/keras_HN_trained_teacher_5000_nda.h5 \n",
      "Small sample of 5000 training images, Supervised learning model with 100epochs \n",
      "\n",
      "Test accuracy: 0.8978949189186096\n",
      "size x_total_unlabeled (should be 45,000):  68257\n",
      "rate=0.05:\n",
      "\n",
      "n_total, n_true, n_pseudo:  5250 5000 250\n",
      "x_true_pseudo.shape:  (5250, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (5250, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.7850687901117938\n",
      "Test accuracy: 0.8641287684440613\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.6841855705443818\n",
      "Test accuracy: 0.8784956932067871\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.9050385011949639\n",
      "Test accuracy: 0.8470728397369385\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.6597560137675156\n",
      "Test accuracy: 0.8961662650108337\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.7103995936519175\n",
      "Test accuracy: 0.8817993402481079\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.669484856821821\n",
      "Test accuracy: 0.8914029002189636\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.6670996053275992\n",
      "Test accuracy: 0.893208384513855\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.8933240585006229\n",
      "Test accuracy: 0.8508374094963074\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.6153475421297089\n",
      "Test accuracy: 0.9008143544197083\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.9571421520433033\n",
      "Test accuracy: 0.8582513928413391\n",
      "rate=0.1:\n",
      "\n",
      "n_total, n_true, n_pseudo:  5500 5000 500\n",
      "x_true_pseudo.shape:  (5500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (5500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.4477500481223065\n",
      "Test accuracy: 0.800937294960022\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.8362546461853574\n",
      "Test accuracy: 0.8534111976623535\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.7122380652710493\n",
      "Test accuracy: 0.8749616146087646\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.7769300128025882\n",
      "Test accuracy: 0.8766902089118958\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.7604519284680056\n",
      "Test accuracy: 0.8778042197227478\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.8636837018803724\n",
      "Test accuracy: 0.8542178869247437\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.804512114157029\n",
      "Test accuracy: 0.8629763126373291\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.662214649056978\n",
      "Test accuracy: 0.889866292476654\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.7070084362581015\n",
      "Test accuracy: 0.8780347108840942\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.9883974523689385\n",
      "Test accuracy: 0.858558714389801\n",
      "rate=0.2:\n",
      "\n",
      "n_total, n_true, n_pseudo:  6000 5000 1000\n",
      "x_true_pseudo.shape:  (6000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (6000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.7813748133717392\n",
      "Test accuracy: 0.8743469715118408\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.7048270654553787\n",
      "Test accuracy: 0.8716195225715637\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.6801945220750024\n",
      "Test accuracy: 0.8815688490867615\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.8609272895521669\n",
      "Test accuracy: 0.855869710445404\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.8102288254580764\n",
      "Test accuracy: 0.8646281361579895\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.9632055862556591\n",
      "Test accuracy: 0.8502228260040283\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 1.088784555496434\n",
      "Test accuracy: 0.8322449326515198\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.9135107030037656\n",
      "Test accuracy: 0.8612092733383179\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.7173882046028669\n",
      "Test accuracy: 0.8859480619430542\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 1.2211489982413015\n",
      "Test accuracy: 0.808696985244751\n",
      "rate=0.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  7500 5000 2500\n",
      "x_true_pseudo.shape:  (7500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (7500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.7010731749470129\n",
      "Test accuracy: 0.8714274764060974\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.745048134419987\n",
      "Test accuracy: 0.8745390176773071\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.7321536090744736\n",
      "Test accuracy: 0.8686232566833496\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.9699550077969411\n",
      "Test accuracy: 0.8500307202339172\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.9074513132431496\n",
      "Test accuracy: 0.8516057133674622\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.8648008556914022\n",
      "Test accuracy: 0.8592501282691956\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 1.1076287339637818\n",
      "Test accuracy: 0.8506453633308411\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.871519705581753\n",
      "Test accuracy: 0.8705823421478271\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.9221406175669674\n",
      "Test accuracy: 0.8701213598251343\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.7094045218398546\n",
      "Test accuracy: 0.8949369788169861\n",
      "rate=0.75:\n",
      "\n",
      "n_total, n_true, n_pseudo:  8750 5000 3750\n",
      "x_true_pseudo.shape:  (8750, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (8750, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.0374721537064246\n",
      "Test accuracy: 0.8056622743606567\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.9483723007394407\n",
      "Test accuracy: 0.8383143544197083\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.836308852927994\n",
      "Test accuracy: 0.8500691652297974\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.9403999797042778\n",
      "Test accuracy: 0.8552550673484802\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.9955415567995657\n",
      "Test accuracy: 0.8390058279037476\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.9316792805904103\n",
      "Test accuracy: 0.83309006690979\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.8147909924452043\n",
      "Test accuracy: 0.8682006597518921\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.9800675741426038\n",
      "Test accuracy: 0.859634280204773\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.9851292232354872\n",
      "Test accuracy: 0.8558312654495239\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.9409402295644839\n",
      "Test accuracy: 0.8360863327980042\n",
      "rate=1.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  10000 5000 5000\n",
      "x_true_pseudo.shape:  (10000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (10000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.7968592095133272\n",
      "Test accuracy: 0.8661647439002991\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.807734853223415\n",
      "Test accuracy: 0.8730792999267578\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.772513564188347\n",
      "Test accuracy: 0.8891364336013794\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 1.5609882012714837\n",
      "Test accuracy: 0.7863783240318298\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.8778114593241504\n",
      "Test accuracy: 0.8564459085464478\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.9395772825374732\n",
      "Test accuracy: 0.8580209016799927\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.8134332003407341\n",
      "Test accuracy: 0.86236172914505\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.9073990063207166\n",
      "Test accuracy: 0.8478026986122131\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.9784374118364143\n",
      "Test accuracy: 0.8483020663261414\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.9239496353878591\n",
      "Test accuracy: 0.8682006597518921\n",
      "rate=2.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  17500 5000 12500\n",
      "x_true_pseudo.shape:  (17500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (17500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.7204404833973297\n",
      "Test accuracy: 0.8730792999267578\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.8603997627100332\n",
      "Test accuracy: 0.8726183176040649\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.7916103074374049\n",
      "Test accuracy: 0.8752304911613464\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.8129951046629779\n",
      "Test accuracy: 0.8758835196495056\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.8134871690494558\n",
      "Test accuracy: 0.878803014755249\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.8212698082580988\n",
      "Test accuracy: 0.8818377256393433\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.8227354153468701\n",
      "Test accuracy: 0.8804163932800293\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 1.023958753041131\n",
      "Test accuracy: 0.8463429808616638\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.8824401774066463\n",
      "Test accuracy: 0.8656269311904907\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 1.164575717990649\n",
      "Test accuracy: 0.8309772610664368\n",
      "rate=5.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  30000 5000 25000\n",
      "x_true_pseudo.shape:  (30000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (30000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.65608376817756\n",
      "Test accuracy: 0.884949266910553\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.7524367830800219\n",
      "Test accuracy: 0.8839120864868164\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.6743786909789781\n",
      "Test accuracy: 0.8895205855369568\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.810171319504933\n",
      "Test accuracy: 0.8752304911613464\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.9754033624283287\n",
      "Test accuracy: 0.8577135801315308\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.7618085054363586\n",
      "Test accuracy: 0.8831438422203064\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.8356951816578767\n",
      "Test accuracy: 0.8761908411979675\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.8399976182181775\n",
      "Test accuracy: 0.8779963254928589\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.9016028948737275\n",
      "Test accuracy: 0.8614782094955444\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.9436244397128222\n",
      "Test accuracy: 0.8662031292915344\n",
      "rate=7.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  42500 5000 37500\n",
      "x_true_pseudo.shape:  (42500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (42500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.5809494244700645\n",
      "Test accuracy: 0.8926321268081665\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.7318940806513413\n",
      "Test accuracy: 0.8828365206718445\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.7329243112724108\n",
      "Test accuracy: 0.8861785531044006\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.6868245737751936\n",
      "Test accuracy: 0.8962046504020691\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.7126937024455606\n",
      "Test accuracy: 0.8919022679328918\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.7953686280600777\n",
      "Test accuracy: 0.8818377256393433\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.7956391920615503\n",
      "Test accuracy: 0.8889443874359131\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.7465905976134278\n",
      "Test accuracy: 0.8904425501823425\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.8555453970487966\n",
      "Test accuracy: 0.874846339225769\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.8627841377925052\n",
      "Test accuracy: 0.8718116283416748\n",
      "rate=9.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  50000 5000 45000\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 0.6300550512946598\n",
      "Test accuracy: 0.8858712315559387\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 0.6948248803542301\n",
      "Test accuracy: 0.8825675845146179\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 0.709316407784261\n",
      "Test accuracy: 0.8845267295837402\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 0.7878189932442766\n",
      "Test accuracy: 0.8805700540542603\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 0.8100223183521993\n",
      "Test accuracy: 0.8778810501098633\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 0.7527735001409281\n",
      "Test accuracy: 0.8753457069396973\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 0.8880854451773572\n",
      "Test accuracy: 0.8693146705627441\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 0.8296667591112848\n",
      "Test accuracy: 0.8807237148284912\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 0.8420333737563559\n",
      "Test accuracy: 0.8698140978813171\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 0.8484510950464813\n",
      "Test accuracy: 0.871196985244751\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAEWCAYAAAAQKVIQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3hb5fn/8fft7ThxljPIgCQQICHsQBgdUChljwItFMoohaa/Qhf9dkEps7SUb+eXQqFQyi7QQlPK6gBaShhhk4RASEIWIc62HS/Z9++Pc+TIimzJsmyNfF7X5cuWdMat46Nz63nOM8zdERERKVRF2Q5ARESkLynRiYhIQVOiExGRgqZEJyIiBU2JTkRECpoSnYiIFDQlOpEYZvZTM1trZkv6eb+3m9m3+3Of4X6/bmarzazezKr6e/+5xsx2NbNItuPY1vT1+Z800ZnZEjNrDD8Iq8KABvZVQKkIYzo8zXW3D99L9MfNrCHm8Ud7EdcqM/tIzOPdzOwRM6s1s3Vm9jcz2zHd7cds98gw7q/2dluyhZlNBr4MTHb3CX24n5lm9o/Y59z9HHe/rq/22UUcVcBPgI+6+0B3b8jANu8zs0t7H12X258ZnvsXxT2/xswO6Kv9dhHLkWa2sD/3mYt6+j/PxvmfaonuOHcfCOwF7A18r68C6mvuvjT8UA8M3xPAnjHP/SeDuxsCPADsDIwG5gJ/ysB2zwbWhb/7lZmV9Pc++9EOwCp3X5ftQPrJdkCxuy9IZeEc+t+vAy4xswHZDkTyhLt3+wMsAQ6PeXwd8LeYx+XA9cBS4EPgJqAyfK0GeATYQHBy/gcoitnut4A3gI3AH4GKmO0eC7wWrvscsEf4/J1AO9AI1APfTvYekrw/B3aKe64S+AWwDFgF/BooD18bDTwexrUW+Ff4/ANhXJvDuL6aYF9jwv1V9SLeweE+TgUiwLS41w8Bng+P6VLgc+HzVcCvwve0EXgGKAGOBBbGbWMV8JHw7x8D94T/nzrgTOBg4IVwOyuBnwMlMevvCfwLWB9u62Jge6ABqI5Z7uBw/aIE77PLfQDFwP8BteHrrwO7dHG8vgS8Hca+EPhCF8sdG55T7eH/76YUj83dwL3h9t8A9opZdgLwF2BN+PO/BF8Um8L/XT1BYgW4D7g0Zt2vAO+F59ifgVHh8xXhOXR++Pp64OfdnC+VwA3AB8By4KdAKbB7+P/wMI7HEqy7axjn+eF582R4zvyJ4LO+AXgqeuyBrwKtQHO4zQfC58fHHIdFwMxenP8zgX8Afwe+E/P8GuCA7t5zF9srAX4ZHueFwEVAJNn5AwyPO1/qw+e6/Wwk2HfCYxm+/jxwZvx7j3l8DPBuuO4vYpcPl/0XwedkY7jcdOACYEW4z9NSvOYdGb737xN85lYAZyT5n18GLA6P21vAMeHz/Xr+d2wvhRNrCWGiA8YBbwK/jHn9F8AsYBgwCPgrcG342rUEF4zS8OejgMVs90WCi/8wYD7hBwDYB1gNzCC4qJ0dLl8eH1Nvf0ic6G4CHiQokQ0GngB+GL72c4IPRglQBnws0UWwi32dBizpZbznA+8DRvBhvy7mtZ3Ck+fkML4RBKVVgFsJLlSjw2P60fB3KhfzZuBoghqASmB/YL9w/R0JPgTR/91Qgg/DhQRfgqqB/cLX/gWcG7OfG4GfdvE+u9vHCcDscNtFwG7AyC62czwwMTxehxNcnHbrYtlOxyLFY7MZ+GQY58+Bp8PXSsNz+sfAgPC4HZToghX/QQ+P9SpgD4IP9s3A3+M+6H8O3/9EggvdIV28p+sIvmDWAKOAl4BLwtd2JeainmDdXcN9/S7mPZQQfB4HhrHcCDyf6H2Ej4sJrhnfIfi87EzwBezjaZ7/0UQ3g+AiWB0+H5vounzPCbb39TC+MQSfl2fpnOi6PH+6OD+6PG8T7DvZsewy0RF8jusJvqCVAt8mSDixia4V+Fy4n58SXDd+Hv4fjidIEhUpXPOODLd1SbivkwgS2MBE//Pwuc8S1BgUAZ8Pl6/p7/O/Y/spnFhLwgNaF+7gn8CQ8DUj+Fa4Y8zyBwKLw7+vJPgmt1MX2439J14H3BRzAbwqbvkFhB8O+jDRhSdFCzA25rlDgfkxcT4ATEqwrS4THcG3+1XAp3sZ77PAj8O/zyX41lgcPr4CuDfBOqXhibpVqYfULuZPJonpu9H9hjHN7mK5s4F/hn+XEVyo9kjxfcfu42iCauD9Cb849eD4PQ58qYvX0kl0j8S8tg+wIeacWUHi0mqyD/rdwJUxrw0hKDmMZssHfXrM67OAr3fxnlYAn4h5fALwdvh3qoluTDfLjA5jq4h/H+HjjwPvxq1zBXBjmud/7MV+FnBF+HdsouvyPSfY3nPAOTGPj09yTDrOn0TnR3fnbQrvLf5YdpfoLgCeinmtiKBwEJvo3ox5fb/wfzk45rmG8H+c7Jp3JEGpsCjm9U2EtRfx//Mu3tvbwKf6+/yP/qR6j+5Edx9EUC22K8E3JQi+AQ0AXjazDWa2ITwRRoSv/5TgG82TZrbIzL4bt91VMX9vJvhmA8G9kouj2wy3O57gW1dSZvZYTOOSM1J8j1FjCBLD3Jh9PwyMDF+/hiC5PGVmC83smynEM5qgNPVTd/9zF8vsHBPzmi6W2ZGgauTu8Kk/EZSgPhk+Hk9QnI+3HcHJvChZrF1YFhfH1PAYf2hmmwiqKaLnRFcxROPdz8zGEiSr5e7+RqIFk+zjMYIS6m+BD83sN101kDKz483sxbAx0AbgEzHbyYSuzuHxBF/42tPY5hiCb98AuPsGggvL2BT228HMjODi8H7M0+/HbSeZdndfGbPNEjO7Pvw8byK4gBlBtV0iOwAT4j7L3wzjio836Wcgzg+Ar5lZx77TeM9j6Hx+x67X4/MnyXkbv2xPj2WXcYfn2Yq4ZT6M+bsRaHb3jXHPDST5NQ+gNu5cTnjOxby388zsjZjt7UTqn7uMnP+xetS9wN2fAW4nuCcHwbeoaFF+SPgz2MNGHu5e5+4Xu/sk4Djgm2Z2WAq7WgZcE7PNIe4+wN3vjYaSJM6jfEvjkru7WzaBDwjqj3eMe0/Dw21vdPevufsOBFWEl5rZwV3FZWY1BFUt97j7/3YT8zsxMXd1Qpwd/v67ma0C3iFIYGeFzy8jqC7p6j1NSvBaA8GXlWi8pQRVyZ3Ci3t8C/AKwTGqJii5W5IYcPd64CGC6pTPE9xv7UqX+/DAz9x9b4LqjT2Br8VvIGxV+ABwFUHV5hCC6lOLX7YLqRybriwjuMAn+ox1e/4SfJHaIWa/gwmqaeIvZN3y4OvuqthtEdwr7cl24mM9FziC4Bv/YIIvvrDlmMYvv4ygNBX7WR7k7icliDeVz0Ds8q8TfLH+TsxzPX3PHxB8KYldNnhDyc+fRP/H7j4b8ZIdy07nH52/HHxAcCspGmsRPfsCE6vba14KOh0HM9uZ4B7fBcCw8LgtpPvjFisj53+sdPrR/QL4pJntFWb4W4Cfm9nIMKixZvap8O9jzWyn8FvWJqAt/EnmFmCmmc2wQJWZHWNmg8LXPyTxRbvX3L0VuA34pZnVhPsfb2afhI5veBPD97SRzu+pU1xmNpTgPtrj7n55b+IK9/d5ghvCe8X8fA440cyqgTuAY83sJDMrNrMRZrZH+J7uCN/TqPC1j5hZMcF9pGFmdlh4Ib+C5OfFIGCju9eb2W4E9w2jHgZ2MrMvm1mZmVWb2X4xr98BfJGgOqS7LyFd7sPMDjCz6Ra0AmwgqHZJdF5VEnxTXQ20m9nxBLUSqUrn2EQ9S1Ddf5WZDTCzSjM7KHztQ2B8uM1E7gXON7NpZlZB0AXgX+6+qovlu3Mv8EMzGx5+Ri8B7kpjO1GDCBoTrCVo4HR13Ovxn81noaO/XkVYitnDzPbpRQyxfkhQFRabEHrynu8HvmFm24VfSmP7ciU7fz4ERsbVJnT32YiX7Fi+BpwSHrddgXNiXpsFzDCzo8PPwTcJand6LNk1LwXx//OBBFWNtUCRmc0kKNHFLt9f5z+QRqJz91qCi9UPwqe+Q5Ctnw+L3/8Adglfmxw+ridoPPAbd386hX3MIThB/o/ghulCOv+TryUoSW0ws2/19D2k4OsE3yrmECSzx9nyj5oCPE1wEfs3cL27Px++dg1wTRjXhcBnCJLRTOvcdy+2SiBVHyeoSrjR3VdFfwhuIK8APuPu7xHcj/g+wXGbQ9BQA4LWUe8BrxJ8sK4iuL+1hqA0dDdBC7VVBCX17nwD+KKZ1RO0bvtj9AV3X09QlXoawQViAfCRmHWfIriAPOvuH6SzD4I6+9sJbkIvIqjm+FX8BsL39i2CBlJrgROBR5O8t/j1e3psouu2ElTP7hmuuxT4dPjy4wT3mVeb2fIE6z5CcI7PIjgPRxN8yUnHZcA8gnuarwH/JbjPnK5bCS5gqwgacTwb9/rNBNXTG8zsvpjjcBDB/6mW4B58RvrietA14kGCcyqqJ+/5/wgarswlaC15f8y2k50/rxP8j94P3+8wuj9v4yU7ltcR1NjUEhzXjmQdfnZOJzjv17CloWBzN/vrTnfXvGTi/+evEDRumUNQWpwY/h3Vn+c/sKUFpEi/MbPnCL709KZkISKhsFS3iqDP8+xsx5NrNASY9KvwfubOZKbjvMg2y8yOMrPBYfXeDwkaZbyc5bBykhKd9Bszu49gAIGvuntjtuMRyXMfI+iUvRo4DDjJ3VuyG1JuUtWliIgUNJXoRESkoOXKIK0ZV1NT4xMmTMh2GCIieeXll19e4+4jki+ZPwo20U2YMIE5c+YkX1BERDqY2fvJl8ov/Vp1acH8TQssGDorfjgwzGwHM/unBUPHPG1msT3/zzazd8Ofs+PXFRERSaTfEl04CscNwFHAVOB0M5sat9j1wB3uvgfB0DnXhusOI2g+O4NgIN8fhqOOiIiIdKs/S3T7E4z0vShsAnsfwSgesaYSzI4AwQga0dc/RTBNw7pw5I2/EwwhJSIi0q3+THRj6TxK+HK2HoT0dYKBkiGY82iQBSOTp7KuiIjIVvoz0SUawTu+E9+3gI+b2asEYzuuIBhVO5V1MbMLzGyOmc2pra3tbbwiIlIA+jPRLafzdBjjCAbs7ODuK9390+H0K5eEz21MZd1w2Zvdfbq7Tx8xoqBax4qISJr6M9G9BEwOp7gpIxjdflbsAuEUEdGYvkcwdQQE07ofYWZDw0YoR4TPiYiIdKvf+tG5eyScuuYJoBi4zd3nmtmVwBx3n0Uw19O1ZuYEU+B8JVx3nZldRZAsIZhmfV2fBPr1r8Nrr/XJpkVE+sVee8EvfpHtKHJGv3YYd/dHiZsPzN0vi/n7QYK5pRKtextbSngiIiIpKdiRUdKmb0EiIgVFgzqLiEhBU6ITEZGCpkQnIiIFTYlOREQKmhJdBq1raOFTP/8379XWZzsUEREJKdFl0Hu19Sz4sI63VmzMdigiIhJSosug+uYIABsbW7MciYiIRCnRZVB9U5joNivRiYjkCiW6DIqW6DY1KdGJiOQKJboMalDVpYhIzlGiy6C6JiU6EZFco0SXQR1Vl42RLEciIiJRSnQZpKpLEZHco0SXQXVKdCIiOUeJLoOi3Qs2KdGJiOQMJboMilZd1jVHaGv3LEcjIiKgRJdR0cYoAHXqSycikhOU6DKorilCcZEBuk8nIpIrlOgyqL45wujqCkBdDEREcoUSXYa4Ow3NEcYMCRKdSnQiIrlBiS5DmiPtRNqdsUMqASU6EZFcoUSXIdHhv8aEiU4DO4uI5AYlugyJdi0YoxKdiEhOUaLLkGjXgpGDyikpMiU6EZEcoUSXIdGqy4EVJVRXlmp0FBGRHKFElyHRqstB5aUMrixViU5EJEco0WVItOoyWqJTohMRyQ1KdBkSnbmgqryYwZWlbGpSh3ERkVygRJchsVWX1RUlukcnIpIjlOgypD4c57KitEj36EREcogSXYbUN0eoKivGzDoSnbum6hERyTYlugypb44wqKIUgOrKUtranc0tbVmOSkRElOgypL4pwsDyEgAGVwYJT9WXIiLZp0SXIfXNEarKiwElOhGRXKJElyH1zREGRqsuw99qeSkikn39mujM7EgzW2BmC83suwle397MnjKzV83sDTM7Ony+1Mz+YGZvmtl8M/tef8adivrmCINUdSkiknP6LdGZWTFwA3AUMBU43cymxi12KXC/u+8NnAb8Jnz+VKDc3XcH9gW+ZGYT+iPuVNU3qepSRCQX9WeJbn9gobsvcvcW4D7ghLhlHKgO/x4MrIx5vsrMSoBKoAXY1Pchp66hOcLA8miry6Bkp9FRRESyrz8T3VhgWczj5eFzsS4HzjSz5cCjwEXh8w8CDcAHwFLgendfF78DM7vAzOaY2Zza2toMh9+19nanviXCwIogwUW7GahEJyKSff2Z6CzBc/E9qk8Hbnf3ccDRwJ1mVkRQGmwDxgATgYvNbNJWG3O/2d2nu/v0ESNGZDb6bmxubcMdBoZVl8VFxiANAyYikhP6M9EtB8bHPB7HlqrJqPOA+wHcfTZQAdQAnwMed/dWd18N/BeY3ucRpyg6zmW06hKClpdKdCIi2defie4lYLKZTTSzMoLGJrPillkKHAZgZlMIEl1t+PwnLFAFHAC83W+RJxE76WqUxrsUEckN/Zbo3D0CXAg8AcwnaF0518yuNLPjw8UuBs43s9eBe4FzPBgw8gZgIPAWQcL8vbu/0V+xJ9MxF11YdQlKdCIiuaIk+SKZ4+6PEjQyiX3uspi/5wEHJ1ivnqCLQU5KVHU5uLKURWvqsxWSiIiENDJKBnRUXZZv+d5QXVmiEp2ISA5QosuALVWXukcnIpJrlOgyoKPqMq4xSlNrO80RTdUjIpJNSnQZEC3RVcU0RqmujA7srNFRRESySYkuA+qaIpQVF1Fe0rnVJWh0FBGRbFOiy4CG5kinakvYUqJTohMRyS4lugyob450aogCMXPSNSnRiYhkkxJdBtQ1RaiKS3SDKzX5qohILlCiy4CGmElXo3SPTkQkNyjRZUB9wnt04Zx0SnQiIlmlRJcB9c1bV12WlxRTUVqkEp2ISJb161iXhSpRYxTQ6Cgiuai93Vm1qYnFaxo6fpat2wxAWUkRZSVFlJcUUVpcRFlxUcdzZSUxj4sTPJfK38VFlBSrfNHflOgyoL4pwqCKxIlOHcZF+p+7s6a+hcVrGliypoFF4e8la4PE1hxp71i2orSI8UMHUFxktETaaWlr7/w70k6kPX6O6PQVGV0nzfBxafGWhBu7TGnMsuVxibQ0Zrl9th/K+GEDMhZzvlOi66VIWzuNrW1UlW19KKsrVKIT6UsbN7eyaE19kMBqG1i8djNLwlJadMQigNJiY/thA5hYU8VHJ9cwoaaKicOrmDiiilGDKigqsm73097uQeKLSX5dJcWtHidaJtl6be3UNUVYG7dMa/h3c/i7Kz/7zJ5KdDGU6HqpoSUYyzK+MQoEJboPNjb1d0giBaWhORKUzDqSWUNHMlu/ecsXySKDcUMHMKGmin13GMqE4cHfk2oGMmZIRa+qDIuKjIqiYipKi5Mv3E/cndY270iEHUkw0s6IQeXZDi+nKNH1UvRbY3z3AggS3YIP6/o7JJG809TaxtJ1mzvumS2JuX+2uq6507LbDa5gwvAqjtp9OyYOrwpKZzVVjB9W2WkYvkJnZpSVGGUlRaC81i0lul6qb4oO6Jyg6lKNUUQ6tLa1s3x941b3zBbVNrByYyMecxusZmAZE4ZX8fGdR3Qksok1VUwYXkVl2baTzCQzlOh6qT7BFD1R1ZWl1DVFaGt3ipPcAxApBO3tzsqNjSxZs5nFa+pZvGZzRwOQZes2d2rUMaiihEk1Vew3YSgTasZtSWY1VR1D6IlkQkYSnZnd7O4XZGJb+SbRpKtR0dFR6ppaGTKgrF/jEukr7k5tXfOW5vkx98zeX7u5U4vGytJiJtRUMXW7ao7efTQThlcxaURQMhtWVYaZvgBK30s50ZnZsK5eAo7OTDj5J1p1mSjRVVdER0eJpJzoGlvauOpv8/jKoTsxdkhl5gIV6aH1DS0sDhuAREtl0ftn0UZYAGXFRWw/PGjReMguI5kwfEtV46jqciUzybqelOhqgfcJEluUh49HZjKofJJodvGodMa7fG3ZBu55YSkr1jdy+7n76SIhfaq+OdL5nln077UNbIhp0VhcZIwbWsnEmir2mzCsI5FNrKlizJBKVc1LTutJolsEHObuS+NfMLNlmQspv9SlUHXZk0S3YkMjAM+8U8vf3vyAY/cYk4EoZVvW1NrG+2tj7pnFVDnWxrVoHDO4gokjqjhm9+063TMbP3RA0LpPJA/1JNH9AhgKbJXogOsyE07+6Wh1maAlWHTy1Z7MSbcyTHRTtqvmyr/O42M7j9CNeUmqta2dZbHN89dGqxk3J2jRWM6kmioO3WVE2M8sSGY7DFOLRilMKSc6d7+hm9d+nZlw8k9DS4TK0uKEnVHTKtGtb2TEoHJ+cvLunHDDf/nZk+9w+fG7ZSxeyW/uzsvvr2feB5s63TNbtr6RtpgWjYMrS5lYU8X+E4cF98xGVIV9zgYwSF+cZBuj7gW9VNe09RQ9UelWXY4ZUske44Zw1gE7cMfsJXx6n7HsMW5IJsKVPNbe7lz72Hxu+c9iAAaUFTOxpordxg7m2D3GdFQzTqqpYmiVWvmKRCnR9VJXMxdAcCEqKbIezUm3ckMjU7arBuDiT+3CY2+t4pKH3uLhrxysG/7bsNa2dr77pzf50yvLOevAHbjw0J0YMUgtGkVSobvLvdTQTaIzsx6NjuLurNjQyNihQbeC6opSLjtuKm+u2Mids5dkKGLJN02tbcy882X+9MpyvnH4zlxx/G6MrK5QkhNJUVolOjP7NPARgu4Fz7r7QxmNKo/UN3Wd6KBnc9KtbWihOdLOmMEVHc8ds/t23L/zcq5/8h2O2n07RlVXdLMFKTQbG1s5/w9zeOn9dVx14jQ+f8AO2Q5JJO/0uERnZr8BZgJvAm8BXzKzLhuqFLq6BLOLx6quLGVTU2pz0q1YH7S4HDt0y/QaZsZVJ+xGa1s7V/51Xu+Clbyyuq6J025+nleXrefXp++tJCeSpnRKdB8HprkHDZbN7A8ESW+b1NCceNLVqOqKkpRLdNE+dGOGdC617TC8ios+sRPXP/kOpyxYzaG7bLP987cZ769t4PO3vsia+mZuPXs/PrbziGyHJJK30rlHtwDYPubxeOCNzISTf7prjALRWcZTS3TRPnTjhmw9YeL5H5vEjiOquOwvb9HU2rbV61I45q3cxMk3zmZTUyt3f3GGkpxIL6WT6IYD883saTN7GpgHjDCzWWY2K6PR5YH6pu6rLnuS6Javb2RgeQnVlVtvr7ykmGtO2p1l6xr59b/eTTteyW0vLl7HZ2+eTWmx8eDMA9l7+6HZDkkk76VTdXlZxqPIU82RNlra2ruvugwbo7h70lZyKzc0MmZI163pDpg0nJP3GcfN/17EiXuNZfKoQb2KX3LLP+Z9yFfueYWxQyu587wZGtRbJEN6XKJz92eAt4FB4c98d38m+pPpAHNZQ3NQhZis6jLS7mxuSV7duGJDY9KL2/eP3pWq8hIuefgtPHZcJ8lrD768nC/d9TK7jh7EgzMPUpITyaB0Wl1+BngROBX4DPCCmZ2S6cDyQXezi0f1ZHSU6Kgo3Rk+sJzvHbUrLy5ex4MvL+9BtJKrbvn3Ir71wOscMGkYd59/AMM0qolIRqVzj+4SYD93P9vdzwL2B36QyopmdqSZLTCzhWb23QSvb29mT5nZq2b2hpkdHfPaHmY228zmmtmbZpb1DmXdTboaFR2QOdnAzg3NETZsbu3oLN6dU/cdz/QdhvKjR+ezvqGlBxFLLnF3fvzY21zz6HyO3n00t52zX7fnkoikJ51EV+Tuq2Mer01lO2ZWDNwAHAVMBU43s6lxi10K3O/uewOnAb8J1y0B7gJmuvtuwCFA6uNq9ZFoouvuHl1HiW5z9+FGW1ymUmVVVGRcfdI06poi/Pixt1MNV3JIJBzS66Zn3uOMGdvz69P3obxEMweI9IV0Et3jZvaEmZ1jZucAfwMeS2G9/YGF7r7I3VuA+4AT4pZxoDr8ezCwMvz7COANd38dwN3XunvW29jXNwfJKxNVlyt6kOgAdh1dzXkfncgf5yzjxcXrUlpHckNTaxtfuecV/jhnGV/9xE5cfeI0jWMq0ofSaYzyP8BvgT2APYGb3f3bKaw6FoidoHV5+Fysy4EzzWw58ChwUfj8zoCHCfYVM0u4PzO7wMzmmNmc2tralN9TuupTaIwS7SqQbHSUjkSXQtVl1NcOm8zYIZVc+vCbtETaU15PsqeuqZVzf/8ST8z9kB8eN5VvHrGLxqwU6WPpNEb5ibv/2d2/6e7fcPeHzOwnqaya4Ln4ZoOnA7e7+zjgaOBOMysi6AbxEeCM8PdJZnbYVhtzv9ndp7v79BEj+r6TbbQxSkpVl0lKdCs3NFJSZIwclPqtxwFlJVx5wm6882E9tz67OOX1JDvW1Ddz+i3P89KSdfzis3tx7sETsx2SyDYhnarLTyZ47qgU1ltOMIpK1Di2VE1GnQfcD+Dus4EKoCZc9xl3X+PumwlKe/v0MO6UuTvzVm5i9aambpdLpeoyOsll0qrL9Y2MHlzR4yqsw6aM4lO7jeKX/3yHZes292hd6T/L1m3mlBufY+Hqem45ezon7h1fmSEifSXlRGdmXzazN4FdwhaR0Z/FpDYE2EvAZDObaGZlBI1N4kdSWQocFu5vCkGiqwWeAPYwswFhw5SPE4zIknGPv7WKY371LEf/6j9ckWQQ5frmNsxgQGnXjQiKi4xBFSVJR0dJpWtBVy4/fjeKzfjhrLnqW5eDFqyq4+Qbn2NdQwt3f3GGxioV6Wc9KdHdAxxHkJyOi/nZ193PTLayu0eACwmS1nyC1pVzzexKMzs+XOxi4Hwzex24FzjHA+uBnxEky9eAV9z9bz2IPWXrwub6o6rL2dDYfdP9+qYIA8tKKEpSCquuSD4M2MoNTYxLM9FtN7iSb3xyZ/719mqemLsqrW1I33j5/XWcetNzmMEDMw9i3x2GZTskkdQiapgAABzqSURBVG1Oyp123H0jsJHgPlpa3P1RgmrH2Ocui/l7HnBwF+veRdDFoE99dr/xnL7/eM743Qs0t3bfwKO+ubXbasuoZHPSRdraWbWpqUcNUeKdc9AE/vzKCi6fNY+PTB6h/lg54KkFq/nyXS8zurqCO8+bwfhhWw/WLSJ9TzOMxykuMsyM8pIimpO0ZGxobmNgNw1RogZXlnbbYfzDumba2j3tqkuAkuIirjlpGh/WNfGzJ99JezuSGQ+/uoLz/zCHHUcM5MEvH6QkJ5JFSnRdKC8ppjnSfVe9uiRT9ERVV3Y/J13HhKu9HN9w7+2HcsaM7bn9ucW8tWJjr7Yl6fv9fxfz9T++xvQJQ7nvggOoGVie7ZBEtmnpdC+40MwKfu6Q8tLkJbr6ptaUEl2yqssVG4LWkr0p0UX9z6d2ZVhVOZc89CZt7WqY0p/cnf99cgFX/HUen9ptFLefu39Hq1sRyZ50SnSjgZfM7P5w7MqC7O1aXlKU9B5dQ3NbyoluU2PXHcZXbgi6MWRixPrBlaX84NgpvL58I/e88H6vtyepaWt3Lnn4LX79r4V8dvp4bvjcPlR00xpXRPpPOiOjXApMBm4FzgHeNbMfmdmOGY4tq1KpuqxvjqR0j666opTG1rYuRy9Zvr6R4VVlVJZl5sJ4/J5j+MhONVz3+AJW13XfF1B6rznSxkX3vsI9Lyzly4fsyI9P3p2SYt0VEMkVaX0aPeistSr8iQBDgQfN7LoMxpZV5SVFSYfVqku16nJA953GV/aiD10iZsZVJ06jua2dqx+Zn7HtytbqmyOcd/scHn1zFZccPYXvHLmrhvQSyTHp3KP7qpm9DFwH/BfY3d2/DOwLnJzh+LKmLEmrS3enoSX1qkvoOtGlMuFqT02sqeIrh+zErNdX8u93+n7cz23RuoYWzrjleWYvWsv1p+7J+R+blO2QRCSBdEp0NcCn3f1T7v6Au7cCuHs7cGxGo8ui8pJiIu1OpC1xsmtqbaet3VOuuoTEc9K5e8ZLdFEzD5nEpJoqfvCXt2hqzfpkDwVlxYZGTrnpOd5eVcdvz9yXU/Ydl+2QRKQL6SS6R4GOeWHMbJCZzQBw94KpJysvDQ5NSxeJri6FcS6jqrsp0W3Y3MrmlrZedRbvSnlJMVedOI33127mN0+/l/Htb6sWrq7jlBufo7aumTvPm8HhU0dlOyQR6UY6ie5GoD7mcUP4XEEpLwkOTVctLxvCKXoG9aDqMtEwYD2dh66nDt6phhP3GsNNT7/He7X1yVeQbr26dD2n3DSb1jbnjxccyP4TNaSXSK5LJ9GZx4wcHFZZFtx4U9HZnru6Txedoqcn9+iykegALjlmKhWlRVz60Fsa9LkX/vNuLWf87gWqK0r505cPZOqY6uQriUjWpZPoFoUNUkrDn68BizIdWLZ1lOi66GLQs6rLYJlEVZcdo6L0QdVl1IhB5XznqF2ZvWgtD7+2os/2U8geeWMlX7j9JbYfNoAHZx7IDsOrsh2SiKQonUQ3EzgIWEEwT9wM4IJMBpULovfouirRdVRdptAYpbykmIrSooSJbuWGRipKixg6oG9H0Dh9v+3Ze/shXP3IfDZu7n4mBensztlLuOjeV9l7/FD++KUDGVmd+uS4IpJ96XQYX+3up7n7SHcf5e6fc/fVfRFcNkWrLrvqSxeddDXVWQK6Gh0l2rWgr/teFRUZ15y4OxsaW/nx42/36b4Khbvzi3+8ww/+MpfDdh3JHeft31ENLSL5o8f31sysgmAm8N0IJkYFwN2/kMG4si5Z1WX0Hl0qVZcQdDFIWHW5oZGxQ/tnZPupY6r5wsETuOU/izll37GaG60b7e3OFX+dyx9mv8/J+4zjJxrtRCRvpfPJvZNgvMtPAc8A44C6TAaVC8qStLqs70HVJXQ9sPPKDY2MHdJ/VWFfP3xnxgyu4JKH3qK1i64T27qWSDtf/+Nr/GH2+5z/0Yn89JQ9lORE8lg6n96d3P0HQIO7/wE4Btg9s2Fl35YSXddVlyVF1rFcMonmpGtqbWNNfUuftriMV1VewuXH78bbq+r4/X8X99t+88XmlghfvGMOs15fyXeO3JXvHz0l6QzyIpLb0kl00av1BjObBgwGJmQsohyxpXtB11WXVeUlKd9bq05QolsZdi3oi1FRunPEbqM5fMoofv73dzu6Nwhs2NzCGb97gWffreUnJ+/Olw/ZUeNWihSAdBLdzeF8dJcCs4B5wE8yGlUOSNbqsj7FKXqiElVd9kcfuq5cfvzU4Pesuf2+71z0wcZGTr1pNnNXbuI3Z+zLZ/fbPtshiUiG9CjRmVkRsMnd17v7v919Utj68rd9FF/WJBsZpb65NeX7cxCU6OqbI7THTIbaH33oujJu6AC+8cnJ/H3ehzw5d1W/7z+XLKqt55QbZ/PBxiZuP3c/jpw2OtshiUgG9SjRhaOgXNhHseSUpFWXzZGUW1wCVFeU4A51TVu6GKzc0EiRwags9cs69+CJ7Dp6EJfPmktDc9cTwxayN5dv5JSbZtPU2sZ9FxzAQTvWZDskEcmwdKou/25m3zKz8WY2LPqT8ciyrC+qLqHz6CjLNzQyurqC0iy16CstLuKak6axcmMTv/znu1mJIZueW7iG026eTWVpMQ/MPJBpYwdnOyQR6QPpjFEZ7S/3lZjnHCioybiStrpsamVcD6ocEyW6vpqepyf23WEYp++/Pbc+u5gT9xq7zYzf+NibH/C1+15jQs0A7vjCDEYP1mgnIoUqnZFRJib4KagkB1BWnKxEF2FgWc/u0UHnOemCzuLZTXQA3zlyF4ZUlnLJw292uodYqO59cSlfuecVdh83mPu/dKCSnEiBS2dklLMSPe/ud/Q+nNxhZuEs44nv0TU0t6U06WpUfImurd1ZtbEp6yU6gCEDyrjkmCl88/7Xue+lZXxuRmG2OHR3fvP0e/z0iQUcsssIfnPGPgzowZcVEclP6XzK94v5uwI4DHgFKKhEB0H1ZaJWl+3tHpToenGPrraumdY2z0rXgkRO2nssD8xZzo8fm88Ru42iZmB5tkPKqPZ255pH53Prs4s5Ya8xXH/qnlm7Nyoi/SudqsuLYn7OB/YGyjIfWvaVlxQnrLpsaEl9Lrqo+DnpVmzYDGSna0EiZsbVJ02jqbWda/5WMBPFA9Da1s63HnidW59dzDkHTeDnn9lLSU5kG5KJT/tmYHIGtpNzyruouoxO0dOTqssBZcUUF1lHiW7FhiYgO53Fu7LjiIHM/PgkHnp1Bc8tXJPtcDKisaWNL935Mn9+dQUXf3JnfnjcVA3pJbKN6XGiM7O/mtms8OcRYAHwl8yHln3lpUUJS3Q9naIHghJT7Ogo0c7iuXCPLtb/O3Qndhg+gEsffqvL+5P5YuPmVj5/6ws8tWA1V584jYsOm6whvUS2Qenco7s+5u8I8L67L89QPDmlvKQ44Xx00U7fPUl0EB3YOVh35YZGBleW9ngbfa2itJirTpjGWbe9yE1PL+Jrh+dnYX31pibOuu1F3qut59en782xe4zJdkgikiXpXGWXAh+4exOAmVWa2QR3X5LRyHJAUHWZ4B5dGlWXEIyOsqXqsjGnqi1jfWznERy35xhueHohx+81hok1VdkOqUeWrGng87e9wNr6Fn5/zv58ZLJGOxHZlqVzj+4BIPbq3xY+V3CCVpdbV9+lU3UJnWcwWLE+N/rQdeUHx0yhvLiIy/7yFu7507du7spgSK/6pgj3nn+AkpyIpJXoSty9Jfog/LsgW12WdVGi603VZV2Y6FbmcIkOYGR1Bd8+chf+8+4aZr2+MtvhpOT5RWs57bfPU1ZsPDDzIPYcPyTbIYlIDkgn0dWa2fHRB2Z2AlAYTfTidNm9oDm9RBct0W1sbKWuOZLTiQ7gczN2YM9xg7nqkfkJZ0fPJU/OXcVZt73IyOpyHvzyQew0cmC2QxKRHJFOopsJfN/MlprZUuA7wJcyG1ZuCFpdJqq6DBJdT2YvgC1z0uVqi8t4xUXGNSftzrqGZq5/YkG2w+nS/XOWMfOul5myXTUPzDwo54+riPSvdDqMv+fuBwBTgd3c/SB3X5jKumZ2pJktMLOFZvbdBK9vb2ZPmdmrZvaGmR2d4PV6M/tWT+NOR1cjo9Q1RygrKaKspGeHb3BlKZF2Z2FtPZA7ncW7M23sYM45aCJ3vfA+ry3bkO1wtvLbZ97j2w++wcE71XDPF2cwrKoga9FFpBfS6Uf3IzMb4u717l5nZkPN7OoU1isGbgCOIkiSp5vZ1LjFLgXud/e9gdOA38S9/nPgsZ7GnK7uqi4HpdEtoLoiGB1l/gebgNzqLN6dbx6xM6MGVfD9P79JpC3xINf9zd259tH5XPvY2xyzx3b87uzpPS5hi8i2IZ2qy6PcveOrvbuvB47uZvmo/YGF7r4obMByH3BC3DIOROeJGQx0tIIwsxOBRcDcNGJOS3lJES2Jqi6bIj3uWgBbhgGbt3ITZSVFDM+T0sfA8hJ+eNxU5n2wiT/Mfj/b4RBpa+fbD77Bb/+9iDMP2J5fnbZ3x0S5IiLx0kl0xWbWMeKvmVUCqYwAPBZYFvN4efhcrMuBM81sOfAocFG4jyqCe4FXpBFv2roeGSVCVRqj3kcT3fwPNjF2SGVeDUV15LTRHLrLCH725AI+2NiYtTiaWtv48t2v8MDLy/nqYZO56oRpFOfRcRSR/pdOorsL+KeZnWdmXwD+TmozFyS6GsV30DoduN3dxxGUEu80syKCBPdzd6/vdgdmF5jZHDObU1tbm0JI3YtWXcb3I6tv7l2JbnVdM2OG5NccaGbGlSdMo82dK2bNy0oMm5paOfu2F/n7vA+5/LipfPOTO2tILxFJqsdXa3e/zszeAA4nSF5XufsTKay6HBgf83gcMVWTofOAI8P9zDazCqAGmAGcYmbXAUOAdjNrcvf/i4vtZuBmgOnTp/e6l3N0lvGWtvZOVWP1zRFGDep5oqqu3HK48+X+XKzxwwbw1cMmc93jC/jn/A85bMqoftt3bV0zZ9/2Iu98WMcvT9uLE/aKrwwQEUksrdkL3P1xd/+Wu18M1JvZDSms9hIw2cwmmlkZQWOTWXHLLCWY3w4zm0Iw312tu3/U3Se4+wTgF8CP4pNcX4gmuvjqy/qmSFoNH6IlOoCxQwb0Lrgs+eJHJjF55EAu+8tcGlv6Z9DnZes2c+pNz7F4TQO/O3u6kpyI9Ehaic7M9jKzn5jZEuBq4O1k67h7BLgQeAKYT9C6cq6ZXRnTAf1i4Hwzex24FzjHszj+VEeii+tiUN/D2cWjBlVsSXT5VnUZVVZSxDUn7c6KDY388p/v9vn+3l61iZNvfI71m1u564szOGSXkX2+TxEpLClfrc1sZ4JS2OnAWuCPgLn7oaluw90fJWhkEvvcZTF/zwMOTrKNy1PdX29FqyvjO43XN7em1b2guMgYVF4SjIqSB33ourL/xGF8Zvo4fvefRZy091h2GT2oT/bz0pJ1nHf7S1SWFfPAzAPZeVTf7EdECltPSnRvE1QrHufuH3H3XxMM6Fywyku3rrqMtLXT1Nqedp+t6rD6Mh/v0cX67lFTGFRRwqUPv0l7e+YL3f96+0M+f+sLDB9YzoMzD1KSE5G09STRnQysAp4ys1vM7DASt6QsGB2NUWISXccUPWkmusGVpZjB6MH5WXUZNayqjO8fPYWXlqzngZeXJV+hBx56dTnn3/EyO40cyAMzD2T8sPy8nykiuSHlROfuD7n7Z4FdgaeBbwCjzOxGMzuij+LLqi1Vl1sSXV10ip407tFB0PJyxMDygujgfMq+49h/4jCufext1tY3Z2Sbtz67mG/88XVmTBzGvecfQM3AVLpoioh0LZ2xLhvc/W53P5agi8BrwFbjVhaCLY1RttTQ1qc5c0HUgZNq+rVZfl8yM645cRoNzRGufSxpe6RuuTs/feJtrnpkHkfuNprbztmvU+MdEZF09WpwQHdfB/w2/Ck4ie7RpTtFT9TXDp/c+8ByyORRg7jgY5O44an3OGXfcRwwaXiPt9HW7lz68Fvc++JSTt9/PFefuLtGOxGRjEmre8G2ImHVZXTS1TSrLgvRhYdOZvywSi556M1O9zNT0Rxp48J7XuHeF5fy/w7ZkR+dpCQnIpmlRNeNso4O45mruixElWXFXHnCNN6rbeCW/yxKeb365gjn/v4lHntrFZceM4VvH7mrhvQSkYxToutGog7jva26LFSH7jKSY3bfjl/9813eX9uQdPm19c2cfvPzvLB4Hf976p588aOT+iFKEdkWKdF1Q1WXPfODY6dSWlzEZX+Zu9VA2LGWr9/MqTfN5p0P67j58/ty8r7j+jFKEdnWKNF1Y0s/uq2rLtOZpqfQjR5cwcVH7Mwz79Ty6JurEi7zzod1nHLjbGrrm7nrizMKpgWqiOQuJbpudNXqckBZsRpMdOGsAycwbWw1V/x1LpuaWju99srS9Zx602za3Ln/Swey34RhWYpSRLYlSnTdKCveOtHVN0d0f64bxUXGj07andr6Zn725Dsdzz/zTi1n3PICQwaU8qeZBzFlu+putiIikjm6YnejpLiIkiLr1OqyrkmJLpk9xg3hrAN24I7ZS/j0PmNZsnYzF9//GjuNHMQfvrAfI9OYy09EJF26YidRXlK0VatLNURJ7uJP7cJjb63igjte5sO6JvabMIzfnT2dao12IiL9TFWXSZSXFm9VdamGKMlVV5Ry2XFTWbWpicN2HcUdX9hfSU5EskJX7CTKiou2qrrUaPqpOXaPMUweOYgdR1RRUqzvVCKSHUp0SZSXFnVuddkSSWvS1W1VX03KKiKSKn3NTqK8pKjT+I31TZG0J10VEZH+p0SXRHnJ1vfo1BhFRCR/KNElUV6y5R5dc6SN1jZX9wIRkTyiRJdEeemW7gX1TRrQWUQk3yjRJRFbdakpekRE8o8SXRKxVZcdiU736ERE8oYSXRJBolPVpYhIvlKiS6IsZggwVV2KiOQfJbokgnt0qroUEclXSnRJxHYYV4lORCT/KNElETsEmO7RiYjkHyW6JMpLiom0O5G2dhqaI5jBgLLibIclIiIpUqJLorwkOEQtbe3UNUcYWFaCmWU5KhERSZUSXRLRRNfc2k59k8a5FBHJN0p0SZSXBtWUzZF2Gloiuj8nIpJnlOiS6CjRRdqo0xQ9IiJ5R4kuibKORNdOfXOEQaq6FBHJK0p0SZSXBFWXLZGg1aWqLkVE8osSXRKxVZeaXVxEJP/0a6IzsyPNbIGZLTSz7yZ4fXsze8rMXjWzN8zs6PD5T5rZy2b2Zvj7E/0Vc2yryzqV6ERE8k6/XbXNrBi4AfgksBx4ycxmufu8mMUuBe539xvNbCrwKDABWAMc5+4rzWwa8AQwtj/ijra6bIq00aB7dCIieac/S3T7AwvdfZG7twD3ASfELeNAdfj3YGAlgLu/6u4rw+fnAhVmVt4PMXeU6DZsbqXdUdWliEie6c9ENxZYFvN4OVuXyi4HzjSz5QSluYsSbOdk4FV3b45/wcwuMLM5ZjantrY2I0FHE93a+hZA41yKiOSb/kx0icbN8rjHpwO3u/s44GjgTjPriNHMdgN+Anwp0Q7c/WZ3n+7u00eMGJGRoKNVl2sbgkSnqksRkfzSn4luOTA+5vE4wqrJGOcB9wO4+2ygAqgBMLNxwEPAWe7+Xp9HGyorjpboggJkVZkSnYhIPunPRPcSMNnMJppZGXAaMCtumaXAYQBmNoUg0dWa2RDgb8D33P2//Rgz5aXBIVoXlug01qWISH7pt0Tn7hHgQoIWk/MJWlfONbMrzez4cLGLgfPN7HXgXuAcd/dwvZ2AH5jZa+HPyP6Iu+MeXYPu0YmI5KN+vWq7+6MEjUxin7ss5u95wMEJ1rsauLrPA0ygo+qyIai6VKITEckvGhklCTOjvKRoS6tLVV2KiOQVJboUlJcUsbmlDVCJTkQk3yjRpSDaxaCkyDru2YmISH7QVTsF0eQ2sKIEs0TdAUVEJFcp0aWgI9Gp2lJEJO8o0aWgLJyTTolORCT/KNGlQCU6EZH8pUSXgth7dCIikl+U6FIQbXWpKXpERPKPEl0KoiW6QUp0IiJ5R4kuBbpHJyKSv5ToUlBeoqpLEZF8pUSXguhUPZp0VUQk/yjRpUBVlyIi+UuJLgVlYaJT1aWISP5RoktB9B6d+tGJiOQfJboUqHuBiEj+UqJLQbmqLkVE8pYSXQqiI6OoMYqISP7RlTsFh08ZSe2mnRg7pDLboYiISA8p0aVgu8GVfPOIXbIdhoiIpEFVlyIiUtCU6EREpKAp0YmISEFTohMRkYKmRCciIgVNiU5ERAqaEp2IiBQ0JToRESlo5u7ZjqFPmFkt8H4aq9YAazIcTj7T8ehMx6MzHY/OCuF47ODuI7IdRCYVbKJLl5nNcffp2Y4jV+h4dKbj0ZmOR2c6HrlJVZciIlLQlOhERKSgKdFt7eZsB5BjdDw60/HoTMejMx2PHKR7dCIiUtBUohMRkYKmRCciIgVNiS5kZkea2QIzW2hm3812PNlkZuPN7Ckzm29mc83sa9mOKReYWbGZvWpmj2Q7lmwzsyFm9qCZvR2eJwdmO6ZsMrNvhJ+Vt8zsXjOryHZMsoUSHcEFDLgBOAqYCpxuZlOzG1VWRYCL3X0KcADwlW38eER9DZif7SByxC+Bx919V2BPtuHjYmZjga8C0919GlAMnJbdqCSWEl1gf2Chuy9y9xbgPuCELMeUNe7+gbu/Ev5dR3ARG5vdqLLLzMYBxwC/y3Ys2WZm1cDHgFsB3L3F3TdkN6qsKwEqzawEGACszHI8EkOJLjAWWBbzeDnb+IU9yswmAHsDL2Q3kqz7BfBtoD3bgeSASUAt8PuwKvd3ZlaV7aCyxd1XANcDS4EPgI3u/mR2o5JYSnQBS/DcNt/vwswGAn8Cvu7um7IdT7aY2bHAand/Odux5IgSYB/gRnffG2gAttn72mY2lKAGaCIwBqgyszOzG5XEUqILLAfGxzwexzZe9WBmpQRJ7m53/3O248myg4HjzWwJQbX2J8zsruyGlFXLgeXuHi3lP0iQ+LZVhwOL3b3W3VuBPwMHZTkmiaFEF3gJmGxmE82sjOBG8qwsx5Q1ZmYE91/mu/vPsh1Ptrn799x9nLtPIDg3/uXu2+w3dndfBSwzs13Cpw4D5mUxpGxbChxgZgPCz85hbMONc3JRSbYDyAXuHjGzC4EnCFpM3ebuc7McVjYdDHweeNPMXguf+767P5rFmCS3XATcHX4xXAScm+V4ssbdXzCzB4FXCFosv4qGAsspGgJMREQKmqouRUSkoCnRiYhIQVOiExGRgqZEJyIiBU2JTkRECpoSnUiGmFmbmb0WjmD/VzMbkmT5IWb2//orPpFtlRKdSOY0uvte4Qj264CvJFl+CKBEJ9LHlOhE+sZswoHBzWygmf3TzF4xszfNLDozxo+BHcNS4E/DZf/HzF4yszfM7IosxS5SUDQyikiGhfMbHkY4jQ3QBJzk7pvMrAZ43sxmEQyEPM3d9wrXOwKYTDBtlAGzzOxj7v7vfn8TIgVEiU4kcyrDIdMmAC8Dfw+fN+BHZvYxgml+xgKjEqx/RPjzavh4IEHiU6IT6QUlOpHMaXT3vcxsMPAIwT26XwFnACOAfd29NZwFoSLB+gZc6+6/7a+ARbYFukcnkmHuvhH4KvCtcLqjwQTz2bWa2aHADuGidcCgmFWfAL4QzgOImY01s5H9GLpIQVKJTqQPuPurZvY6wbQ+dwN/NbM5wGvA2+Eya83sv2b2FvCYu/+PmU0BZgezvVAPnAmszsqbECkQmr1AREQKmqouRUSkoCnRiYhIQVOiExGRgqZEJyIiBU2JTkRECpoSnYiIFDQlOhERKWj/H38SRLaZgHDYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Running Test2 for HouseNumbers\n",
    "\n",
    "(x_train_house,y_train_house),(x_test_house,y_test_house) = prepare_house_numbers_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate5=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,9])\n",
    "model_name= 'keras_HN_trained_teacher_5000_nda.h5'\n",
    "accuracy_5 = stns_5000(init, model_name, x_train_house,y_train_house,x_test_house,y_test_house,rate5,teacher, student, sample_size=5000, epochs=100,\n",
    "                      data_augmentation=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train is: (32, 32, 3, 73257)\n",
      "Shape of y_train is: (73257, 1)\n",
      "Shape of x_test is: (32, 32, 3, 26032)\n",
      "Shape of y_test is: (26032, 1)\n",
      "Shape of x_train is now: (73257, 32, 32, 3)\n",
      "Shape of x_test is now: (26032, 32, 32, 3)\n",
      "Shape of y_train is now: (73257,)\n",
      "Shape of y_test is now: (26032,)\n",
      "labels of y_train are [0 1 2 3 4 5 6 7 8 9]\n",
      "labels of y_test are [0 1 2 3 4 5 6 7 8 9]\n",
      "Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "ResNet20v1\n",
      "73257\n",
      "(73257, 10)\n",
      "(73257,)\n",
      "0: 4948\n",
      "1: 13861\n",
      "2: 10585\n",
      "3: 8497\n",
      "4: 7458\n",
      "5: 6882\n",
      "6: 5727\n",
      "7: 5595\n",
      "8: 5045\n",
      "9: 4659\n",
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "0: 100\n",
      "1: 100\n",
      "2: 100\n",
      "3: 100\n",
      "4: 100\n",
      "5: 100\n",
      "6: 100\n",
      "7: 100\n",
      "8: 100\n",
      "9: 100\n",
      "Learning rate:  0.001\n",
      "Not using data augmentation.\n",
      "Train on 1000 samples, validate on 26032 samples\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 2.7511 - accuracy: 0.0930 - val_loss: 2.4455 - val_accuracy: 0.1545\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      "  96/1000 [=>............................] - ETA: 1s - loss: 2.4565 - accuracy: 0.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 10s 10ms/step - loss: 2.4538 - accuracy: 0.1420 - val_loss: 2.4786 - val_accuracy: 0.0821\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.2502 - accuracy: 0.2510 - val_loss: 2.6539 - val_accuracy: 0.1723\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.0482 - accuracy: 0.3360 - val_loss: 2.7640 - val_accuracy: 0.1335\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 1.8028 - accuracy: 0.4200 - val_loss: 2.8322 - val_accuracy: 0.2167\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 1.5052 - accuracy: 0.5610 - val_loss: 5.4410 - val_accuracy: 0.1042\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3047 - accuracy: 0.6100 - val_loss: 5.4787 - val_accuracy: 0.2068\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 1.0748 - accuracy: 0.7330 - val_loss: 5.5317 - val_accuracy: 0.1933\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.7712 - accuracy: 0.8380 - val_loss: 3.2781 - val_accuracy: 0.2790\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.6714 - accuracy: 0.8730 - val_loss: 4.0242 - val_accuracy: 0.2840\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.5686 - accuracy: 0.8990 - val_loss: 3.2894 - val_accuracy: 0.3722\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3839 - accuracy: 0.9640 - val_loss: 2.9849 - val_accuracy: 0.3256\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3389 - accuracy: 0.9690 - val_loss: 3.8225 - val_accuracy: 0.2862\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3207 - accuracy: 0.9650 - val_loss: 3.2732 - val_accuracy: 0.3368\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2893 - accuracy: 0.9740 - val_loss: 2.8548 - val_accuracy: 0.3470\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2843 - accuracy: 0.9770 - val_loss: 2.6301 - val_accuracy: 0.3927\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2907 - accuracy: 0.9770 - val_loss: 4.3721 - val_accuracy: 0.2528\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2677 - accuracy: 0.9770 - val_loss: 3.6961 - val_accuracy: 0.2728\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2352 - accuracy: 0.9880 - val_loss: 2.5825 - val_accuracy: 0.4500\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2508 - accuracy: 0.9830 - val_loss: 3.7281 - val_accuracy: 0.2904\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.4418 - accuracy: 0.9160 - val_loss: 9.0868 - val_accuracy: 0.1393\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.4274 - accuracy: 0.9270 - val_loss: 3.8324 - val_accuracy: 0.3314\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3028 - accuracy: 0.9620 - val_loss: 2.8309 - val_accuracy: 0.4097\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3376 - accuracy: 0.9560 - val_loss: 2.6087 - val_accuracy: 0.4034\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3847 - accuracy: 0.9440 - val_loss: 3.4970 - val_accuracy: 0.3441\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3254 - accuracy: 0.9480 - val_loss: 3.8329 - val_accuracy: 0.3526\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2465 - accuracy: 0.9880 - val_loss: 2.5813 - val_accuracy: 0.4542\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2012 - accuracy: 0.9980 - val_loss: 2.3204 - val_accuracy: 0.4548\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1973 - accuracy: 0.9940 - val_loss: 2.5090 - val_accuracy: 0.4378\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1887 - accuracy: 1.0000 - val_loss: 2.3535 - val_accuracy: 0.4682\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1802 - accuracy: 0.9980 - val_loss: 2.1243 - val_accuracy: 0.4812\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1748 - accuracy: 0.9990 - val_loss: 1.9657 - val_accuracy: 0.5106\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1698 - accuracy: 1.0000 - val_loss: 1.7924 - val_accuracy: 0.5469\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1710 - accuracy: 1.0000 - val_loss: 1.8240 - val_accuracy: 0.5491\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1709 - accuracy: 1.0000 - val_loss: 1.9229 - val_accuracy: 0.5337\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1684 - accuracy: 1.0000 - val_loss: 1.7996 - val_accuracy: 0.5506\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1683 - accuracy: 1.0000 - val_loss: 1.8608 - val_accuracy: 0.5352\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1674 - accuracy: 1.0000 - val_loss: 1.8998 - val_accuracy: 0.5212\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1688 - accuracy: 0.9990 - val_loss: 2.3310 - val_accuracy: 0.4741\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1694 - accuracy: 0.9990 - val_loss: 2.0516 - val_accuracy: 0.5069\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3959 - accuracy: 0.9410 - val_loss: 5.2276 - val_accuracy: 0.1865\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.4035 - accuracy: 0.9210 - val_loss: 4.6523 - val_accuracy: 0.2981\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3624 - accuracy: 0.9360 - val_loss: 4.4537 - val_accuracy: 0.3948\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2630 - accuracy: 0.9720 - val_loss: 3.9407 - val_accuracy: 0.3889\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2422 - accuracy: 0.9790 - val_loss: 2.4611 - val_accuracy: 0.4829\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2206 - accuracy: 0.9860 - val_loss: 2.9980 - val_accuracy: 0.4148\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2376 - accuracy: 0.9780 - val_loss: 2.8632 - val_accuracy: 0.4211\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2075 - accuracy: 0.9890 - val_loss: 2.3953 - val_accuracy: 0.4602\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1879 - accuracy: 0.9980 - val_loss: 3.2447 - val_accuracy: 0.3823\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1964 - accuracy: 0.9940 - val_loss: 5.7957 - val_accuracy: 0.2077\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1855 - accuracy: 0.9950 - val_loss: 2.9992 - val_accuracy: 0.4091\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2095 - accuracy: 0.9870 - val_loss: 2.9464 - val_accuracy: 0.4498\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1905 - accuracy: 0.9980 - val_loss: 2.4521 - val_accuracy: 0.5081\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1871 - accuracy: 0.9950 - val_loss: 2.2126 - val_accuracy: 0.5090\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1717 - accuracy: 0.9990 - val_loss: 2.2288 - val_accuracy: 0.4981\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1684 - accuracy: 1.0000 - val_loss: 2.0185 - val_accuracy: 0.5406\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1657 - accuracy: 1.0000 - val_loss: 1.7596 - val_accuracy: 0.5775\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1649 - accuracy: 1.0000 - val_loss: 1.7608 - val_accuracy: 0.5741\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1643 - accuracy: 1.0000 - val_loss: 1.8470 - val_accuracy: 0.5610\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1630 - accuracy: 1.0000 - val_loss: 1.8139 - val_accuracy: 0.5799\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1627 - accuracy: 1.0000 - val_loss: 1.8712 - val_accuracy: 0.5713\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1610 - accuracy: 1.0000 - val_loss: 1.8819 - val_accuracy: 0.5676\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1603 - accuracy: 1.0000 - val_loss: 1.7619 - val_accuracy: 0.5764\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1615 - accuracy: 0.9980 - val_loss: 1.9924 - val_accuracy: 0.5312\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2991 - accuracy: 0.9540 - val_loss: 4.5839 - val_accuracy: 0.3292\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5301 - accuracy: 0.8850 - val_loss: 7.0770 - val_accuracy: 0.2583\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3736 - accuracy: 0.9290 - val_loss: 3.6492 - val_accuracy: 0.3975\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2408 - accuracy: 0.9800 - val_loss: 3.6659 - val_accuracy: 0.3813\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2429 - accuracy: 0.9770 - val_loss: 4.4228 - val_accuracy: 0.3413\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2172 - accuracy: 0.9850 - val_loss: 3.1056 - val_accuracy: 0.4436\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2190 - accuracy: 0.9880 - val_loss: 1.9258 - val_accuracy: 0.5769\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2117 - accuracy: 0.9860 - val_loss: 2.7841 - val_accuracy: 0.4917\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1884 - accuracy: 0.9970 - val_loss: 2.3876 - val_accuracy: 0.5004\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1750 - accuracy: 0.9980 - val_loss: 2.1695 - val_accuracy: 0.5098\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1686 - accuracy: 1.0000 - val_loss: 1.7798 - val_accuracy: 0.5782\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1690 - accuracy: 0.9990 - val_loss: 1.9827 - val_accuracy: 0.5599\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1647 - accuracy: 1.0000 - val_loss: 1.7983 - val_accuracy: 0.5770\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1634 - accuracy: 1.0000 - val_loss: 1.6713 - val_accuracy: 0.5981\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1626 - accuracy: 1.0000 - val_loss: 1.6352 - val_accuracy: 0.6084\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1634 - accuracy: 0.9990 - val_loss: 1.5938 - val_accuracy: 0.6156\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1794 - accuracy: 0.9960 - val_loss: 2.3353 - val_accuracy: 0.4819\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1748 - accuracy: 0.9960 - val_loss: 1.9380 - val_accuracy: 0.5480\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1651 - accuracy: 1.0000 - val_loss: 1.7535 - val_accuracy: 0.5854\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1649 - accuracy: 0.9990 - val_loss: 1.6677 - val_accuracy: 0.6021\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1640 - accuracy: 0.9990 - val_loss: 1.6343 - val_accuracy: 0.6105\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1634 - accuracy: 1.0000 - val_loss: 1.6191 - val_accuracy: 0.6151\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1633 - accuracy: 0.9990 - val_loss: 1.6228 - val_accuracy: 0.6155\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1629 - accuracy: 1.0000 - val_loss: 1.6329 - val_accuracy: 0.6147\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1636 - accuracy: 0.9990 - val_loss: 1.6314 - val_accuracy: 0.6145\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1612 - accuracy: 1.0000 - val_loss: 1.6360 - val_accuracy: 0.6157\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1608 - accuracy: 1.0000 - val_loss: 1.6179 - val_accuracy: 0.6164\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1619 - accuracy: 1.0000 - val_loss: 1.6140 - val_accuracy: 0.6185\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1603 - accuracy: 1.0000 - val_loss: 1.6201 - val_accuracy: 0.6157\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1611 - accuracy: 1.0000 - val_loss: 1.6257 - val_accuracy: 0.6162\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1614 - accuracy: 0.9990 - val_loss: 1.6083 - val_accuracy: 0.6195\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1621 - accuracy: 0.9990 - val_loss: 1.6019 - val_accuracy: 0.6209\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1602 - accuracy: 1.0000 - val_loss: 1.6111 - val_accuracy: 0.6198\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1609 - accuracy: 1.0000 - val_loss: 1.6053 - val_accuracy: 0.6209\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1609 - accuracy: 0.9990 - val_loss: 1.5940 - val_accuracy: 0.6227\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1597 - accuracy: 1.0000 - val_loss: 1.6165 - val_accuracy: 0.6186\n",
      "26032/26032 [==============================] - 9s 348us/step\n",
      "Supervised learning model with 100epochs \n",
      "\n",
      "Test loss: 1.6165411616661245\n",
      "Test accuracy: 0.6185848116874695\n",
      "Saved trained model at /home/ANDURAND/pandurand/saved_models/keras_HN_trained_teacher_1000_nda.h5 \n",
      "Small sample of 5000 training images, Supervised learning model with 100epochs \n",
      "\n",
      "Test accuracy: 0.6185848116874695\n",
      "size x_total_unlabeled (should be 45,000):  72257\n",
      "rate=0.05:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1050 1000 50\n",
      "x_true_pseudo.shape:  (1050, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1050, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 3.8441725297434313\n",
      "Test accuracy: 0.4468730688095093\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 1.8680913028500163\n",
      "Test accuracy: 0.6137446165084839\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.7725573468222966\n",
      "Test accuracy: 0.45179009437561035\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 1.8624925150340808\n",
      "Test accuracy: 0.6120544075965881\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.598207285745641\n",
      "Test accuracy: 0.5397587418556213\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 2.635903053512292\n",
      "Test accuracy: 0.5353026986122131\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 1.7326448486125314\n",
      "Test accuracy: 0.65822833776474\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.040511281928477\n",
      "Test accuracy: 0.5258527994155884\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 1.6264276103733064\n",
      "Test accuracy: 0.6572679877281189\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 1.5708027970006881\n",
      "Test accuracy: 0.6796634793281555\n",
      "rate=0.1:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1100 1000 100\n",
      "x_true_pseudo.shape:  (1100, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1100, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.4868486199411204\n",
      "Test accuracy: 0.546826958656311\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 1.4992694567886686\n",
      "Test accuracy: 0.6458205580711365\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 1.9699985276136065\n",
      "Test accuracy: 0.6109787821769714\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 1.390288904245162\n",
      "Test accuracy: 0.6845805048942566\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 1.6529771966588374\n",
      "Test accuracy: 0.6487016081809998\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 2.1554808993242824\n",
      "Test accuracy: 0.5642670392990112\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 1.3259594567216242\n",
      "Test accuracy: 0.6978718638420105\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 1.6378657759225068\n",
      "Test accuracy: 0.6504302620887756\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 1.3564623508925224\n",
      "Test accuracy: 0.7127381563186646\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 1.8803163448871831\n",
      "Test accuracy: 0.6578826308250427\n",
      "rate=0.2:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1200 1000 200\n",
      "x_true_pseudo.shape:  (1200, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1200, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.6520043812284604\n",
      "Test accuracy: 0.6123232841491699\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 1.7862093973980406\n",
      "Test accuracy: 0.611094057559967\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 1.6513540076198354\n",
      "Test accuracy: 0.664374589920044\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 1.389654522302624\n",
      "Test accuracy: 0.6904963254928589\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 4.400244603658206\n",
      "Test accuracy: 0.41352951526641846\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 1.808425394720876\n",
      "Test accuracy: 0.6260371804237366\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.200003164197011\n",
      "Test accuracy: 0.6297249794006348\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 1.5362910830175942\n",
      "Test accuracy: 0.6725568771362305\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 1.6107529006490842\n",
      "Test accuracy: 0.6820067763328552\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 7.023630471631069\n",
      "Test accuracy: 0.34549784660339355\n",
      "rate=0.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1500 1000 500\n",
      "x_true_pseudo.shape:  (1500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.7724712210413716\n",
      "Test accuracy: 0.5786339640617371\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 1.8086594986124278\n",
      "Test accuracy: 0.6116702556610107\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 1.873942150172824\n",
      "Test accuracy: 0.6255761981010437\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.135620457523794\n",
      "Test accuracy: 0.6146665811538696\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.00053289556474\n",
      "Test accuracy: 0.6232329607009888\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 1.6635475090550877\n",
      "Test accuracy: 0.6668331027030945\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.165996515611673\n",
      "Test accuracy: 0.6034111976623535\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 1.47478460664755\n",
      "Test accuracy: 0.703941285610199\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 1.637445855741495\n",
      "Test accuracy: 0.6758989095687866\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 1.9705973369033336\n",
      "Test accuracy: 0.6293792128562927\n",
      "rate=0.75:\n",
      "\n",
      "n_total, n_true, n_pseudo:  1750 1000 750\n",
      "x_true_pseudo.shape:  (1750, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (1750, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.208781172031996\n",
      "Test accuracy: 0.5389520525932312\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.0404871346105002\n",
      "Test accuracy: 0.5894668102264404\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.7986060522345584\n",
      "Test accuracy: 0.5066072344779968\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 1.777180710964285\n",
      "Test accuracy: 0.6246542930603027\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 1.7488867404934227\n",
      "Test accuracy: 0.6418254375457764\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 2.021786991774706\n",
      "Test accuracy: 0.613437294960022\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.781144134604132\n",
      "Test accuracy: 0.5603488087654114\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 2.163582955098401\n",
      "Test accuracy: 0.6203518509864807\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 1.9585604781963815\n",
      "Test accuracy: 0.6433236002922058\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 1.860770005606843\n",
      "Test accuracy: 0.6473954916000366\n",
      "rate=1.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  2000 1000 1000\n",
      "x_true_pseudo.shape:  (2000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (2000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.221621493217618\n",
      "Test accuracy: 0.5059926509857178\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.4357997221527614\n",
      "Test accuracy: 0.5205516219139099\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.5403762679929445\n",
      "Test accuracy: 0.530692994594574\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.1622728554837605\n",
      "Test accuracy: 0.581976056098938\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.0085077727580996\n",
      "Test accuracy: 0.6012600064277649\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 2.267705204740359\n",
      "Test accuracy: 0.5614243745803833\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.112956341307956\n",
      "Test accuracy: 0.5976490378379822\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 2.3410131473371276\n",
      "Test accuracy: 0.6009910702705383\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 1.7453601613100425\n",
      "Test accuracy: 0.6608020663261414\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.5247543757139135\n",
      "Test accuracy: 0.5266594886779785\n",
      "rate=2.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  3500 1000 2500\n",
      "x_true_pseudo.shape:  (3500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (3500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.7107736149226953\n",
      "Test accuracy: 0.633873701095581\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 1.7391325338676655\n",
      "Test accuracy: 0.6465888023376465\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.262073448265559\n",
      "Test accuracy: 0.6195451617240906\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.283600490153312\n",
      "Test accuracy: 0.6028733849525452\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.1568067613084696\n",
      "Test accuracy: 0.6273816823959351\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 1.8643624538575203\n",
      "Test accuracy: 0.6700599193572998\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 1.8832896889977904\n",
      "Test accuracy: 0.6758604645729065\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 2.1232890827053517\n",
      "Test accuracy: 0.6441687345504761\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 2.828010474236083\n",
      "Test accuracy: 0.5895436406135559\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 2.1382169676618337\n",
      "Test accuracy: 0.6369468569755554\n",
      "rate=5.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  6000 1000 5000\n",
      "x_true_pseudo.shape:  (6000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (6000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.086791838834677\n",
      "Test accuracy: 0.5436002016067505\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.484077366422713\n",
      "Test accuracy: 0.5601951479911804\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.4346297183649335\n",
      "Test accuracy: 0.5726797580718994\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.18449275641626\n",
      "Test accuracy: 0.4789489805698395\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.3505333483988475\n",
      "Test accuracy: 0.5056853294372559\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.0879735276973093\n",
      "Test accuracy: 0.5002688765525818\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.6980264515105974\n",
      "Test accuracy: 0.4963122308254242\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.710538091583346\n",
      "Test accuracy: 0.5005378127098083\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.6713409442878167\n",
      "Test accuracy: 0.48375076055526733\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.834504181868325\n",
      "Test accuracy: 0.4709588289260864\n",
      "rate=7.5:\n",
      "\n",
      "n_total, n_true, n_pseudo:  8500 1000 7500\n",
      "x_true_pseudo.shape:  (8500, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (8500, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.2063973070876175\n",
      "Test accuracy: 0.5572372674942017\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.2743357359886756\n",
      "Test accuracy: 0.5717962384223938\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.1802357527545864\n",
      "Test accuracy: 0.6052935123443604\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.2853819103276134\n",
      "Test accuracy: 0.518976628780365\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.183634476567897\n",
      "Test accuracy: 0.5159035325050354\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.7869014943679127\n",
      "Test accuracy: 0.486631840467453\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 4.073374715258757\n",
      "Test accuracy: 0.4835202693939209\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.252018848810741\n",
      "Test accuracy: 0.5225491523742676\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 4.8736111632724\n",
      "Test accuracy: 0.4564766585826874\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 4.114894242248582\n",
      "Test accuracy: 0.4965043067932129\n",
      "rate=10.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  11000 1000 10000\n",
      "x_true_pseudo.shape:  (11000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (11000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 2.0854431066911645\n",
      "Test accuracy: 0.582590639591217\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 3.156750160122328\n",
      "Test accuracy: 0.514251708984375\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 3.652713676134941\n",
      "Test accuracy: 0.49297019839286804\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 3.053148140186317\n",
      "Test accuracy: 0.5492086410522461\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.8279000876062694\n",
      "Test accuracy: 0.5087584257125854\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.8415697882639974\n",
      "Test accuracy: 0.514405369758606\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.5655928601790734\n",
      "Test accuracy: 0.529118001461029\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.673922368186196\n",
      "Test accuracy: 0.5321527123451233\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.9761683879341168\n",
      "Test accuracy: 0.5140596032142639\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.8969212245120546\n",
      "Test accuracy: 0.5225107669830322\n",
      "rate=15.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  16000 1000 15000\n",
      "x_true_pseudo.shape:  (16000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (16000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.6173148155945096\n",
      "Test accuracy: 0.616126298904419\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 1.9638639833565203\n",
      "Test accuracy: 0.6225414872169495\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.376758742273843\n",
      "Test accuracy: 0.6104025840759277\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.3935433758792515\n",
      "Test accuracy: 0.6347188353538513\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.9214537158361154\n",
      "Test accuracy: 0.6047940850257874\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.002863901117204\n",
      "Test accuracy: 0.5974569916725159\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.8425460381968004\n",
      "Test accuracy: 0.6166256666183472\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.051742546970154\n",
      "Test accuracy: 0.5942686200141907\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.264234654948839\n",
      "Test accuracy: 0.5857406258583069\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.0574044414876647\n",
      "Test accuracy: 0.5988783240318298\n",
      "rate=20.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  21000 1000 20000\n",
      "x_true_pseudo.shape:  (21000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (21000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.8454569797978932\n",
      "Test accuracy: 0.591618001461029\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.6489304201545787\n",
      "Test accuracy: 0.566303014755249\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.7585350817965817\n",
      "Test accuracy: 0.5784419178962708\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.9346771470300244\n",
      "Test accuracy: 0.5868930816650391\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 3.7207597518304265\n",
      "Test accuracy: 0.5499001145362854\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 3.1017914190567852\n",
      "Test accuracy: 0.5898509621620178\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 3.219092479716361\n",
      "Test accuracy: 0.5979948043823242\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 3.9409137868265764\n",
      "Test accuracy: 0.549669623374939\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.6649789317810395\n",
      "Test accuracy: 0.5622311234474182\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 3.3951116582698155\n",
      "Test accuracy: 0.5618853569030762\n",
      "rate=30.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  31000 1000 30000\n",
      "x_true_pseudo.shape:  (31000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (31000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.4039184487371557\n",
      "Test accuracy: 0.6617240309715271\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 1.8919683335086797\n",
      "Test accuracy: 0.6709434390068054\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.4671317982981376\n",
      "Test accuracy: 0.632606029510498\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.51016528656238\n",
      "Test accuracy: 0.6244621872901917\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.664946579977128\n",
      "Test accuracy: 0.6198140978813171\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 2.5576466456394513\n",
      "Test accuracy: 0.6405962109565735\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.9521926914026344\n",
      "Test accuracy: 0.613283634185791\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 2.531311797890549\n",
      "Test accuracy: 0.6437845826148987\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 2.7329476396951047\n",
      "Test accuracy: 0.6242316961288452\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 2.8570468592072236\n",
      "Test accuracy: 0.6268054842948914\n",
      "rate=49.0:\n",
      "\n",
      "n_total, n_true, n_pseudo:  50000 1000 49000\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "iteration:  0\n",
      "Test loss: 1.5163085180041096\n",
      "Test accuracy: 0.6480101346969604\n",
      "1\n",
      "iteration:  1\n",
      "Test loss: 2.1676079047261094\n",
      "Test accuracy: 0.6381760835647583\n",
      "2\n",
      "iteration:  2\n",
      "Test loss: 2.5787367964347743\n",
      "Test accuracy: 0.6324907541275024\n",
      "3\n",
      "iteration:  3\n",
      "Test loss: 2.4207188743862114\n",
      "Test accuracy: 0.6434004306793213\n",
      "4\n",
      "iteration:  4\n",
      "Test loss: 2.876121594265479\n",
      "Test accuracy: 0.6190841794013977\n",
      "5\n",
      "iteration:  5\n",
      "Test loss: 2.6259773672176565\n",
      "Test accuracy: 0.6200445890426636\n",
      "6\n",
      "iteration:  6\n",
      "Test loss: 2.690298951617325\n",
      "Test accuracy: 0.6324907541275024\n",
      "7\n",
      "iteration:  7\n",
      "Test loss: 2.6281916685608464\n",
      "Test accuracy: 0.6412492394447327\n",
      "8\n",
      "iteration:  8\n",
      "Test loss: 3.0159759698796287\n",
      "Test accuracy: 0.6304548382759094\n",
      "9\n",
      "iteration:  9\n",
      "Test loss: 2.9627433219568524\n",
      "Test accuracy: 0.6175092458724976\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Running Test2 for HouseNumbers\n",
    "\n",
    "(x_train_house,y_train_house),(x_test_house,y_test_house) = prepare_house_numbers_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "rate13=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,10,15,20,30,49])\n",
    "model_name= 'keras_HN_trained_teacher_1000_nda.h5'\n",
    "accuracy_13 = stns_5000(init, model_name, x_train_house,y_train_house,x_test_house,y_test_house,rate13,teacher, student, sample_size=1000, \n",
    "                       epochs=100,\n",
    "                      data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test1\n",
    "#Restnet20 on full CIFAR10 train dataset with data augmentation\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_cifar10_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "accuracy_supervised, teacher_path = train_model(init, teacher, x_train, y_train, x_test, y_test, batch_size=32, num_classes=10, epochs=100, \n",
    "                                  data_augmentation=True, model_name= 'keras_cifar10_trained_teacher_da.h5')\n",
    "\n",
    "rate6=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,9])\n",
    "accuracy_6=stns_full_dataset(x_train,y_train,x_test,y_test,rate6,teacher, student,teacher_path,accuracy_supervised, data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "ResNet20v1\n",
      "Learning rate:  0.001\n",
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 73s 1ms/step - loss: 1.4791 - accuracy: 0.5225 - val_loss: 1.4291 - val_accuracy: 0.5355\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      "   96/50000 [..............................] - ETA: 1:04 - loss: 1.0557 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 68s 1ms/step - loss: 1.0596 - accuracy: 0.6831 - val_loss: 1.1484 - val_accuracy: 0.6573\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.8938 - accuracy: 0.7456 - val_loss: 1.2904 - val_accuracy: 0.6324\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.8016 - accuracy: 0.7800 - val_loss: 1.1302 - val_accuracy: 0.6981\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7288 - accuracy: 0.8077 - val_loss: 0.9043 - val_accuracy: 0.7547\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.6790 - accuracy: 0.8278 - val_loss: 0.8182 - val_accuracy: 0.7854\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.6402 - accuracy: 0.8427 - val_loss: 1.0553 - val_accuracy: 0.7187\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.6047 - accuracy: 0.8573 - val_loss: 0.9209 - val_accuracy: 0.7591\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.5762 - accuracy: 0.8694 - val_loss: 1.0430 - val_accuracy: 0.7470\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.5533 - accuracy: 0.8789 - val_loss: 0.9499 - val_accuracy: 0.7584\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.5313 - accuracy: 0.8881 - val_loss: 0.9150 - val_accuracy: 0.7751\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.5122 - accuracy: 0.8978 - val_loss: 1.0866 - val_accuracy: 0.7456\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4960 - accuracy: 0.9041 - val_loss: 0.9370 - val_accuracy: 0.7843\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4921 - accuracy: 0.9079 - val_loss: 1.1422 - val_accuracy: 0.7509\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.4747 - accuracy: 0.9158 - val_loss: 1.2291 - val_accuracy: 0.7350\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4715 - accuracy: 0.9178 - val_loss: 1.0303 - val_accuracy: 0.7715\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4607 - accuracy: 0.9221 - val_loss: 1.1571 - val_accuracy: 0.7598\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4573 - accuracy: 0.9256 - val_loss: 1.0834 - val_accuracy: 0.7763\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4517 - accuracy: 0.9282 - val_loss: 1.1303 - val_accuracy: 0.7711\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4465 - accuracy: 0.9307 - val_loss: 1.2841 - val_accuracy: 0.7682\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4458 - accuracy: 0.9332 - val_loss: 1.1925 - val_accuracy: 0.7635\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4414 - accuracy: 0.9352 - val_loss: 1.1332 - val_accuracy: 0.7678\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4347 - accuracy: 0.9392 - val_loss: 1.1532 - val_accuracy: 0.7780\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4391 - accuracy: 0.9383 - val_loss: 1.6444 - val_accuracy: 0.7042\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4328 - accuracy: 0.9404 - val_loss: 1.1588 - val_accuracy: 0.7811\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4316 - accuracy: 0.9409 - val_loss: 1.0256 - val_accuracy: 0.7912\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4342 - accuracy: 0.9422 - val_loss: 1.0842 - val_accuracy: 0.7857\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4320 - accuracy: 0.9429 - val_loss: 1.4590 - val_accuracy: 0.7250\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4261 - accuracy: 0.9455 - val_loss: 1.1074 - val_accuracy: 0.7854\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4299 - accuracy: 0.9436 - val_loss: 1.0852 - val_accuracy: 0.7963\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4240 - accuracy: 0.9464 - val_loss: 1.1161 - val_accuracy: 0.7797\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4308 - accuracy: 0.9437 - val_loss: 1.1844 - val_accuracy: 0.7816\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4256 - accuracy: 0.9467 - val_loss: 1.0569 - val_accuracy: 0.7975\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4229 - accuracy: 0.9474 - val_loss: 1.4079 - val_accuracy: 0.7431\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4254 - accuracy: 0.9465 - val_loss: 1.0675 - val_accuracy: 0.8013\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4217 - accuracy: 0.9476 - val_loss: 1.2599 - val_accuracy: 0.7843\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4205 - accuracy: 0.9484 - val_loss: 1.0747 - val_accuracy: 0.7978\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4176 - accuracy: 0.9488 - val_loss: 1.0671 - val_accuracy: 0.7968\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4206 - accuracy: 0.9487 - val_loss: 1.6147 - val_accuracy: 0.7269\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4187 - accuracy: 0.9498 - val_loss: 1.1537 - val_accuracy: 0.7895\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4170 - accuracy: 0.9502 - val_loss: 1.2317 - val_accuracy: 0.7600\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4186 - accuracy: 0.9498 - val_loss: 1.1382 - val_accuracy: 0.7890\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4217 - accuracy: 0.9482 - val_loss: 1.0920 - val_accuracy: 0.7970\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4097 - accuracy: 0.9530 - val_loss: 1.0457 - val_accuracy: 0.7966\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.4144 - accuracy: 0.9511 - val_loss: 1.3197 - val_accuracy: 0.7743\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4131 - accuracy: 0.9512 - val_loss: 1.1924 - val_accuracy: 0.7822\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4119 - accuracy: 0.9525 - val_loss: 1.1698 - val_accuracy: 0.7780\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.4103 - accuracy: 0.9527 - val_loss: 1.1751 - val_accuracy: 0.7748\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4150 - accuracy: 0.9518 - val_loss: 1.0373 - val_accuracy: 0.8043\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4083 - accuracy: 0.9527 - val_loss: 1.0837 - val_accuracy: 0.8034\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4035 - accuracy: 0.9545 - val_loss: 1.4627 - val_accuracy: 0.7584\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4121 - accuracy: 0.9520 - val_loss: 1.1259 - val_accuracy: 0.7865\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4078 - accuracy: 0.9537 - val_loss: 1.2647 - val_accuracy: 0.7789\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4031 - accuracy: 0.9534 - val_loss: 1.1269 - val_accuracy: 0.8025\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4010 - accuracy: 0.9538 - val_loss: 1.1488 - val_accuracy: 0.7852\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4106 - accuracy: 0.9513 - val_loss: 1.0558 - val_accuracy: 0.7964\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4071 - accuracy: 0.9528 - val_loss: 1.1202 - val_accuracy: 0.7953\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4004 - accuracy: 0.9550 - val_loss: 1.1320 - val_accuracy: 0.7927\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4017 - accuracy: 0.9544 - val_loss: 1.2676 - val_accuracy: 0.7732\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4040 - accuracy: 0.9544 - val_loss: 1.1124 - val_accuracy: 0.7984\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.4009 - accuracy: 0.9549 - val_loss: 1.2036 - val_accuracy: 0.7758\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4032 - accuracy: 0.9553 - val_loss: 1.3328 - val_accuracy: 0.7574\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.4024 - accuracy: 0.9548 - val_loss: 1.0798 - val_accuracy: 0.7909\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3959 - accuracy: 0.9561 - val_loss: 1.2771 - val_accuracy: 0.7807\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3981 - accuracy: 0.9560 - val_loss: 1.0196 - val_accuracy: 0.8076\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.4009 - accuracy: 0.9554 - val_loss: 1.0011 - val_accuracy: 0.8082\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3941 - accuracy: 0.9575 - val_loss: 1.0851 - val_accuracy: 0.8028\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3976 - accuracy: 0.9554 - val_loss: 1.0915 - val_accuracy: 0.8073\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.3980 - accuracy: 0.9556 - val_loss: 1.1573 - val_accuracy: 0.7884\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.3970 - accuracy: 0.9556 - val_loss: 1.1033 - val_accuracy: 0.7858\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3959 - accuracy: 0.9562 - val_loss: 1.4388 - val_accuracy: 0.7479\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3983 - accuracy: 0.9561 - val_loss: 1.2605 - val_accuracy: 0.7746\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3900 - accuracy: 0.9589 - val_loss: 1.1776 - val_accuracy: 0.7899\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3944 - accuracy: 0.9562 - val_loss: 1.1983 - val_accuracy: 0.7783\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3975 - accuracy: 0.9552 - val_loss: 1.3109 - val_accuracy: 0.7720\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3873 - accuracy: 0.9588 - val_loss: 1.0786 - val_accuracy: 0.8016\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3878 - accuracy: 0.9575 - val_loss: 1.1417 - val_accuracy: 0.7976\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.3937 - accuracy: 0.9554 - val_loss: 1.0736 - val_accuracy: 0.8143\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.3854 - accuracy: 0.9588 - val_loss: 1.2393 - val_accuracy: 0.7753\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.3925 - accuracy: 0.9568 - val_loss: 1.2215 - val_accuracy: 0.7878\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.3892 - accuracy: 0.9569 - val_loss: 1.0567 - val_accuracy: 0.8082\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.3174 - accuracy: 0.9854 - val_loss: 0.9277 - val_accuracy: 0.8359\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.2848 - accuracy: 0.9953 - val_loss: 0.9463 - val_accuracy: 0.8381\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.2695 - accuracy: 0.9974 - val_loss: 0.9577 - val_accuracy: 0.8399\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.2565 - accuracy: 0.9985 - val_loss: 0.9836 - val_accuracy: 0.8389\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.2434 - accuracy: 0.9991 - val_loss: 1.0000 - val_accuracy: 0.8375\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.2316 - accuracy: 0.9989 - val_loss: 0.9892 - val_accuracy: 0.8376\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.2192 - accuracy: 0.9993 - val_loss: 1.0150 - val_accuracy: 0.8353\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.2075 - accuracy: 0.9996 - val_loss: 1.0248 - val_accuracy: 0.8377\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1971 - accuracy: 0.9994 - val_loss: 1.0124 - val_accuracy: 0.8388\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.1876 - accuracy: 0.9993 - val_loss: 1.0090 - val_accuracy: 0.8397\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.1788 - accuracy: 0.9993 - val_loss: 1.0487 - val_accuracy: 0.8325\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1714 - accuracy: 0.9994 - val_loss: 1.0241 - val_accuracy: 0.8369\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.1636 - accuracy: 0.9997 - val_loss: 1.0555 - val_accuracy: 0.8345\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.1574 - accuracy: 0.9992 - val_loss: 1.0373 - val_accuracy: 0.8386\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.1509 - accuracy: 0.9996 - val_loss: 1.0607 - val_accuracy: 0.8349\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.1461 - accuracy: 0.9992 - val_loss: 1.0778 - val_accuracy: 0.8347\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.1412 - accuracy: 0.9993 - val_loss: 1.1048 - val_accuracy: 0.8279\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.1375 - accuracy: 0.9991 - val_loss: 1.0599 - val_accuracy: 0.8323\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.1323 - accuracy: 0.9995 - val_loss: 1.0796 - val_accuracy: 0.8357\n",
      "10000/10000 [==============================] - 3s 349us/step\n",
      "Supervised learning model with 100epochs \n",
      "\n",
      "Test loss: 1.0796337428092957\n",
      "Test accuracy: 0.8356999754905701\n",
      "Saved trained model at /home/ANDURAND/pandurand/saved_models/keras_cifar10_trained_teacher_nda.h5 \n",
      "rate=0.05:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (2381, 10)\n",
      "Shape y_true (47619, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 74s 47ms/step - loss: 1.1330 - accuracy: 0.7314 - val_loss: 0.7827 - val_accuracy: 0.7927\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "   3/1563 [..............................] - ETA: 1:02 - loss: 1.1231 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8430 - accuracy: 0.7769 - val_loss: 0.6990 - val_accuracy: 0.8212\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7992 - accuracy: 0.7945 - val_loss: 0.8312 - val_accuracy: 0.7784\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7767 - accuracy: 0.8053 - val_loss: 0.7501 - val_accuracy: 0.8072\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7650 - accuracy: 0.8122 - val_loss: 0.8185 - val_accuracy: 0.8060\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7551 - accuracy: 0.8191 - val_loss: 0.7641 - val_accuracy: 0.8140\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7322 - accuracy: 0.8293 - val_loss: 0.7768 - val_accuracy: 0.8094\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7356 - accuracy: 0.8278 - val_loss: 0.7683 - val_accuracy: 0.8252\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7185 - accuracy: 0.8345 - val_loss: 0.7840 - val_accuracy: 0.8122\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7151 - accuracy: 0.8373 - val_loss: 0.7824 - val_accuracy: 0.8145\n",
      "iteration:  0\n",
      "Test loss: 0.782357534122467\n",
      "Test accuracy: 0.8144999742507935\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7147 - accuracy: 0.8404 - val_loss: 0.9124 - val_accuracy: 0.7843\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7141 - accuracy: 0.8409 - val_loss: 0.7647 - val_accuracy: 0.8273\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7025 - accuracy: 0.8453 - val_loss: 0.7068 - val_accuracy: 0.8398\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6952 - accuracy: 0.8472 - val_loss: 0.7117 - val_accuracy: 0.8417\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6976 - accuracy: 0.8470 - val_loss: 0.7436 - val_accuracy: 0.8325\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6841 - accuracy: 0.8506 - val_loss: 0.7672 - val_accuracy: 0.8157\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6880 - accuracy: 0.8509 - val_loss: 0.8132 - val_accuracy: 0.8141\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6752 - accuracy: 0.8554 - val_loss: 0.7356 - val_accuracy: 0.8363\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6785 - accuracy: 0.8536 - val_loss: 0.7099 - val_accuracy: 0.8422\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6653 - accuracy: 0.8600 - val_loss: 0.7709 - val_accuracy: 0.8244\n",
      "iteration:  1\n",
      "Test loss: 0.7709169481277466\n",
      "Test accuracy: 0.824400007724762\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6691 - accuracy: 0.8595 - val_loss: 0.6822 - val_accuracy: 0.8486\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6694 - accuracy: 0.8579 - val_loss: 0.7074 - val_accuracy: 0.8468\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6619 - accuracy: 0.8620 - val_loss: 0.7330 - val_accuracy: 0.8423\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6563 - accuracy: 0.8624 - val_loss: 0.7100 - val_accuracy: 0.8463\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6640 - accuracy: 0.8593 - val_loss: 0.9050 - val_accuracy: 0.7973\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6545 - accuracy: 0.8632 - val_loss: 0.8150 - val_accuracy: 0.8197\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6436 - accuracy: 0.8669 - val_loss: 0.7669 - val_accuracy: 0.8246\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6473 - accuracy: 0.8667 - val_loss: 0.7742 - val_accuracy: 0.8256\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6477 - accuracy: 0.8671 - val_loss: 0.7271 - val_accuracy: 0.8448\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6464 - accuracy: 0.8664 - val_loss: 0.8815 - val_accuracy: 0.8027\n",
      "iteration:  2\n",
      "Test loss: 0.8815434845924377\n",
      "Test accuracy: 0.8026999831199646\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6510 - accuracy: 0.8671 - val_loss: 0.6655 - val_accuracy: 0.8612\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6503 - accuracy: 0.8673 - val_loss: 0.9362 - val_accuracy: 0.7970\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6456 - accuracy: 0.8675 - val_loss: 0.7089 - val_accuracy: 0.8488\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6439 - accuracy: 0.8692 - val_loss: 0.8217 - val_accuracy: 0.8152\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6429 - accuracy: 0.8686 - val_loss: 0.8843 - val_accuracy: 0.8124\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6416 - accuracy: 0.8685 - val_loss: 0.7373 - val_accuracy: 0.8393\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6335 - accuracy: 0.8716 - val_loss: 0.7148 - val_accuracy: 0.8422\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6340 - accuracy: 0.8695 - val_loss: 0.7318 - val_accuracy: 0.8398\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6386 - accuracy: 0.8703 - val_loss: 0.7091 - val_accuracy: 0.8485\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6316 - accuracy: 0.8716 - val_loss: 0.9503 - val_accuracy: 0.7969\n",
      "iteration:  3\n",
      "Test loss: 0.9502670852661133\n",
      "Test accuracy: 0.7968999743461609\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6378 - accuracy: 0.8723 - val_loss: 0.8952 - val_accuracy: 0.7962\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6323 - accuracy: 0.8729 - val_loss: 0.7576 - val_accuracy: 0.8375\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6262 - accuracy: 0.8746 - val_loss: 0.7669 - val_accuracy: 0.8337\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6310 - accuracy: 0.8748 - val_loss: 0.9560 - val_accuracy: 0.7868\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6242 - accuracy: 0.8751 - val_loss: 0.6854 - val_accuracy: 0.8567\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6275 - accuracy: 0.8739 - val_loss: 0.6889 - val_accuracy: 0.8527\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6240 - accuracy: 0.8735 - val_loss: 0.7422 - val_accuracy: 0.8407\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6224 - accuracy: 0.8759 - val_loss: 0.7662 - val_accuracy: 0.8350\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6244 - accuracy: 0.8753 - val_loss: 0.6941 - val_accuracy: 0.8507\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6196 - accuracy: 0.8772 - val_loss: 0.6415 - val_accuracy: 0.8700\n",
      "iteration:  4\n",
      "Test loss: 0.6415331131935119\n",
      "Test accuracy: 0.8700000047683716\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6073 - accuracy: 0.8792 - val_loss: 0.7304 - val_accuracy: 0.8459\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6064 - accuracy: 0.8801 - val_loss: 0.6883 - val_accuracy: 0.8559\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6069 - accuracy: 0.8780 - val_loss: 0.7241 - val_accuracy: 0.8464\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5998 - accuracy: 0.8824 - val_loss: 0.7029 - val_accuracy: 0.8522\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6024 - accuracy: 0.8797 - val_loss: 0.7366 - val_accuracy: 0.8374\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5994 - accuracy: 0.8828 - val_loss: 0.7971 - val_accuracy: 0.8355\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5964 - accuracy: 0.8839 - val_loss: 0.6908 - val_accuracy: 0.8587\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5972 - accuracy: 0.8825 - val_loss: 0.7291 - val_accuracy: 0.8447\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5944 - accuracy: 0.8844 - val_loss: 0.7152 - val_accuracy: 0.8487\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6004 - accuracy: 0.8836 - val_loss: 0.8185 - val_accuracy: 0.8299\n",
      "iteration:  5\n",
      "Test loss: 0.818502116394043\n",
      "Test accuracy: 0.8299000263214111\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6110 - accuracy: 0.8806 - val_loss: 0.7628 - val_accuracy: 0.8407\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6099 - accuracy: 0.8810 - val_loss: 0.7162 - val_accuracy: 0.8488\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6066 - accuracy: 0.8821 - val_loss: 0.6749 - val_accuracy: 0.8581\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6004 - accuracy: 0.8833 - val_loss: 0.7309 - val_accuracy: 0.8438\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6021 - accuracy: 0.8833 - val_loss: 0.6865 - val_accuracy: 0.8587\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5984 - accuracy: 0.8829 - val_loss: 0.8222 - val_accuracy: 0.8263\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6093 - accuracy: 0.8825 - val_loss: 0.7067 - val_accuracy: 0.8478\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6019 - accuracy: 0.8828 - val_loss: 0.8005 - val_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5972 - accuracy: 0.8848 - val_loss: 0.7523 - val_accuracy: 0.8465\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5942 - accuracy: 0.8854 - val_loss: 0.7111 - val_accuracy: 0.8492\n",
      "iteration:  6\n",
      "Test loss: 0.7110700438022614\n",
      "Test accuracy: 0.8492000102996826\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6003 - accuracy: 0.8845 - val_loss: 0.6536 - val_accuracy: 0.8678\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5949 - accuracy: 0.8855 - val_loss: 0.7685 - val_accuracy: 0.8334\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5920 - accuracy: 0.8869 - val_loss: 0.6878 - val_accuracy: 0.8578\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5912 - accuracy: 0.8866 - val_loss: 0.6826 - val_accuracy: 0.8597\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5878 - accuracy: 0.8862 - val_loss: 0.6933 - val_accuracy: 0.8562\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5940 - accuracy: 0.8849 - val_loss: 0.6773 - val_accuracy: 0.8564\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5893 - accuracy: 0.8868 - val_loss: 0.6254 - val_accuracy: 0.8750\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5877 - accuracy: 0.8879 - val_loss: 0.8721 - val_accuracy: 0.8196\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5873 - accuracy: 0.8870 - val_loss: 0.6840 - val_accuracy: 0.8600\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5929 - accuracy: 0.8861 - val_loss: 0.6701 - val_accuracy: 0.8651\n",
      "iteration:  7\n",
      "Test loss: 0.6701017226219177\n",
      "Test accuracy: 0.8651000261306763\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5837 - accuracy: 0.8880 - val_loss: 0.8354 - val_accuracy: 0.8216\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5845 - accuracy: 0.8897 - val_loss: 0.6679 - val_accuracy: 0.8652\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5853 - accuracy: 0.8889 - val_loss: 0.7163 - val_accuracy: 0.8506\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5850 - accuracy: 0.8893 - val_loss: 0.7097 - val_accuracy: 0.8550\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5838 - accuracy: 0.8885 - val_loss: 0.7628 - val_accuracy: 0.8465\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5863 - accuracy: 0.8883 - val_loss: 0.6677 - val_accuracy: 0.8638\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5765 - accuracy: 0.8908 - val_loss: 0.7485 - val_accuracy: 0.8435\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5778 - accuracy: 0.8892 - val_loss: 0.7082 - val_accuracy: 0.8584\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5830 - accuracy: 0.8890 - val_loss: 0.7100 - val_accuracy: 0.8491\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5830 - accuracy: 0.8881 - val_loss: 0.6345 - val_accuracy: 0.8684\n",
      "iteration:  8\n",
      "Test loss: 0.6345367740631104\n",
      "Test accuracy: 0.868399977684021\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5816 - accuracy: 0.8897 - val_loss: 0.7094 - val_accuracy: 0.8552\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5776 - accuracy: 0.8896 - val_loss: 0.7896 - val_accuracy: 0.8263\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5797 - accuracy: 0.8903 - val_loss: 0.7128 - val_accuracy: 0.8542\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5747 - accuracy: 0.8913 - val_loss: 0.7462 - val_accuracy: 0.8443\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5754 - accuracy: 0.8918 - val_loss: 0.7089 - val_accuracy: 0.8580\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5738 - accuracy: 0.8917 - val_loss: 0.7076 - val_accuracy: 0.8494\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5752 - accuracy: 0.8905 - val_loss: 0.7136 - val_accuracy: 0.8480\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5678 - accuracy: 0.8932 - val_loss: 0.6843 - val_accuracy: 0.8589\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5719 - accuracy: 0.8920 - val_loss: 0.6457 - val_accuracy: 0.8690\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5736 - accuracy: 0.8909 - val_loss: 0.6417 - val_accuracy: 0.8699\n",
      "iteration:  9\n",
      "Test loss: 0.6416845315933227\n",
      "Test accuracy: 0.8698999881744385\n",
      "rate=0.1:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (4546, 10)\n",
      "Shape y_true (45454, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 1.1944 - accuracy: 0.6894 - val_loss: 0.9490 - val_accuracy: 0.7554\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8724 - accuracy: 0.7741 - val_loss: 0.8683 - val_accuracy: 0.7676\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8232 - accuracy: 0.7919 - val_loss: 0.8763 - val_accuracy: 0.7815\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7935 - accuracy: 0.8051 - val_loss: 0.7856 - val_accuracy: 0.7987\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7752 - accuracy: 0.8119 - val_loss: 0.7616 - val_accuracy: 0.8134\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7534 - accuracy: 0.8207 - val_loss: 0.7657 - val_accuracy: 0.8138\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.7457 - accuracy: 0.8274 - val_loss: 0.7466 - val_accuracy: 0.8249\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7329 - accuracy: 0.8299 - val_loss: 0.7356 - val_accuracy: 0.8238\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7220 - accuracy: 0.8344 - val_loss: 0.7228 - val_accuracy: 0.8356\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7141 - accuracy: 0.8371 - val_loss: 0.8117 - val_accuracy: 0.8175\n",
      "iteration:  0\n",
      "Test loss: 0.8117338222503662\n",
      "Test accuracy: 0.8174999952316284\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7244 - accuracy: 0.8369 - val_loss: 0.8154 - val_accuracy: 0.8086\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7266 - accuracy: 0.8396 - val_loss: 0.9123 - val_accuracy: 0.7791\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7147 - accuracy: 0.8409 - val_loss: 0.7090 - val_accuracy: 0.8403\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7022 - accuracy: 0.8448 - val_loss: 0.7706 - val_accuracy: 0.8210\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7019 - accuracy: 0.8450 - val_loss: 0.8145 - val_accuracy: 0.8117\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6924 - accuracy: 0.8464 - val_loss: 0.7023 - val_accuracy: 0.8412\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6937 - accuracy: 0.8492 - val_loss: 0.7690 - val_accuracy: 0.8238\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6860 - accuracy: 0.8514 - val_loss: 0.7299 - val_accuracy: 0.8390\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6834 - accuracy: 0.8516 - val_loss: 0.8334 - val_accuracy: 0.8098\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6768 - accuracy: 0.8540 - val_loss: 0.8071 - val_accuracy: 0.8108\n",
      "iteration:  1\n",
      "Test loss: 0.8070652609825134\n",
      "Test accuracy: 0.8108000159263611\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6848 - accuracy: 0.8528 - val_loss: 0.7553 - val_accuracy: 0.8324\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6791 - accuracy: 0.8546 - val_loss: 0.7286 - val_accuracy: 0.8382\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6729 - accuracy: 0.8553 - val_loss: 0.7857 - val_accuracy: 0.8187\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6697 - accuracy: 0.8576 - val_loss: 0.7504 - val_accuracy: 0.8356\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6658 - accuracy: 0.8580 - val_loss: 0.7086 - val_accuracy: 0.8487\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6612 - accuracy: 0.8591 - val_loss: 0.7545 - val_accuracy: 0.8319\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6610 - accuracy: 0.8597 - val_loss: 0.7733 - val_accuracy: 0.8262\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6602 - accuracy: 0.8588 - val_loss: 0.7627 - val_accuracy: 0.8302\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6585 - accuracy: 0.8607 - val_loss: 0.7213 - val_accuracy: 0.8390\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6548 - accuracy: 0.8622 - val_loss: 0.8278 - val_accuracy: 0.8187\n",
      "iteration:  2\n",
      "Test loss: 0.8278353197574615\n",
      "Test accuracy: 0.8187000155448914\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6771 - accuracy: 0.8576 - val_loss: 0.7723 - val_accuracy: 0.8313\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6608 - accuracy: 0.8625 - val_loss: 0.7090 - val_accuracy: 0.8389\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6560 - accuracy: 0.8646 - val_loss: 0.6829 - val_accuracy: 0.8516\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6551 - accuracy: 0.8640 - val_loss: 0.8063 - val_accuracy: 0.8274\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6528 - accuracy: 0.8634 - val_loss: 0.7359 - val_accuracy: 0.8333\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6498 - accuracy: 0.8669 - val_loss: 0.6795 - val_accuracy: 0.8530\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6489 - accuracy: 0.8648 - val_loss: 0.8252 - val_accuracy: 0.8159\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6459 - accuracy: 0.8664 - val_loss: 0.7008 - val_accuracy: 0.8458\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6427 - accuracy: 0.8686 - val_loss: 0.6954 - val_accuracy: 0.8493\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6409 - accuracy: 0.8677 - val_loss: 0.6653 - val_accuracy: 0.8553\n",
      "iteration:  3\n",
      "Test loss: 0.665334663772583\n",
      "Test accuracy: 0.8553000092506409\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6287 - accuracy: 0.8710 - val_loss: 0.6762 - val_accuracy: 0.8511\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6293 - accuracy: 0.8725 - val_loss: 0.8597 - val_accuracy: 0.8060\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6218 - accuracy: 0.8753 - val_loss: 0.8072 - val_accuracy: 0.8261\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6213 - accuracy: 0.8745 - val_loss: 0.7413 - val_accuracy: 0.8417\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6146 - accuracy: 0.8774 - val_loss: 0.7948 - val_accuracy: 0.8323\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6143 - accuracy: 0.8764 - val_loss: 0.7134 - val_accuracy: 0.8434\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6114 - accuracy: 0.8754 - val_loss: 0.6764 - val_accuracy: 0.8508\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6154 - accuracy: 0.8752 - val_loss: 0.6871 - val_accuracy: 0.8576\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6095 - accuracy: 0.8786 - val_loss: 0.8401 - val_accuracy: 0.8308\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6133 - accuracy: 0.8766 - val_loss: 0.6761 - val_accuracy: 0.8549\n",
      "iteration:  4\n",
      "Test loss: 0.6760758618354797\n",
      "Test accuracy: 0.8549000024795532\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 66s 43ms/step - loss: 0.6158 - accuracy: 0.8757 - val_loss: 0.7471 - val_accuracy: 0.8383\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6106 - accuracy: 0.8779 - val_loss: 0.7335 - val_accuracy: 0.8362\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6131 - accuracy: 0.8753 - val_loss: 0.7414 - val_accuracy: 0.8400\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6072 - accuracy: 0.8783 - val_loss: 0.7083 - val_accuracy: 0.8435\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6123 - accuracy: 0.8759 - val_loss: 0.7461 - val_accuracy: 0.8436\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6059 - accuracy: 0.8797 - val_loss: 0.6767 - val_accuracy: 0.8570\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6118 - accuracy: 0.8775 - val_loss: 0.7019 - val_accuracy: 0.8500\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6015 - accuracy: 0.8825 - val_loss: 0.6782 - val_accuracy: 0.8538\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6059 - accuracy: 0.8815 - val_loss: 0.6869 - val_accuracy: 0.8563\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6035 - accuracy: 0.8807 - val_loss: 0.7080 - val_accuracy: 0.8511\n",
      "iteration:  5\n",
      "Test loss: 0.7079633549690246\n",
      "Test accuracy: 0.8511000275611877\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6042 - accuracy: 0.8804 - val_loss: 0.6689 - val_accuracy: 0.8589\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6057 - accuracy: 0.8813 - val_loss: 0.7856 - val_accuracy: 0.8289\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6058 - accuracy: 0.8798 - val_loss: 0.7110 - val_accuracy: 0.8540\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6002 - accuracy: 0.8803 - val_loss: 0.9068 - val_accuracy: 0.8058\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6006 - accuracy: 0.8811 - val_loss: 0.7883 - val_accuracy: 0.8329\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5975 - accuracy: 0.8830 - val_loss: 0.6544 - val_accuracy: 0.8637\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5962 - accuracy: 0.8828 - val_loss: 0.8231 - val_accuracy: 0.8253\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6002 - accuracy: 0.8819 - val_loss: 0.7227 - val_accuracy: 0.8439\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5935 - accuracy: 0.8844 - val_loss: 0.8431 - val_accuracy: 0.8226\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5999 - accuracy: 0.8820 - val_loss: 0.7601 - val_accuracy: 0.8360\n",
      "iteration:  6\n",
      "Test loss: 0.7600861882209777\n",
      "Test accuracy: 0.8360000252723694\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6105 - accuracy: 0.8799 - val_loss: 0.7273 - val_accuracy: 0.8415\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6093 - accuracy: 0.8785 - val_loss: 0.7787 - val_accuracy: 0.8334\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6045 - accuracy: 0.8829 - val_loss: 0.8219 - val_accuracy: 0.8247\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6020 - accuracy: 0.8821 - val_loss: 0.7611 - val_accuracy: 0.8390\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5990 - accuracy: 0.8844 - val_loss: 0.6963 - val_accuracy: 0.8490\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5996 - accuracy: 0.8820 - val_loss: 0.8741 - val_accuracy: 0.8087\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5976 - accuracy: 0.8819 - val_loss: 0.7826 - val_accuracy: 0.8356\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5982 - accuracy: 0.8832 - val_loss: 0.7831 - val_accuracy: 0.8359\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5952 - accuracy: 0.8843 - val_loss: 0.7101 - val_accuracy: 0.8491\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5998 - accuracy: 0.8838 - val_loss: 0.7025 - val_accuracy: 0.8504\n",
      "iteration:  7\n",
      "Test loss: 0.7024591925621033\n",
      "Test accuracy: 0.8503999710083008\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5968 - accuracy: 0.8846 - val_loss: 0.7059 - val_accuracy: 0.8485\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5974 - accuracy: 0.8834 - val_loss: 0.7954 - val_accuracy: 0.8328\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5957 - accuracy: 0.8844 - val_loss: 0.6406 - val_accuracy: 0.8661\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5940 - accuracy: 0.8849 - val_loss: 0.9318 - val_accuracy: 0.7996\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5902 - accuracy: 0.8845 - val_loss: 0.6479 - val_accuracy: 0.8658\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5857 - accuracy: 0.8861 - val_loss: 0.6738 - val_accuracy: 0.8583\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.5905 - accuracy: 0.8839 - val_loss: 0.7709 - val_accuracy: 0.8406\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5855 - accuracy: 0.8871 - val_loss: 0.7031 - val_accuracy: 0.8474\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5865 - accuracy: 0.8872 - val_loss: 0.7883 - val_accuracy: 0.8343\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5901 - accuracy: 0.8871 - val_loss: 0.7419 - val_accuracy: 0.8545\n",
      "iteration:  8\n",
      "Test loss: 0.7418776921272278\n",
      "Test accuracy: 0.8544999957084656\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5946 - accuracy: 0.8855 - val_loss: 0.6910 - val_accuracy: 0.8554\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5892 - accuracy: 0.8855 - val_loss: 0.6510 - val_accuracy: 0.8670\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.5902 - accuracy: 0.8853 - val_loss: 0.7048 - val_accuracy: 0.8509\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5892 - accuracy: 0.8870 - val_loss: 0.6928 - val_accuracy: 0.8574\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5892 - accuracy: 0.8849 - val_loss: 0.6763 - val_accuracy: 0.8613\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5841 - accuracy: 0.8870 - val_loss: 0.6540 - val_accuracy: 0.8607\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.5799 - accuracy: 0.8872 - val_loss: 0.6395 - val_accuracy: 0.8707\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5815 - accuracy: 0.8883 - val_loss: 0.7158 - val_accuracy: 0.8529\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5810 - accuracy: 0.8874 - val_loss: 0.6909 - val_accuracy: 0.8533\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5831 - accuracy: 0.8878 - val_loss: 0.6587 - val_accuracy: 0.8625\n",
      "iteration:  9\n",
      "Test loss: 0.6586990489006043\n",
      "Test accuracy: 0.862500011920929\n",
      "rate=0.2:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (8334, 10)\n",
      "Shape y_true (41666, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 1.1637 - accuracy: 0.6946 - val_loss: 0.9115 - val_accuracy: 0.7598\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8843 - accuracy: 0.7689 - val_loss: 0.8076 - val_accuracy: 0.7964\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8269 - accuracy: 0.7915 - val_loss: 0.8913 - val_accuracy: 0.7753\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7963 - accuracy: 0.8030 - val_loss: 0.9851 - val_accuracy: 0.7619\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7747 - accuracy: 0.8130 - val_loss: 0.7803 - val_accuracy: 0.8114\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7687 - accuracy: 0.8175 - val_loss: 0.9277 - val_accuracy: 0.7834\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7499 - accuracy: 0.8251 - val_loss: 0.8670 - val_accuracy: 0.7962\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7410 - accuracy: 0.8299 - val_loss: 0.7730 - val_accuracy: 0.8132\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7261 - accuracy: 0.8350 - val_loss: 0.7084 - val_accuracy: 0.8361\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7146 - accuracy: 0.8389 - val_loss: 0.7078 - val_accuracy: 0.8343\n",
      "iteration:  0\n",
      "Test loss: 0.7077526412010193\n",
      "Test accuracy: 0.8342999815940857\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7200 - accuracy: 0.8379 - val_loss: 0.7688 - val_accuracy: 0.8197\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7183 - accuracy: 0.8374 - val_loss: 0.7473 - val_accuracy: 0.8302\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7114 - accuracy: 0.8424 - val_loss: 0.8357 - val_accuracy: 0.8115\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7027 - accuracy: 0.8466 - val_loss: 0.7320 - val_accuracy: 0.8327\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6929 - accuracy: 0.8474 - val_loss: 0.6783 - val_accuracy: 0.8503\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6942 - accuracy: 0.8476 - val_loss: 0.9140 - val_accuracy: 0.7889\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6898 - accuracy: 0.8514 - val_loss: 0.7330 - val_accuracy: 0.8338\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6831 - accuracy: 0.8518 - val_loss: 0.7906 - val_accuracy: 0.8236\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6798 - accuracy: 0.8518 - val_loss: 0.7375 - val_accuracy: 0.8346\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6747 - accuracy: 0.8539 - val_loss: 0.7333 - val_accuracy: 0.8315\n",
      "iteration:  1\n",
      "Test loss: 0.7332680767059326\n",
      "Test accuracy: 0.8314999938011169\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6912 - accuracy: 0.8522 - val_loss: 0.7682 - val_accuracy: 0.8290\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6849 - accuracy: 0.8528 - val_loss: 0.7427 - val_accuracy: 0.8383\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6789 - accuracy: 0.8540 - val_loss: 0.7731 - val_accuracy: 0.8276\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6803 - accuracy: 0.8552 - val_loss: 0.9274 - val_accuracy: 0.7802\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6762 - accuracy: 0.8571 - val_loss: 0.8451 - val_accuracy: 0.8037\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6766 - accuracy: 0.8565 - val_loss: 0.8408 - val_accuracy: 0.8010\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6681 - accuracy: 0.8592 - val_loss: 0.8219 - val_accuracy: 0.8191\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6688 - accuracy: 0.8587 - val_loss: 0.7245 - val_accuracy: 0.8277\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6640 - accuracy: 0.8599 - val_loss: 0.7863 - val_accuracy: 0.8212\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6657 - accuracy: 0.8602 - val_loss: 0.7338 - val_accuracy: 0.8339\n",
      "iteration:  2\n",
      "Test loss: 0.733762852859497\n",
      "Test accuracy: 0.833899974822998\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6699 - accuracy: 0.8593 - val_loss: 0.7187 - val_accuracy: 0.8370\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6674 - accuracy: 0.8590 - val_loss: 0.6983 - val_accuracy: 0.8385\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6601 - accuracy: 0.8609 - val_loss: 0.9069 - val_accuracy: 0.7948\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6582 - accuracy: 0.8603 - val_loss: 0.8135 - val_accuracy: 0.8109\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6432 - accuracy: 0.8646 - val_loss: 0.8692 - val_accuracy: 0.8131\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6475 - accuracy: 0.8639 - val_loss: 0.7424 - val_accuracy: 0.8362\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6506 - accuracy: 0.8640 - val_loss: 0.7350 - val_accuracy: 0.8308\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6415 - accuracy: 0.8647 - val_loss: 0.7147 - val_accuracy: 0.8373\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6445 - accuracy: 0.8649 - val_loss: 0.7680 - val_accuracy: 0.8231\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6442 - accuracy: 0.8654 - val_loss: 0.8019 - val_accuracy: 0.8203\n",
      "iteration:  3\n",
      "Test loss: 0.8018941651344299\n",
      "Test accuracy: 0.8202999830245972\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6622 - accuracy: 0.8611 - val_loss: 0.7651 - val_accuracy: 0.8220\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6517 - accuracy: 0.8624 - val_loss: 0.7184 - val_accuracy: 0.8395\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6564 - accuracy: 0.8621 - val_loss: 0.7692 - val_accuracy: 0.8223\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6488 - accuracy: 0.8644 - val_loss: 0.7087 - val_accuracy: 0.8396\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6425 - accuracy: 0.8656 - val_loss: 0.6828 - val_accuracy: 0.8524\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6477 - accuracy: 0.8638 - val_loss: 0.7321 - val_accuracy: 0.8380\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6426 - accuracy: 0.8660 - val_loss: 0.9484 - val_accuracy: 0.7881\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6427 - accuracy: 0.8668 - val_loss: 0.7004 - val_accuracy: 0.8447\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6413 - accuracy: 0.8668 - val_loss: 0.7794 - val_accuracy: 0.8231\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6390 - accuracy: 0.8654 - val_loss: 0.7442 - val_accuracy: 0.8360\n",
      "iteration:  4\n",
      "Test loss: 0.7442208971977234\n",
      "Test accuracy: 0.8360000252723694\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6405 - accuracy: 0.8670 - val_loss: 0.7352 - val_accuracy: 0.8351\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.6418 - accuracy: 0.8664 - val_loss: 0.7193 - val_accuracy: 0.8410\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6383 - accuracy: 0.8673 - val_loss: 0.7289 - val_accuracy: 0.8419\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6345 - accuracy: 0.8692 - val_loss: 0.7187 - val_accuracy: 0.8491\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6352 - accuracy: 0.8682 - val_loss: 0.8047 - val_accuracy: 0.8139\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6269 - accuracy: 0.8710 - val_loss: 0.6846 - val_accuracy: 0.8560\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6312 - accuracy: 0.8693 - val_loss: 0.6786 - val_accuracy: 0.8473\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6268 - accuracy: 0.8712 - val_loss: 0.6891 - val_accuracy: 0.8502\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6239 - accuracy: 0.8702 - val_loss: 0.6542 - val_accuracy: 0.8632\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6243 - accuracy: 0.8721 - val_loss: 0.7517 - val_accuracy: 0.8405\n",
      "iteration:  5\n",
      "Test loss: 0.7517317101478577\n",
      "Test accuracy: 0.840499997138977\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6406 - accuracy: 0.8681 - val_loss: 0.6584 - val_accuracy: 0.8573\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6313 - accuracy: 0.8716 - val_loss: 0.6586 - val_accuracy: 0.8526\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6252 - accuracy: 0.8735 - val_loss: 0.7184 - val_accuracy: 0.8400\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6258 - accuracy: 0.8713 - val_loss: 0.6623 - val_accuracy: 0.8549\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6242 - accuracy: 0.8730 - val_loss: 0.7298 - val_accuracy: 0.8374\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6261 - accuracy: 0.8728 - val_loss: 0.6684 - val_accuracy: 0.8566\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6164 - accuracy: 0.8763 - val_loss: 0.7356 - val_accuracy: 0.8387\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6196 - accuracy: 0.8738 - val_loss: 0.7243 - val_accuracy: 0.8401\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6235 - accuracy: 0.8731 - val_loss: 0.7357 - val_accuracy: 0.8360\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6134 - accuracy: 0.8766 - val_loss: 0.7820 - val_accuracy: 0.8364\n",
      "iteration:  6\n",
      "Test loss: 0.7819658361434937\n",
      "Test accuracy: 0.8363999724388123\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6277 - accuracy: 0.8731 - val_loss: 0.7085 - val_accuracy: 0.8459\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6203 - accuracy: 0.8729 - val_loss: 0.7549 - val_accuracy: 0.8396\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6238 - accuracy: 0.8740 - val_loss: 0.8949 - val_accuracy: 0.8060\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6236 - accuracy: 0.8738 - val_loss: 0.7550 - val_accuracy: 0.8325\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6172 - accuracy: 0.8724 - val_loss: 0.6755 - val_accuracy: 0.8583\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6159 - accuracy: 0.8758 - val_loss: 0.7367 - val_accuracy: 0.8457\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6129 - accuracy: 0.8757 - val_loss: 0.7231 - val_accuracy: 0.8409\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6088 - accuracy: 0.8769 - val_loss: 0.7412 - val_accuracy: 0.8417\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6173 - accuracy: 0.8758 - val_loss: 0.7420 - val_accuracy: 0.8371\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6067 - accuracy: 0.8775 - val_loss: 0.7959 - val_accuracy: 0.8345\n",
      "iteration:  7\n",
      "Test loss: 0.795928914642334\n",
      "Test accuracy: 0.8345000147819519\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6340 - accuracy: 0.8733 - val_loss: 0.7157 - val_accuracy: 0.8492\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6275 - accuracy: 0.8738 - val_loss: 0.6900 - val_accuracy: 0.8530\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6200 - accuracy: 0.8753 - val_loss: 0.7556 - val_accuracy: 0.8347\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6179 - accuracy: 0.8739 - val_loss: 0.6856 - val_accuracy: 0.8538\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6168 - accuracy: 0.8739 - val_loss: 0.7281 - val_accuracy: 0.8460\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6162 - accuracy: 0.8749 - val_loss: 0.7111 - val_accuracy: 0.8391\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6178 - accuracy: 0.8756 - val_loss: 0.6697 - val_accuracy: 0.8502\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6142 - accuracy: 0.8759 - val_loss: 0.6866 - val_accuracy: 0.8527\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6130 - accuracy: 0.8778 - val_loss: 0.7297 - val_accuracy: 0.8403\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6157 - accuracy: 0.8755 - val_loss: 0.7601 - val_accuracy: 0.8384\n",
      "iteration:  8\n",
      "Test loss: 0.7600576050758362\n",
      "Test accuracy: 0.8384000062942505\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6186 - accuracy: 0.8753 - val_loss: 0.7970 - val_accuracy: 0.8298\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6175 - accuracy: 0.8764 - val_loss: 0.7092 - val_accuracy: 0.8491\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6129 - accuracy: 0.8777 - val_loss: 0.7627 - val_accuracy: 0.8376\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6143 - accuracy: 0.8760 - val_loss: 0.7691 - val_accuracy: 0.8359\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6123 - accuracy: 0.8769 - val_loss: 0.7730 - val_accuracy: 0.8348\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6106 - accuracy: 0.8772 - val_loss: 0.7759 - val_accuracy: 0.8339\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6067 - accuracy: 0.8773 - val_loss: 0.6590 - val_accuracy: 0.8574\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6062 - accuracy: 0.8784 - val_loss: 0.7718 - val_accuracy: 0.8282\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6107 - accuracy: 0.8792 - val_loss: 0.6261 - val_accuracy: 0.8648\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6051 - accuracy: 0.8790 - val_loss: 0.6716 - val_accuracy: 0.8548\n",
      "iteration:  9\n",
      "Test loss: 0.6715708622932434\n",
      "Test accuracy: 0.8547999858856201\n",
      "rate=0.5:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (16667, 10)\n",
      "Shape y_true (33333, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 1.2395 - accuracy: 0.6783 - val_loss: 0.8021 - val_accuracy: 0.7889\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8729 - accuracy: 0.7732 - val_loss: 0.8428 - val_accuracy: 0.7825\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8299 - accuracy: 0.7911 - val_loss: 0.7123 - val_accuracy: 0.8234\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7900 - accuracy: 0.8043 - val_loss: 0.8256 - val_accuracy: 0.8022\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7695 - accuracy: 0.8123 - val_loss: 1.0940 - val_accuracy: 0.7433\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7559 - accuracy: 0.8222 - val_loss: 0.7583 - val_accuracy: 0.8209\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7430 - accuracy: 0.8264 - val_loss: 0.7172 - val_accuracy: 0.8302\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7325 - accuracy: 0.8302 - val_loss: 0.9262 - val_accuracy: 0.7844\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7221 - accuracy: 0.8353 - val_loss: 0.7221 - val_accuracy: 0.8326\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7115 - accuracy: 0.8383 - val_loss: 0.7103 - val_accuracy: 0.8372\n",
      "iteration:  0\n",
      "Test loss: 0.7103243831634521\n",
      "Test accuracy: 0.8371999859809875\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7364 - accuracy: 0.8324 - val_loss: 0.7142 - val_accuracy: 0.8344\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7275 - accuracy: 0.8381 - val_loss: 0.8902 - val_accuracy: 0.7888\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7206 - accuracy: 0.8402 - val_loss: 0.7605 - val_accuracy: 0.8170\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7179 - accuracy: 0.8393 - val_loss: 0.7905 - val_accuracy: 0.8163\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7022 - accuracy: 0.8429 - val_loss: 0.8791 - val_accuracy: 0.8035\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7080 - accuracy: 0.8423 - val_loss: 0.7978 - val_accuracy: 0.8192\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6982 - accuracy: 0.8443 - val_loss: 0.6971 - val_accuracy: 0.8454\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6918 - accuracy: 0.8474 - val_loss: 0.7925 - val_accuracy: 0.8176\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6903 - accuracy: 0.8478 - val_loss: 0.7662 - val_accuracy: 0.8185\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6866 - accuracy: 0.8480 - val_loss: 0.7796 - val_accuracy: 0.8143\n",
      "iteration:  1\n",
      "Test loss: 0.7796053954124451\n",
      "Test accuracy: 0.814300000667572\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7241 - accuracy: 0.8393 - val_loss: 0.7564 - val_accuracy: 0.8205\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7148 - accuracy: 0.8418 - val_loss: 0.8593 - val_accuracy: 0.8003\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7109 - accuracy: 0.8409 - val_loss: 0.7319 - val_accuracy: 0.8292\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7045 - accuracy: 0.8421 - val_loss: 0.7351 - val_accuracy: 0.8309\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7008 - accuracy: 0.8446 - val_loss: 0.7548 - val_accuracy: 0.8217\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6934 - accuracy: 0.8470 - val_loss: 0.7480 - val_accuracy: 0.8196\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6884 - accuracy: 0.8494 - val_loss: 0.7108 - val_accuracy: 0.8369\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6901 - accuracy: 0.8493 - val_loss: 0.7817 - val_accuracy: 0.8195\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6907 - accuracy: 0.8493 - val_loss: 0.6938 - val_accuracy: 0.8453\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6852 - accuracy: 0.8500 - val_loss: 0.8481 - val_accuracy: 0.7933\n",
      "iteration:  2\n",
      "Test loss: 0.8480631906509399\n",
      "Test accuracy: 0.7932999730110168\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7315 - accuracy: 0.8394 - val_loss: 0.7775 - val_accuracy: 0.8132\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7236 - accuracy: 0.8394 - val_loss: 0.7388 - val_accuracy: 0.8271\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7180 - accuracy: 0.8426 - val_loss: 0.7367 - val_accuracy: 0.8291\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7163 - accuracy: 0.8404 - val_loss: 0.7808 - val_accuracy: 0.8149\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7073 - accuracy: 0.8445 - val_loss: 0.7648 - val_accuracy: 0.8188\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7044 - accuracy: 0.8446 - val_loss: 0.7923 - val_accuracy: 0.8164\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7031 - accuracy: 0.8437 - val_loss: 0.7493 - val_accuracy: 0.8207\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6976 - accuracy: 0.8455 - val_loss: 0.7424 - val_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7025 - accuracy: 0.8439 - val_loss: 0.8257 - val_accuracy: 0.7975\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6983 - accuracy: 0.8437 - val_loss: 0.8144 - val_accuracy: 0.8075\n",
      "iteration:  3\n",
      "Test loss: 0.8144302243232727\n",
      "Test accuracy: 0.8075000047683716\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7015 - accuracy: 0.8442 - val_loss: 0.7171 - val_accuracy: 0.8289\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6963 - accuracy: 0.8466 - val_loss: 0.7253 - val_accuracy: 0.8317\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6907 - accuracy: 0.8451 - val_loss: 0.7941 - val_accuracy: 0.8150\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6839 - accuracy: 0.8466 - val_loss: 0.8750 - val_accuracy: 0.7990\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6825 - accuracy: 0.8473 - val_loss: 0.7994 - val_accuracy: 0.8144\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6770 - accuracy: 0.8525 - val_loss: 0.8811 - val_accuracy: 0.7937\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6747 - accuracy: 0.8472 - val_loss: 0.8633 - val_accuracy: 0.8007\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6715 - accuracy: 0.8516 - val_loss: 0.7660 - val_accuracy: 0.8161\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6710 - accuracy: 0.8520 - val_loss: 0.7914 - val_accuracy: 0.8165\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6741 - accuracy: 0.8513 - val_loss: 0.7795 - val_accuracy: 0.8224\n",
      "iteration:  4\n",
      "Test loss: 0.779495171546936\n",
      "Test accuracy: 0.8223999738693237\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6809 - accuracy: 0.8511 - val_loss: 0.8749 - val_accuracy: 0.8028\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6696 - accuracy: 0.8534 - val_loss: 0.7110 - val_accuracy: 0.8355\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6657 - accuracy: 0.8542 - val_loss: 0.7707 - val_accuracy: 0.8170\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6629 - accuracy: 0.8553 - val_loss: 0.7651 - val_accuracy: 0.8245\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6569 - accuracy: 0.8567 - val_loss: 0.7428 - val_accuracy: 0.8255\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6576 - accuracy: 0.8567 - val_loss: 0.7781 - val_accuracy: 0.8143\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6595 - accuracy: 0.8572 - val_loss: 0.6902 - val_accuracy: 0.8411\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6503 - accuracy: 0.8576 - val_loss: 0.7749 - val_accuracy: 0.8252\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6449 - accuracy: 0.8595 - val_loss: 0.8059 - val_accuracy: 0.8118\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6522 - accuracy: 0.8588 - val_loss: 0.8531 - val_accuracy: 0.8019\n",
      "iteration:  5\n",
      "Test loss: 0.8531166013717651\n",
      "Test accuracy: 0.8019000291824341\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6918 - accuracy: 0.8461 - val_loss: 0.8010 - val_accuracy: 0.8085\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6853 - accuracy: 0.8484 - val_loss: 0.7018 - val_accuracy: 0.8388\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6794 - accuracy: 0.8503 - val_loss: 0.6736 - val_accuracy: 0.8465\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6727 - accuracy: 0.8531 - val_loss: 0.7234 - val_accuracy: 0.8356\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6728 - accuracy: 0.8509 - val_loss: 0.7268 - val_accuracy: 0.8350\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6743 - accuracy: 0.8527 - val_loss: 0.6677 - val_accuracy: 0.8438\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6723 - accuracy: 0.8514 - val_loss: 0.6975 - val_accuracy: 0.8458\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6649 - accuracy: 0.8540 - val_loss: 0.7521 - val_accuracy: 0.8326\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6653 - accuracy: 0.8532 - val_loss: 0.7551 - val_accuracy: 0.8182\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6677 - accuracy: 0.8531 - val_loss: 0.7082 - val_accuracy: 0.8416\n",
      "iteration:  6\n",
      "Test loss: 0.7082403851509094\n",
      "Test accuracy: 0.8416000008583069\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6516 - accuracy: 0.8577 - val_loss: 0.6688 - val_accuracy: 0.8475\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6479 - accuracy: 0.8614 - val_loss: 0.7134 - val_accuracy: 0.8376\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6417 - accuracy: 0.8611 - val_loss: 0.7698 - val_accuracy: 0.8174\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6350 - accuracy: 0.8633 - val_loss: 0.8474 - val_accuracy: 0.8093\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6339 - accuracy: 0.8633 - val_loss: 0.8631 - val_accuracy: 0.8085\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6336 - accuracy: 0.8660 - val_loss: 0.7070 - val_accuracy: 0.8448\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6267 - accuracy: 0.8645 - val_loss: 0.7588 - val_accuracy: 0.8276\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6333 - accuracy: 0.8640 - val_loss: 0.7637 - val_accuracy: 0.8270\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6298 - accuracy: 0.8664 - val_loss: 0.7508 - val_accuracy: 0.8273\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6240 - accuracy: 0.8659 - val_loss: 0.7440 - val_accuracy: 0.8314\n",
      "iteration:  7\n",
      "Test loss: 0.7439718894004822\n",
      "Test accuracy: 0.8313999772071838\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6399 - accuracy: 0.8620 - val_loss: 0.7278 - val_accuracy: 0.8395\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6420 - accuracy: 0.8592 - val_loss: 0.6972 - val_accuracy: 0.8372\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6301 - accuracy: 0.8662 - val_loss: 0.8990 - val_accuracy: 0.7959\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6348 - accuracy: 0.8646 - val_loss: 0.9044 - val_accuracy: 0.7966\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6268 - accuracy: 0.8642 - val_loss: 0.7131 - val_accuracy: 0.8377\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6323 - accuracy: 0.8635 - val_loss: 0.7221 - val_accuracy: 0.8356\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6265 - accuracy: 0.8648 - val_loss: 0.8061 - val_accuracy: 0.8185\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6308 - accuracy: 0.8649 - val_loss: 0.7112 - val_accuracy: 0.8363\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6206 - accuracy: 0.8675 - val_loss: 0.8754 - val_accuracy: 0.8008\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6236 - accuracy: 0.8669 - val_loss: 0.7083 - val_accuracy: 0.8394\n",
      "iteration:  8\n",
      "Test loss: 0.7083046613693237\n",
      "Test accuracy: 0.8393999934196472\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6415 - accuracy: 0.8619 - val_loss: 0.6893 - val_accuracy: 0.8398\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6388 - accuracy: 0.8632 - val_loss: 0.6955 - val_accuracy: 0.8424\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6295 - accuracy: 0.8673 - val_loss: 0.7503 - val_accuracy: 0.8284\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6318 - accuracy: 0.8656 - val_loss: 0.8068 - val_accuracy: 0.8162\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6223 - accuracy: 0.8673 - val_loss: 0.7540 - val_accuracy: 0.8216\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6250 - accuracy: 0.8686 - val_loss: 0.7386 - val_accuracy: 0.8288\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6240 - accuracy: 0.8669 - val_loss: 0.7238 - val_accuracy: 0.8356\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6193 - accuracy: 0.8697 - val_loss: 0.8460 - val_accuracy: 0.8041\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6163 - accuracy: 0.8685 - val_loss: 0.6659 - val_accuracy: 0.8529\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6237 - accuracy: 0.8692 - val_loss: 0.6744 - val_accuracy: 0.8482\n",
      "iteration:  9\n",
      "Test loss: 0.674368602180481\n",
      "Test accuracy: 0.8482000231742859\n",
      "rate=0.75:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (21429, 10)\n",
      "Shape y_true (28571, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.1790 - accuracy: 0.6923 - val_loss: 0.8707 - val_accuracy: 0.7709\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.8727 - accuracy: 0.7722 - val_loss: 0.8442 - val_accuracy: 0.7834\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.8197 - accuracy: 0.7936 - val_loss: 0.7665 - val_accuracy: 0.8123\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7915 - accuracy: 0.8044 - val_loss: 0.8437 - val_accuracy: 0.7899\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7744 - accuracy: 0.8125 - val_loss: 0.7324 - val_accuracy: 0.8208\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7520 - accuracy: 0.8215 - val_loss: 0.7530 - val_accuracy: 0.8192\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7500 - accuracy: 0.8254 - val_loss: 0.7022 - val_accuracy: 0.8375\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7305 - accuracy: 0.8313 - val_loss: 0.7299 - val_accuracy: 0.8323\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7163 - accuracy: 0.8376 - val_loss: 0.8920 - val_accuracy: 0.7990\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7112 - accuracy: 0.8387 - val_loss: 0.7400 - val_accuracy: 0.8254\n",
      "iteration:  0\n",
      "Test loss: 0.7400041086196899\n",
      "Test accuracy: 0.8253999948501587\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.7586 - accuracy: 0.8276 - val_loss: 0.7378 - val_accuracy: 0.8315\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7408 - accuracy: 0.8317 - val_loss: 0.8615 - val_accuracy: 0.7966\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7321 - accuracy: 0.8344 - val_loss: 0.8067 - val_accuracy: 0.8124\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7304 - accuracy: 0.8333 - val_loss: 0.8364 - val_accuracy: 0.8058\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7231 - accuracy: 0.8371 - val_loss: 0.7938 - val_accuracy: 0.8211\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7147 - accuracy: 0.8396 - val_loss: 0.8500 - val_accuracy: 0.7991\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7185 - accuracy: 0.8391 - val_loss: 0.8087 - val_accuracy: 0.8144\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7063 - accuracy: 0.8403 - val_loss: 0.8486 - val_accuracy: 0.7936\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7092 - accuracy: 0.8410 - val_loss: 0.7329 - val_accuracy: 0.8317\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7044 - accuracy: 0.8421 - val_loss: 0.8621 - val_accuracy: 0.7950\n",
      "iteration:  1\n",
      "Test loss: 0.8621417244911194\n",
      "Test accuracy: 0.7950000166893005\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7467 - accuracy: 0.8301 - val_loss: 0.7527 - val_accuracy: 0.8258\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7389 - accuracy: 0.8314 - val_loss: 0.8091 - val_accuracy: 0.8121\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7286 - accuracy: 0.8332 - val_loss: 0.7860 - val_accuracy: 0.8073\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7208 - accuracy: 0.8347 - val_loss: 0.7625 - val_accuracy: 0.8168\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7256 - accuracy: 0.8352 - val_loss: 0.7875 - val_accuracy: 0.8133\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7185 - accuracy: 0.8346 - val_loss: 0.8019 - val_accuracy: 0.8091\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7116 - accuracy: 0.8366 - val_loss: 0.8933 - val_accuracy: 0.7932\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7092 - accuracy: 0.8382 - val_loss: 0.7648 - val_accuracy: 0.8218\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7055 - accuracy: 0.8401 - val_loss: 0.7842 - val_accuracy: 0.8034\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7048 - accuracy: 0.8390 - val_loss: 0.8008 - val_accuracy: 0.8112\n",
      "iteration:  2\n",
      "Test loss: 0.8008085074424743\n",
      "Test accuracy: 0.8112000226974487\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6999 - accuracy: 0.8411 - val_loss: 0.8689 - val_accuracy: 0.8019\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6923 - accuracy: 0.8424 - val_loss: 0.8156 - val_accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6877 - accuracy: 0.8448 - val_loss: 1.1431 - val_accuracy: 0.7438\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6834 - accuracy: 0.8436 - val_loss: 0.7895 - val_accuracy: 0.8150\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6818 - accuracy: 0.8441 - val_loss: 0.8235 - val_accuracy: 0.8036\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6814 - accuracy: 0.8456 - val_loss: 0.7371 - val_accuracy: 0.8288\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6767 - accuracy: 0.8484 - val_loss: 0.7873 - val_accuracy: 0.8187\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6757 - accuracy: 0.8480 - val_loss: 0.7849 - val_accuracy: 0.8234\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6716 - accuracy: 0.8503 - val_loss: 0.7196 - val_accuracy: 0.8304\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6742 - accuracy: 0.8483 - val_loss: 0.7455 - val_accuracy: 0.8225\n",
      "iteration:  3\n",
      "Test loss: 0.7455169569969178\n",
      "Test accuracy: 0.8224999904632568\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6781 - accuracy: 0.8494 - val_loss: 0.7966 - val_accuracy: 0.8072\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6713 - accuracy: 0.8486 - val_loss: 0.7918 - val_accuracy: 0.8157\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6693 - accuracy: 0.8506 - val_loss: 0.8025 - val_accuracy: 0.8100\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6629 - accuracy: 0.8514 - val_loss: 0.9738 - val_accuracy: 0.7786\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6624 - accuracy: 0.8523 - val_loss: 0.8277 - val_accuracy: 0.7925\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6577 - accuracy: 0.8536 - val_loss: 0.7162 - val_accuracy: 0.8341\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6605 - accuracy: 0.8534 - val_loss: 0.7483 - val_accuracy: 0.8225\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6524 - accuracy: 0.8566 - val_loss: 0.7112 - val_accuracy: 0.8330\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6527 - accuracy: 0.8544 - val_loss: 0.6692 - val_accuracy: 0.8454\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6536 - accuracy: 0.8547 - val_loss: 0.8395 - val_accuracy: 0.8062\n",
      "iteration:  4\n",
      "Test loss: 0.8394638039588929\n",
      "Test accuracy: 0.8062000274658203\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7116 - accuracy: 0.8408 - val_loss: 0.7700 - val_accuracy: 0.8216\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7057 - accuracy: 0.8407 - val_loss: 0.7981 - val_accuracy: 0.8138\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6952 - accuracy: 0.8452 - val_loss: 0.7851 - val_accuracy: 0.8141\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6968 - accuracy: 0.8429 - val_loss: 0.8090 - val_accuracy: 0.8047\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6881 - accuracy: 0.8447 - val_loss: 0.7697 - val_accuracy: 0.8091\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6885 - accuracy: 0.8452 - val_loss: 0.7133 - val_accuracy: 0.8330\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6829 - accuracy: 0.8467 - val_loss: 0.6809 - val_accuracy: 0.8421\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6874 - accuracy: 0.8469 - val_loss: 1.0102 - val_accuracy: 0.7609\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6785 - accuracy: 0.8476 - val_loss: 0.8268 - val_accuracy: 0.8090\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6819 - accuracy: 0.8477 - val_loss: 0.7767 - val_accuracy: 0.8204\n",
      "iteration:  5\n",
      "Test loss: 0.7766973007202148\n",
      "Test accuracy: 0.8203999996185303\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6854 - accuracy: 0.8450 - val_loss: 0.7916 - val_accuracy: 0.8193\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6710 - accuracy: 0.8494 - val_loss: 0.7892 - val_accuracy: 0.8159\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6658 - accuracy: 0.8512 - val_loss: 0.7720 - val_accuracy: 0.8149\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6649 - accuracy: 0.8513 - val_loss: 0.7992 - val_accuracy: 0.8143\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6661 - accuracy: 0.8495 - val_loss: 0.7759 - val_accuracy: 0.8174\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 76s 49ms/step - loss: 0.6606 - accuracy: 0.8532 - val_loss: 0.8180 - val_accuracy: 0.8076\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6527 - accuracy: 0.8554 - val_loss: 0.8957 - val_accuracy: 0.7945\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6583 - accuracy: 0.8531 - val_loss: 0.7482 - val_accuracy: 0.8217\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6535 - accuracy: 0.8546 - val_loss: 0.7360 - val_accuracy: 0.8297\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6560 - accuracy: 0.8545 - val_loss: 0.8508 - val_accuracy: 0.8019\n",
      "iteration:  6\n",
      "Test loss: 0.8508029324531555\n",
      "Test accuracy: 0.8019000291824341\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7006 - accuracy: 0.8423 - val_loss: 0.8154 - val_accuracy: 0.8113\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6864 - accuracy: 0.8429 - val_loss: 0.7316 - val_accuracy: 0.8292\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6811 - accuracy: 0.8464 - val_loss: 0.7328 - val_accuracy: 0.8230\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6731 - accuracy: 0.8468 - val_loss: 0.7661 - val_accuracy: 0.8131\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6664 - accuracy: 0.8469 - val_loss: 0.7770 - val_accuracy: 0.8216\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6688 - accuracy: 0.8467 - val_loss: 0.7910 - val_accuracy: 0.8131\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6677 - accuracy: 0.8481 - val_loss: 0.7802 - val_accuracy: 0.8197\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6633 - accuracy: 0.8485 - val_loss: 0.8144 - val_accuracy: 0.8088\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6628 - accuracy: 0.8498 - val_loss: 0.8148 - val_accuracy: 0.8102\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6598 - accuracy: 0.8497 - val_loss: 0.7172 - val_accuracy: 0.8274\n",
      "iteration:  7\n",
      "Test loss: 0.7171952509880066\n",
      "Test accuracy: 0.8274000287055969\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6584 - accuracy: 0.8515 - val_loss: 0.7196 - val_accuracy: 0.8338\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6551 - accuracy: 0.8519 - val_loss: 0.8176 - val_accuracy: 0.8031\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6439 - accuracy: 0.8553 - val_loss: 0.7520 - val_accuracy: 0.8294\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6423 - accuracy: 0.8548 - val_loss: 0.7494 - val_accuracy: 0.8284\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6416 - accuracy: 0.8575 - val_loss: 0.6753 - val_accuracy: 0.8410\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6385 - accuracy: 0.8557 - val_loss: 0.8600 - val_accuracy: 0.7956\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6373 - accuracy: 0.8577 - val_loss: 0.7913 - val_accuracy: 0.8233\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6374 - accuracy: 0.8567 - val_loss: 0.7373 - val_accuracy: 0.8250\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6313 - accuracy: 0.8589 - val_loss: 0.6702 - val_accuracy: 0.8462\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6324 - accuracy: 0.8578 - val_loss: 0.8867 - val_accuracy: 0.7906\n",
      "iteration:  8\n",
      "Test loss: 0.8866506906509399\n",
      "Test accuracy: 0.7906000018119812\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7142 - accuracy: 0.8377 - val_loss: 0.8573 - val_accuracy: 0.7974\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6988 - accuracy: 0.8397 - val_loss: 0.7711 - val_accuracy: 0.8150\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6976 - accuracy: 0.8421 - val_loss: 0.8434 - val_accuracy: 0.8010\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6890 - accuracy: 0.8425 - val_loss: 0.7638 - val_accuracy: 0.8215\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6890 - accuracy: 0.8433 - val_loss: 0.7339 - val_accuracy: 0.8255\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6825 - accuracy: 0.8442 - val_loss: 0.6743 - val_accuracy: 0.8439\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6820 - accuracy: 0.8452 - val_loss: 0.9308 - val_accuracy: 0.7832\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6787 - accuracy: 0.8452 - val_loss: 0.7321 - val_accuracy: 0.8255\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6819 - accuracy: 0.8449 - val_loss: 0.8141 - val_accuracy: 0.8116\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6728 - accuracy: 0.8470 - val_loss: 0.8751 - val_accuracy: 0.7930\n",
      "iteration:  9\n",
      "Test loss: 0.8751029270172119\n",
      "Test accuracy: 0.7929999828338623\n",
      "rate=1.0:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (25000, 10)\n",
      "Shape y_true (25000, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.1967 - accuracy: 0.6917 - val_loss: 1.0472 - val_accuracy: 0.7364\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.8721 - accuracy: 0.7714 - val_loss: 0.7843 - val_accuracy: 0.8006\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8284 - accuracy: 0.7914 - val_loss: 0.9101 - val_accuracy: 0.7649\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7938 - accuracy: 0.8057 - val_loss: 0.7511 - val_accuracy: 0.8151\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7748 - accuracy: 0.8112 - val_loss: 0.7563 - val_accuracy: 0.8198\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7575 - accuracy: 0.8194 - val_loss: 0.9148 - val_accuracy: 0.7840\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7451 - accuracy: 0.8261 - val_loss: 0.7830 - val_accuracy: 0.8183\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7313 - accuracy: 0.8320 - val_loss: 0.7075 - val_accuracy: 0.8369\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7244 - accuracy: 0.8335 - val_loss: 0.8231 - val_accuracy: 0.8034\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7161 - accuracy: 0.8375 - val_loss: 0.7617 - val_accuracy: 0.8278\n",
      "iteration:  0\n",
      "Test loss: 0.7616844475746155\n",
      "Test accuracy: 0.8277999758720398\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7485 - accuracy: 0.8268 - val_loss: 0.7549 - val_accuracy: 0.8176\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7454 - accuracy: 0.8297 - val_loss: 0.7969 - val_accuracy: 0.8100\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7356 - accuracy: 0.8312 - val_loss: 0.8801 - val_accuracy: 0.7922\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7291 - accuracy: 0.8325 - val_loss: 0.9117 - val_accuracy: 0.7803\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7234 - accuracy: 0.8350 - val_loss: 0.8748 - val_accuracy: 0.7928\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.7156 - accuracy: 0.8377 - val_loss: 0.7617 - val_accuracy: 0.8227\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7171 - accuracy: 0.8358 - val_loss: 0.7168 - val_accuracy: 0.8365\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7057 - accuracy: 0.8391 - val_loss: 0.8307 - val_accuracy: 0.7967\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7019 - accuracy: 0.8416 - val_loss: 0.8102 - val_accuracy: 0.8077\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7022 - accuracy: 0.8422 - val_loss: 0.8969 - val_accuracy: 0.7869\n",
      "iteration:  1\n",
      "Test loss: 0.8969495767593384\n",
      "Test accuracy: 0.786899983882904\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7525 - accuracy: 0.8255 - val_loss: 0.8757 - val_accuracy: 0.7808\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7444 - accuracy: 0.8260 - val_loss: 0.8656 - val_accuracy: 0.7836\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7352 - accuracy: 0.8273 - val_loss: 0.8507 - val_accuracy: 0.7852\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7268 - accuracy: 0.8295 - val_loss: 0.8273 - val_accuracy: 0.8005\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7269 - accuracy: 0.8293 - val_loss: 0.8816 - val_accuracy: 0.7741\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7195 - accuracy: 0.8319 - val_loss: 0.7853 - val_accuracy: 0.8033\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7151 - accuracy: 0.8317 - val_loss: 0.7385 - val_accuracy: 0.8191\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7210 - accuracy: 0.8302 - val_loss: 0.8098 - val_accuracy: 0.8057\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7140 - accuracy: 0.8325 - val_loss: 0.8342 - val_accuracy: 0.8001\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7045 - accuracy: 0.8335 - val_loss: 0.8893 - val_accuracy: 0.7813\n",
      "iteration:  2\n",
      "Test loss: 0.8893157433509826\n",
      "Test accuracy: 0.7813000082969666\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7512 - accuracy: 0.8212 - val_loss: 0.8017 - val_accuracy: 0.8079\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7421 - accuracy: 0.8229 - val_loss: 0.8536 - val_accuracy: 0.7910\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7341 - accuracy: 0.8238 - val_loss: 0.9005 - val_accuracy: 0.7800\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7356 - accuracy: 0.8243 - val_loss: 0.7907 - val_accuracy: 0.8172\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7221 - accuracy: 0.8267 - val_loss: 0.8922 - val_accuracy: 0.7870\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7208 - accuracy: 0.8280 - val_loss: 0.8370 - val_accuracy: 0.8015\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7171 - accuracy: 0.8269 - val_loss: 0.7900 - val_accuracy: 0.8143\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7177 - accuracy: 0.8269 - val_loss: 0.7812 - val_accuracy: 0.8166\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7166 - accuracy: 0.8290 - val_loss: 0.7976 - val_accuracy: 0.8116\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7112 - accuracy: 0.8296 - val_loss: 0.9156 - val_accuracy: 0.7800\n",
      "iteration:  3\n",
      "Test loss: 0.9156352716445922\n",
      "Test accuracy: 0.7799999713897705\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7501 - accuracy: 0.8220 - val_loss: 0.9658 - val_accuracy: 0.7712\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7385 - accuracy: 0.8237 - val_loss: 0.7784 - val_accuracy: 0.8127\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7383 - accuracy: 0.8269 - val_loss: 0.7848 - val_accuracy: 0.8137\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7293 - accuracy: 0.8265 - val_loss: 0.8315 - val_accuracy: 0.7997\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7301 - accuracy: 0.8286 - val_loss: 0.7603 - val_accuracy: 0.8184\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7255 - accuracy: 0.8288 - val_loss: 0.7733 - val_accuracy: 0.8086\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.7187 - accuracy: 0.8304 - val_loss: 0.8892 - val_accuracy: 0.8039\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7220 - accuracy: 0.8315 - val_loss: 0.7968 - val_accuracy: 0.8022\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7175 - accuracy: 0.8305 - val_loss: 0.7539 - val_accuracy: 0.8262\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7265 - accuracy: 0.8288 - val_loss: 0.8057 - val_accuracy: 0.8109\n",
      "iteration:  4\n",
      "Test loss: 0.8057182300567627\n",
      "Test accuracy: 0.8108999729156494\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6993 - accuracy: 0.8393 - val_loss: 0.8145 - val_accuracy: 0.8080\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6907 - accuracy: 0.8399 - val_loss: 0.8916 - val_accuracy: 0.7837\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6831 - accuracy: 0.8427 - val_loss: 0.8182 - val_accuracy: 0.8044\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6829 - accuracy: 0.8437 - val_loss: 0.8573 - val_accuracy: 0.8020\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6844 - accuracy: 0.8435 - val_loss: 0.8592 - val_accuracy: 0.7996\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6738 - accuracy: 0.8450 - val_loss: 0.7760 - val_accuracy: 0.8165\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6785 - accuracy: 0.8449 - val_loss: 0.7832 - val_accuracy: 0.8159\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6742 - accuracy: 0.8439 - val_loss: 0.7810 - val_accuracy: 0.8196\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6727 - accuracy: 0.8463 - val_loss: 0.7065 - val_accuracy: 0.8344\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6717 - accuracy: 0.8481 - val_loss: 0.7589 - val_accuracy: 0.8298\n",
      "iteration:  5\n",
      "Test loss: 0.7588932089805603\n",
      "Test accuracy: 0.829800009727478\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6638 - accuracy: 0.8491 - val_loss: 0.8151 - val_accuracy: 0.8040\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6540 - accuracy: 0.8509 - val_loss: 0.7768 - val_accuracy: 0.8187\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6569 - accuracy: 0.8508 - val_loss: 0.7835 - val_accuracy: 0.8181\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6454 - accuracy: 0.8538 - val_loss: 0.7059 - val_accuracy: 0.8382\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6517 - accuracy: 0.8514 - val_loss: 0.7579 - val_accuracy: 0.8269\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6428 - accuracy: 0.8548 - val_loss: 0.9944 - val_accuracy: 0.7778\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6429 - accuracy: 0.8530 - val_loss: 0.7940 - val_accuracy: 0.8170\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6403 - accuracy: 0.8562 - val_loss: 0.7086 - val_accuracy: 0.8373\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6461 - accuracy: 0.8541 - val_loss: 0.6929 - val_accuracy: 0.8364\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6364 - accuracy: 0.8577 - val_loss: 0.7877 - val_accuracy: 0.8202\n",
      "iteration:  6\n",
      "Test loss: 0.7877343307495117\n",
      "Test accuracy: 0.8202000260353088\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6747 - accuracy: 0.8500 - val_loss: 0.7012 - val_accuracy: 0.8368\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6655 - accuracy: 0.8515 - val_loss: 0.7233 - val_accuracy: 0.8325\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6630 - accuracy: 0.8527 - val_loss: 0.7784 - val_accuracy: 0.8210\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6610 - accuracy: 0.8515 - val_loss: 0.8036 - val_accuracy: 0.8048\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6545 - accuracy: 0.8531 - val_loss: 0.8308 - val_accuracy: 0.7993\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6484 - accuracy: 0.8526 - val_loss: 0.9105 - val_accuracy: 0.7813\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6477 - accuracy: 0.8565 - val_loss: 0.8788 - val_accuracy: 0.7939\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6508 - accuracy: 0.8536 - val_loss: 0.7370 - val_accuracy: 0.8283\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6474 - accuracy: 0.8554 - val_loss: 0.8595 - val_accuracy: 0.7939\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6473 - accuracy: 0.8546 - val_loss: 1.0496 - val_accuracy: 0.7486\n",
      "iteration:  7\n",
      "Test loss: 1.0495965475082398\n",
      "Test accuracy: 0.7486000061035156\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7503 - accuracy: 0.8217 - val_loss: 1.2100 - val_accuracy: 0.6929\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7371 - accuracy: 0.8228 - val_loss: 0.9421 - val_accuracy: 0.7651\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7252 - accuracy: 0.8266 - val_loss: 0.8605 - val_accuracy: 0.7880\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7271 - accuracy: 0.8254 - val_loss: 0.8072 - val_accuracy: 0.8013\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7227 - accuracy: 0.8285 - val_loss: 0.9349 - val_accuracy: 0.7633\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7221 - accuracy: 0.8267 - val_loss: 0.8127 - val_accuracy: 0.8005\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7184 - accuracy: 0.8287 - val_loss: 0.9545 - val_accuracy: 0.7676\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7184 - accuracy: 0.8282 - val_loss: 0.7883 - val_accuracy: 0.8095\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7143 - accuracy: 0.8283 - val_loss: 0.8808 - val_accuracy: 0.7867\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7133 - accuracy: 0.8296 - val_loss: 0.9247 - val_accuracy: 0.7741\n",
      "iteration:  8\n",
      "Test loss: 0.9247372313499451\n",
      "Test accuracy: 0.7741000056266785\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7114 - accuracy: 0.8294 - val_loss: 0.8223 - val_accuracy: 0.8035\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7007 - accuracy: 0.8313 - val_loss: 0.8434 - val_accuracy: 0.8036\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6943 - accuracy: 0.8333 - val_loss: 0.8059 - val_accuracy: 0.8044\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6908 - accuracy: 0.8363 - val_loss: 0.8863 - val_accuracy: 0.7823\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6921 - accuracy: 0.8327 - val_loss: 1.0563 - val_accuracy: 0.7455\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6862 - accuracy: 0.8364 - val_loss: 0.8137 - val_accuracy: 0.8147\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6872 - accuracy: 0.8349 - val_loss: 0.8223 - val_accuracy: 0.7970\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6818 - accuracy: 0.8358 - val_loss: 0.7188 - val_accuracy: 0.8306\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6788 - accuracy: 0.8383 - val_loss: 0.8391 - val_accuracy: 0.7931\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6841 - accuracy: 0.8353 - val_loss: 0.9951 - val_accuracy: 0.7627\n",
      "iteration:  9\n",
      "Test loss: 0.995133074760437\n",
      "Test accuracy: 0.7627000212669373\n",
      "rate=2.5:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (35715, 10)\n",
      "Shape y_true (14285, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 1.1897 - accuracy: 0.6897 - val_loss: 0.8906 - val_accuracy: 0.7579\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8810 - accuracy: 0.7721 - val_loss: 0.8751 - val_accuracy: 0.7650\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8278 - accuracy: 0.7927 - val_loss: 0.8225 - val_accuracy: 0.7977\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.8067 - accuracy: 0.8036 - val_loss: 0.7713 - val_accuracy: 0.8105\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7767 - accuracy: 0.8114 - val_loss: 0.7878 - val_accuracy: 0.8064\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7626 - accuracy: 0.8183 - val_loss: 0.7780 - val_accuracy: 0.8169\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7451 - accuracy: 0.8262 - val_loss: 0.8915 - val_accuracy: 0.7898\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7365 - accuracy: 0.8294 - val_loss: 0.8037 - val_accuracy: 0.8128\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7183 - accuracy: 0.8363 - val_loss: 0.7850 - val_accuracy: 0.8097\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7184 - accuracy: 0.8355 - val_loss: 0.8078 - val_accuracy: 0.8104\n",
      "iteration:  0\n",
      "Test loss: 0.8078387618064881\n",
      "Test accuracy: 0.8104000091552734\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7945 - accuracy: 0.8156 - val_loss: 1.0144 - val_accuracy: 0.7624\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7776 - accuracy: 0.8189 - val_loss: 0.8727 - val_accuracy: 0.7814\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7742 - accuracy: 0.8217 - val_loss: 0.8677 - val_accuracy: 0.7978\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7625 - accuracy: 0.8237 - val_loss: 0.9504 - val_accuracy: 0.7756\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7556 - accuracy: 0.8270 - val_loss: 0.9950 - val_accuracy: 0.7671\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7506 - accuracy: 0.8264 - val_loss: 0.8362 - val_accuracy: 0.8006\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7414 - accuracy: 0.8299 - val_loss: 0.9593 - val_accuracy: 0.7642\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7392 - accuracy: 0.8293 - val_loss: 0.8183 - val_accuracy: 0.8067\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7369 - accuracy: 0.8319 - val_loss: 0.8924 - val_accuracy: 0.7955\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7291 - accuracy: 0.8314 - val_loss: 0.8386 - val_accuracy: 0.7939\n",
      "iteration:  1\n",
      "Test loss: 0.8385565215110778\n",
      "Test accuracy: 0.7939000129699707\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7507 - accuracy: 0.8231 - val_loss: 1.0329 - val_accuracy: 0.7519\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7331 - accuracy: 0.8279 - val_loss: 0.9160 - val_accuracy: 0.7789\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7299 - accuracy: 0.8277 - val_loss: 0.8050 - val_accuracy: 0.8021\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7279 - accuracy: 0.8276 - val_loss: 0.8768 - val_accuracy: 0.7833\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7187 - accuracy: 0.8321 - val_loss: 0.8440 - val_accuracy: 0.7956\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7158 - accuracy: 0.8309 - val_loss: 0.9270 - val_accuracy: 0.7809\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7125 - accuracy: 0.8333 - val_loss: 0.8982 - val_accuracy: 0.7788\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7099 - accuracy: 0.8319 - val_loss: 1.0669 - val_accuracy: 0.7424\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7076 - accuracy: 0.8335 - val_loss: 0.9593 - val_accuracy: 0.7670\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7022 - accuracy: 0.8344 - val_loss: 1.0382 - val_accuracy: 0.7571\n",
      "iteration:  2\n",
      "Test loss: 1.0381916991233826\n",
      "Test accuracy: 0.757099986076355\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7594 - accuracy: 0.8174 - val_loss: 1.0191 - val_accuracy: 0.7465\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7498 - accuracy: 0.8175 - val_loss: 1.1532 - val_accuracy: 0.7199\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7436 - accuracy: 0.8186 - val_loss: 1.0901 - val_accuracy: 0.7346\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7331 - accuracy: 0.8220 - val_loss: 0.8635 - val_accuracy: 0.7876\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7297 - accuracy: 0.8220 - val_loss: 1.0194 - val_accuracy: 0.7538\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7270 - accuracy: 0.8249 - val_loss: 1.2079 - val_accuracy: 0.7163\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7231 - accuracy: 0.8238 - val_loss: 1.0463 - val_accuracy: 0.7442\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7238 - accuracy: 0.8250 - val_loss: 1.2833 - val_accuracy: 0.6903\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7134 - accuracy: 0.8275 - val_loss: 0.9573 - val_accuracy: 0.7643\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7142 - accuracy: 0.8259 - val_loss: 1.0113 - val_accuracy: 0.7448\n",
      "iteration:  3\n",
      "Test loss: 1.0113059127807618\n",
      "Test accuracy: 0.7447999715805054\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7229 - accuracy: 0.8198 - val_loss: 1.0739 - val_accuracy: 0.7156\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7201 - accuracy: 0.8217 - val_loss: 0.9679 - val_accuracy: 0.7460\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7064 - accuracy: 0.8244 - val_loss: 1.0859 - val_accuracy: 0.7166\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7053 - accuracy: 0.8265 - val_loss: 1.0983 - val_accuracy: 0.7199\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7033 - accuracy: 0.8271 - val_loss: 1.0902 - val_accuracy: 0.6982\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6989 - accuracy: 0.8268 - val_loss: 1.0409 - val_accuracy: 0.7318\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7041 - accuracy: 0.8266 - val_loss: 1.0444 - val_accuracy: 0.7358\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7001 - accuracy: 0.8257 - val_loss: 0.9486 - val_accuracy: 0.7478\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6932 - accuracy: 0.8273 - val_loss: 0.9147 - val_accuracy: 0.7590\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6933 - accuracy: 0.8295 - val_loss: 1.0391 - val_accuracy: 0.7201\n",
      "iteration:  4\n",
      "Test loss: 1.0390801114082338\n",
      "Test accuracy: 0.7200999855995178\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7185 - accuracy: 0.8187 - val_loss: 1.0045 - val_accuracy: 0.7249\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7116 - accuracy: 0.8195 - val_loss: 1.0912 - val_accuracy: 0.7051\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 46ms/step - loss: 0.7071 - accuracy: 0.8202 - val_loss: 1.0837 - val_accuracy: 0.7080\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7048 - accuracy: 0.8216 - val_loss: 0.9872 - val_accuracy: 0.7211\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6997 - accuracy: 0.8209 - val_loss: 1.1759 - val_accuracy: 0.6876\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6969 - accuracy: 0.8235 - val_loss: 0.9596 - val_accuracy: 0.7351\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6951 - accuracy: 0.8234 - val_loss: 1.0729 - val_accuracy: 0.7018\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6898 - accuracy: 0.8265 - val_loss: 1.0507 - val_accuracy: 0.7092\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6910 - accuracy: 0.8255 - val_loss: 1.0856 - val_accuracy: 0.7070\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6893 - accuracy: 0.8261 - val_loss: 1.1312 - val_accuracy: 0.6897\n",
      "iteration:  5\n",
      "Test loss: 1.1312170610427856\n",
      "Test accuracy: 0.6897000074386597\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7658 - accuracy: 0.8003 - val_loss: 1.0914 - val_accuracy: 0.6791\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7532 - accuracy: 0.8038 - val_loss: 1.3177 - val_accuracy: 0.6421\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7498 - accuracy: 0.8044 - val_loss: 1.1134 - val_accuracy: 0.7038\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7408 - accuracy: 0.8045 - val_loss: 0.9867 - val_accuracy: 0.7174\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7351 - accuracy: 0.8064 - val_loss: 0.9908 - val_accuracy: 0.7248\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7335 - accuracy: 0.8066 - val_loss: 1.3026 - val_accuracy: 0.6552\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7271 - accuracy: 0.8102 - val_loss: 1.1278 - val_accuracy: 0.6776\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7351 - accuracy: 0.8072 - val_loss: 1.1929 - val_accuracy: 0.6748\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7270 - accuracy: 0.8095 - val_loss: 1.1953 - val_accuracy: 0.6585\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7247 - accuracy: 0.8083 - val_loss: 1.2082 - val_accuracy: 0.6799\n",
      "iteration:  6\n",
      "Test loss: 1.2081805555343628\n",
      "Test accuracy: 0.6798999905586243\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7207 - accuracy: 0.8087 - val_loss: 1.1475 - val_accuracy: 0.6629\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7172 - accuracy: 0.8093 - val_loss: 1.2082 - val_accuracy: 0.6517\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7138 - accuracy: 0.8100 - val_loss: 1.1554 - val_accuracy: 0.6567\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7135 - accuracy: 0.8091 - val_loss: 1.1224 - val_accuracy: 0.6756\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7066 - accuracy: 0.8106 - val_loss: 1.1615 - val_accuracy: 0.6740\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6954 - accuracy: 0.8141 - val_loss: 1.1590 - val_accuracy: 0.6879\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6990 - accuracy: 0.8137 - val_loss: 1.0840 - val_accuracy: 0.6841\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6980 - accuracy: 0.8128 - val_loss: 1.0565 - val_accuracy: 0.6835\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6993 - accuracy: 0.8133 - val_loss: 1.2274 - val_accuracy: 0.6493\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6954 - accuracy: 0.8146 - val_loss: 1.0183 - val_accuracy: 0.7049\n",
      "iteration:  7\n",
      "Test loss: 1.0183441440582275\n",
      "Test accuracy: 0.7049000263214111\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7023 - accuracy: 0.8150 - val_loss: 1.2326 - val_accuracy: 0.6856\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7012 - accuracy: 0.8149 - val_loss: 1.0543 - val_accuracy: 0.6964\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6852 - accuracy: 0.8179 - val_loss: 1.1136 - val_accuracy: 0.6872\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6927 - accuracy: 0.8185 - val_loss: 1.1840 - val_accuracy: 0.6716\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6808 - accuracy: 0.8200 - val_loss: 1.2859 - val_accuracy: 0.6625\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6909 - accuracy: 0.8185 - val_loss: 1.0684 - val_accuracy: 0.6985\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6839 - accuracy: 0.8196 - val_loss: 1.0540 - val_accuracy: 0.7106\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6811 - accuracy: 0.8211 - val_loss: 1.1250 - val_accuracy: 0.6817\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6793 - accuracy: 0.8221 - val_loss: 1.0209 - val_accuracy: 0.7120\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6813 - accuracy: 0.8215 - val_loss: 1.1254 - val_accuracy: 0.6826\n",
      "iteration:  8\n",
      "Test loss: 1.1253952653884887\n",
      "Test accuracy: 0.6826000213623047\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7075 - accuracy: 0.8130 - val_loss: 1.0331 - val_accuracy: 0.6989\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6989 - accuracy: 0.8145 - val_loss: 1.1358 - val_accuracy: 0.6817\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6998 - accuracy: 0.8157 - val_loss: 1.1897 - val_accuracy: 0.6582\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6961 - accuracy: 0.8160 - val_loss: 1.1893 - val_accuracy: 0.6601\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6928 - accuracy: 0.8168 - val_loss: 1.0460 - val_accuracy: 0.7072\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6924 - accuracy: 0.8194 - val_loss: 1.2179 - val_accuracy: 0.6504\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6902 - accuracy: 0.8165 - val_loss: 1.0391 - val_accuracy: 0.6987\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6907 - accuracy: 0.8199 - val_loss: 1.1143 - val_accuracy: 0.6850\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6832 - accuracy: 0.8191 - val_loss: 1.2158 - val_accuracy: 0.6767\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6843 - accuracy: 0.8206 - val_loss: 1.1736 - val_accuracy: 0.6729\n",
      "iteration:  9\n",
      "Test loss: 1.1736196920394897\n",
      "Test accuracy: 0.6729000210762024\n",
      "rate=5.0:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (41667, 10)\n",
      "Shape y_true (8333, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 1.2301 - accuracy: 0.6845 - val_loss: 0.8589 - val_accuracy: 0.7750\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8861 - accuracy: 0.7720 - val_loss: 0.7328 - val_accuracy: 0.8203\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.8282 - accuracy: 0.7910 - val_loss: 0.8027 - val_accuracy: 0.7977\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7939 - accuracy: 0.8047 - val_loss: 0.8638 - val_accuracy: 0.7717\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7756 - accuracy: 0.8138 - val_loss: 0.8077 - val_accuracy: 0.8063\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7593 - accuracy: 0.8204 - val_loss: 0.8316 - val_accuracy: 0.8014\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7514 - accuracy: 0.8234 - val_loss: 0.9351 - val_accuracy: 0.7761\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7307 - accuracy: 0.8320 - val_loss: 0.7667 - val_accuracy: 0.8176\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7178 - accuracy: 0.8364 - val_loss: 0.8179 - val_accuracy: 0.8131\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7176 - accuracy: 0.8393 - val_loss: 0.9045 - val_accuracy: 0.7888\n",
      "iteration:  0\n",
      "Test loss: 0.9045247279167176\n",
      "Test accuracy: 0.7888000011444092\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8312 - accuracy: 0.8020 - val_loss: 0.9177 - val_accuracy: 0.7789\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.8021 - accuracy: 0.8085 - val_loss: 0.8352 - val_accuracy: 0.8065\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7894 - accuracy: 0.8121 - val_loss: 0.9112 - val_accuracy: 0.7711\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7822 - accuracy: 0.8119 - val_loss: 0.8902 - val_accuracy: 0.7845\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7809 - accuracy: 0.8122 - val_loss: 0.9337 - val_accuracy: 0.7768\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7676 - accuracy: 0.8157 - val_loss: 0.9778 - val_accuracy: 0.7595\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7635 - accuracy: 0.8186 - val_loss: 0.9109 - val_accuracy: 0.7808\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7628 - accuracy: 0.8181 - val_loss: 0.8880 - val_accuracy: 0.7863\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7560 - accuracy: 0.8179 - val_loss: 0.9309 - val_accuracy: 0.7712\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7503 - accuracy: 0.8198 - val_loss: 0.7922 - val_accuracy: 0.8083\n",
      "iteration:  1\n",
      "Test loss: 0.7921641134262085\n",
      "Test accuracy: 0.8083000183105469\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7287 - accuracy: 0.8296 - val_loss: 0.9989 - val_accuracy: 0.7627\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7159 - accuracy: 0.8326 - val_loss: 1.0781 - val_accuracy: 0.7534\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7023 - accuracy: 0.8341 - val_loss: 0.9195 - val_accuracy: 0.7829\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6955 - accuracy: 0.8347 - val_loss: 0.8606 - val_accuracy: 0.7918\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6943 - accuracy: 0.8356 - val_loss: 0.9284 - val_accuracy: 0.7657\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6942 - accuracy: 0.8358 - val_loss: 0.9243 - val_accuracy: 0.7815\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6844 - accuracy: 0.8383 - val_loss: 0.9836 - val_accuracy: 0.7675\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6894 - accuracy: 0.8397 - val_loss: 0.9420 - val_accuracy: 0.7558\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6796 - accuracy: 0.8429 - val_loss: 0.9527 - val_accuracy: 0.7705\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6836 - accuracy: 0.8403 - val_loss: 0.8498 - val_accuracy: 0.7999\n",
      "iteration:  2\n",
      "Test loss: 0.8498236435890197\n",
      "Test accuracy: 0.7998999953269958\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7108 - accuracy: 0.8291 - val_loss: 0.9957 - val_accuracy: 0.7482\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7062 - accuracy: 0.8310 - val_loss: 0.9672 - val_accuracy: 0.7523\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6982 - accuracy: 0.8326 - val_loss: 1.1305 - val_accuracy: 0.7257\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6964 - accuracy: 0.8341 - val_loss: 1.2857 - val_accuracy: 0.7214\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6866 - accuracy: 0.8355 - val_loss: 1.1783 - val_accuracy: 0.7313\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6826 - accuracy: 0.8373 - val_loss: 0.9944 - val_accuracy: 0.7637\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6809 - accuracy: 0.8385 - val_loss: 1.0842 - val_accuracy: 0.7476\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6718 - accuracy: 0.8413 - val_loss: 1.0168 - val_accuracy: 0.7478\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6709 - accuracy: 0.8420 - val_loss: 1.0066 - val_accuracy: 0.7573\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6739 - accuracy: 0.8394 - val_loss: 1.0571 - val_accuracy: 0.7469\n",
      "iteration:  3\n",
      "Test loss: 1.0570931953430176\n",
      "Test accuracy: 0.7469000220298767\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7428 - accuracy: 0.8183 - val_loss: 1.3972 - val_accuracy: 0.6690\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7283 - accuracy: 0.8209 - val_loss: 1.2953 - val_accuracy: 0.6865\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7271 - accuracy: 0.8230 - val_loss: 1.1586 - val_accuracy: 0.7074\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7150 - accuracy: 0.8235 - val_loss: 1.0673 - val_accuracy: 0.7358\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7135 - accuracy: 0.8260 - val_loss: 0.9935 - val_accuracy: 0.7478\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7105 - accuracy: 0.8256 - val_loss: 0.9909 - val_accuracy: 0.7510\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7031 - accuracy: 0.8281 - val_loss: 1.0301 - val_accuracy: 0.7499\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7023 - accuracy: 0.8276 - val_loss: 1.1816 - val_accuracy: 0.7193\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7009 - accuracy: 0.8279 - val_loss: 1.0223 - val_accuracy: 0.7480\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6996 - accuracy: 0.8274 - val_loss: 1.0538 - val_accuracy: 0.7361\n",
      "iteration:  4\n",
      "Test loss: 1.0538334323883056\n",
      "Test accuracy: 0.7361000180244446\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7349 - accuracy: 0.8162 - val_loss: 1.1954 - val_accuracy: 0.7080\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7249 - accuracy: 0.8190 - val_loss: 1.0566 - val_accuracy: 0.7391\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7203 - accuracy: 0.8189 - val_loss: 1.1161 - val_accuracy: 0.7226\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7145 - accuracy: 0.8214 - val_loss: 1.3589 - val_accuracy: 0.6722\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7092 - accuracy: 0.8228 - val_loss: 1.2305 - val_accuracy: 0.6944\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7100 - accuracy: 0.8204 - val_loss: 1.1446 - val_accuracy: 0.6978\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7023 - accuracy: 0.8235 - val_loss: 1.1167 - val_accuracy: 0.7131\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6983 - accuracy: 0.8238 - val_loss: 1.1828 - val_accuracy: 0.7173\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6983 - accuracy: 0.8260 - val_loss: 1.4049 - val_accuracy: 0.6485\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6979 - accuracy: 0.8251 - val_loss: 1.2661 - val_accuracy: 0.6824\n",
      "iteration:  5\n",
      "Test loss: 1.266094809436798\n",
      "Test accuracy: 0.6823999881744385\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7455 - accuracy: 0.8087 - val_loss: 1.1231 - val_accuracy: 0.6953\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7328 - accuracy: 0.8111 - val_loss: 1.3018 - val_accuracy: 0.6648\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7271 - accuracy: 0.8134 - val_loss: 1.2813 - val_accuracy: 0.6858\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7245 - accuracy: 0.8134 - val_loss: 1.4832 - val_accuracy: 0.6361\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7115 - accuracy: 0.8165 - val_loss: 1.5940 - val_accuracy: 0.6140\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7139 - accuracy: 0.8159 - val_loss: 1.4753 - val_accuracy: 0.6335\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.7123 - accuracy: 0.8174 - val_loss: 1.4198 - val_accuracy: 0.6426\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7111 - accuracy: 0.8174 - val_loss: 1.3479 - val_accuracy: 0.6559\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7070 - accuracy: 0.8177 - val_loss: 1.2970 - val_accuracy: 0.6570\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7035 - accuracy: 0.8168 - val_loss: 1.2883 - val_accuracy: 0.6592\n",
      "iteration:  6\n",
      "Test loss: 1.288341671562195\n",
      "Test accuracy: 0.6592000126838684\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7420 - accuracy: 0.8017 - val_loss: 1.7586 - val_accuracy: 0.5884\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7339 - accuracy: 0.8063 - val_loss: 1.2122 - val_accuracy: 0.6461\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7273 - accuracy: 0.8055 - val_loss: 1.3697 - val_accuracy: 0.6376\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7234 - accuracy: 0.8076 - val_loss: 1.4405 - val_accuracy: 0.6096\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7206 - accuracy: 0.8092 - val_loss: 1.2837 - val_accuracy: 0.6585\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7196 - accuracy: 0.8094 - val_loss: 1.2937 - val_accuracy: 0.6498\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7135 - accuracy: 0.8107 - val_loss: 1.2291 - val_accuracy: 0.6721\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7060 - accuracy: 0.8115 - val_loss: 1.4088 - val_accuracy: 0.6298\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7147 - accuracy: 0.8108 - val_loss: 1.3702 - val_accuracy: 0.6325\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7040 - accuracy: 0.8110 - val_loss: 1.4107 - val_accuracy: 0.6320\n",
      "iteration:  7\n",
      "Test loss: 1.4107311771392823\n",
      "Test accuracy: 0.6320000290870667\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7344 - accuracy: 0.8026 - val_loss: 1.4261 - val_accuracy: 0.6262\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7266 - accuracy: 0.8038 - val_loss: 1.4819 - val_accuracy: 0.6019\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7244 - accuracy: 0.8058 - val_loss: 1.4872 - val_accuracy: 0.5758\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7191 - accuracy: 0.8065 - val_loss: 1.5269 - val_accuracy: 0.5876\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7147 - accuracy: 0.8072 - val_loss: 1.4850 - val_accuracy: 0.6173\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7058 - accuracy: 0.8100 - val_loss: 1.3753 - val_accuracy: 0.6346\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7053 - accuracy: 0.8116 - val_loss: 1.4582 - val_accuracy: 0.6084\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7091 - accuracy: 0.8091 - val_loss: 1.4160 - val_accuracy: 0.6271\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7055 - accuracy: 0.8105 - val_loss: 1.3586 - val_accuracy: 0.6218\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7038 - accuracy: 0.8104 - val_loss: 1.4123 - val_accuracy: 0.6058\n",
      "iteration:  8\n",
      "Test loss: 1.4123190313339233\n",
      "Test accuracy: 0.6057999730110168\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7469 - accuracy: 0.7979 - val_loss: 1.4014 - val_accuracy: 0.5927\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7385 - accuracy: 0.7959 - val_loss: 1.5142 - val_accuracy: 0.5856\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7336 - accuracy: 0.7999 - val_loss: 1.3510 - val_accuracy: 0.6113\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7289 - accuracy: 0.8006 - val_loss: 1.5412 - val_accuracy: 0.5897\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7325 - accuracy: 0.7988 - val_loss: 1.6776 - val_accuracy: 0.5608\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7254 - accuracy: 0.8002 - val_loss: 1.3256 - val_accuracy: 0.6263\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7189 - accuracy: 0.8014 - val_loss: 1.6128 - val_accuracy: 0.5829\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7171 - accuracy: 0.8031 - val_loss: 1.6478 - val_accuracy: 0.5682\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7205 - accuracy: 0.8026 - val_loss: 1.4681 - val_accuracy: 0.5749\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7198 - accuracy: 0.8018 - val_loss: 1.5149 - val_accuracy: 0.5742\n",
      "iteration:  9\n",
      "Test loss: 1.5148974132537842\n",
      "Test accuracy: 0.5741999745368958\n",
      "rate=7.5:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (44118, 10)\n",
      "Shape y_true (5882, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 1.2104 - accuracy: 0.6848 - val_loss: 0.8528 - val_accuracy: 0.7719\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8897 - accuracy: 0.7691 - val_loss: 0.7988 - val_accuracy: 0.7937\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.8254 - accuracy: 0.7889 - val_loss: 0.7306 - val_accuracy: 0.8208\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7963 - accuracy: 0.8042 - val_loss: 0.7800 - val_accuracy: 0.8142\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7764 - accuracy: 0.8117 - val_loss: 0.9028 - val_accuracy: 0.7830\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7605 - accuracy: 0.8190 - val_loss: 0.7903 - val_accuracy: 0.8132\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7421 - accuracy: 0.8268 - val_loss: 0.8890 - val_accuracy: 0.7777\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7371 - accuracy: 0.8295 - val_loss: 0.8112 - val_accuracy: 0.8018\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7244 - accuracy: 0.8354 - val_loss: 0.7008 - val_accuracy: 0.8359\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7167 - accuracy: 0.8388 - val_loss: 0.9410 - val_accuracy: 0.7836\n",
      "iteration:  0\n",
      "Test loss: 0.9409720292091369\n",
      "Test accuracy: 0.7835999727249146\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8735 - accuracy: 0.7937 - val_loss: 0.8685 - val_accuracy: 0.7902\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8501 - accuracy: 0.7965 - val_loss: 0.8451 - val_accuracy: 0.7945\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8336 - accuracy: 0.8005 - val_loss: 0.9274 - val_accuracy: 0.7691\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8208 - accuracy: 0.8024 - val_loss: 1.4020 - val_accuracy: 0.6633\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8187 - accuracy: 0.8070 - val_loss: 0.9006 - val_accuracy: 0.7829\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8145 - accuracy: 0.8053 - val_loss: 1.0391 - val_accuracy: 0.7426\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8075 - accuracy: 0.8061 - val_loss: 0.9518 - val_accuracy: 0.7597\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8007 - accuracy: 0.8073 - val_loss: 1.1039 - val_accuracy: 0.7295\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7897 - accuracy: 0.8100 - val_loss: 0.8294 - val_accuracy: 0.8047\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7914 - accuracy: 0.8099 - val_loss: 0.9094 - val_accuracy: 0.7808\n",
      "iteration:  1\n",
      "Test loss: 0.9094095492839813\n",
      "Test accuracy: 0.7807999849319458\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7772 - accuracy: 0.8124 - val_loss: 1.1839 - val_accuracy: 0.7161\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7642 - accuracy: 0.8135 - val_loss: 1.1631 - val_accuracy: 0.7192\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7527 - accuracy: 0.8157 - val_loss: 1.0253 - val_accuracy: 0.7564\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7500 - accuracy: 0.8174 - val_loss: 1.0214 - val_accuracy: 0.7581\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7430 - accuracy: 0.8193 - val_loss: 0.9825 - val_accuracy: 0.7632\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7430 - accuracy: 0.8185 - val_loss: 0.9644 - val_accuracy: 0.7576\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7344 - accuracy: 0.8220 - val_loss: 1.0129 - val_accuracy: 0.7590\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7327 - accuracy: 0.8218 - val_loss: 0.9534 - val_accuracy: 0.7703\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7287 - accuracy: 0.8235 - val_loss: 1.0545 - val_accuracy: 0.7408\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7275 - accuracy: 0.8214 - val_loss: 0.9409 - val_accuracy: 0.7737\n",
      "iteration:  2\n",
      "Test loss: 0.9408547112464904\n",
      "Test accuracy: 0.7736999988555908\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7403 - accuracy: 0.8211 - val_loss: 0.9223 - val_accuracy: 0.7631\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7303 - accuracy: 0.8218 - val_loss: 0.9412 - val_accuracy: 0.7715\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7125 - accuracy: 0.8255 - val_loss: 1.0454 - val_accuracy: 0.7468\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7132 - accuracy: 0.8262 - val_loss: 1.1989 - val_accuracy: 0.7226\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7087 - accuracy: 0.8289 - val_loss: 0.9666 - val_accuracy: 0.7730\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7020 - accuracy: 0.8282 - val_loss: 1.0782 - val_accuracy: 0.7460\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7048 - accuracy: 0.8274 - val_loss: 1.1415 - val_accuracy: 0.7278\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6989 - accuracy: 0.8294 - val_loss: 1.1671 - val_accuracy: 0.7272\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6992 - accuracy: 0.8290 - val_loss: 1.1210 - val_accuracy: 0.7390\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6936 - accuracy: 0.8300 - val_loss: 0.9680 - val_accuracy: 0.7674\n",
      "iteration:  3\n",
      "Test loss: 0.9679871163368226\n",
      "Test accuracy: 0.7674000263214111\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 46ms/step - loss: 0.7082 - accuracy: 0.8239 - val_loss: 0.9864 - val_accuracy: 0.7530\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7018 - accuracy: 0.8277 - val_loss: 0.9433 - val_accuracy: 0.7687\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6932 - accuracy: 0.8279 - val_loss: 1.0605 - val_accuracy: 0.7420\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6871 - accuracy: 0.8306 - val_loss: 1.0853 - val_accuracy: 0.7371\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6827 - accuracy: 0.8317 - val_loss: 1.0994 - val_accuracy: 0.7361\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6788 - accuracy: 0.8331 - val_loss: 1.0935 - val_accuracy: 0.7381\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6804 - accuracy: 0.8329 - val_loss: 1.1053 - val_accuracy: 0.7360\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6754 - accuracy: 0.8368 - val_loss: 0.9826 - val_accuracy: 0.7626\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6663 - accuracy: 0.8375 - val_loss: 1.0633 - val_accuracy: 0.7450\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 46ms/step - loss: 0.6722 - accuracy: 0.8356 - val_loss: 0.9286 - val_accuracy: 0.7693\n",
      "iteration:  4\n",
      "Test loss: 0.9285796653747559\n",
      "Test accuracy: 0.7692999839782715\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7034 - accuracy: 0.8252 - val_loss: 1.0051 - val_accuracy: 0.7508\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6870 - accuracy: 0.8304 - val_loss: 1.4007 - val_accuracy: 0.6844\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6728 - accuracy: 0.8349 - val_loss: 1.0892 - val_accuracy: 0.7376\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6700 - accuracy: 0.8335 - val_loss: 1.0880 - val_accuracy: 0.7376\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6732 - accuracy: 0.8334 - val_loss: 1.0552 - val_accuracy: 0.7423\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6667 - accuracy: 0.8360 - val_loss: 1.0082 - val_accuracy: 0.7572\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6616 - accuracy: 0.8377 - val_loss: 0.9639 - val_accuracy: 0.7626\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6588 - accuracy: 0.8393 - val_loss: 0.9546 - val_accuracy: 0.7639\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6578 - accuracy: 0.8376 - val_loss: 1.0508 - val_accuracy: 0.7525\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6581 - accuracy: 0.8374 - val_loss: 1.0068 - val_accuracy: 0.7500\n",
      "iteration:  5\n",
      "Test loss: 1.0067802552223206\n",
      "Test accuracy: 0.75\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7047 - accuracy: 0.8259 - val_loss: 1.1599 - val_accuracy: 0.7227\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6910 - accuracy: 0.8257 - val_loss: 1.1852 - val_accuracy: 0.7067\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6888 - accuracy: 0.8261 - val_loss: 1.1841 - val_accuracy: 0.7103\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6777 - accuracy: 0.8293 - val_loss: 1.0903 - val_accuracy: 0.7379\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6838 - accuracy: 0.8282 - val_loss: 1.2716 - val_accuracy: 0.6886\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6708 - accuracy: 0.8308 - val_loss: 1.1003 - val_accuracy: 0.7296\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6674 - accuracy: 0.8332 - val_loss: 0.9892 - val_accuracy: 0.7639\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6685 - accuracy: 0.8308 - val_loss: 1.3264 - val_accuracy: 0.6845\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6731 - accuracy: 0.8305 - val_loss: 1.1732 - val_accuracy: 0.7210\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6660 - accuracy: 0.8330 - val_loss: 1.1670 - val_accuracy: 0.7240\n",
      "iteration:  6\n",
      "Test loss: 1.1670334733963013\n",
      "Test accuracy: 0.7239999771118164\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6920 - accuracy: 0.8245 - val_loss: 1.1511 - val_accuracy: 0.7226\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6795 - accuracy: 0.8259 - val_loss: 1.1365 - val_accuracy: 0.7155\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6720 - accuracy: 0.8287 - val_loss: 1.3081 - val_accuracy: 0.6911\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6663 - accuracy: 0.8305 - val_loss: 1.2453 - val_accuracy: 0.6949\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6626 - accuracy: 0.8312 - val_loss: 1.1172 - val_accuracy: 0.7288\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6585 - accuracy: 0.8314 - val_loss: 1.6983 - val_accuracy: 0.6492\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6545 - accuracy: 0.8327 - val_loss: 1.2835 - val_accuracy: 0.7039\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6531 - accuracy: 0.8340 - val_loss: 1.2939 - val_accuracy: 0.6980\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6566 - accuracy: 0.8348 - val_loss: 1.1584 - val_accuracy: 0.7146\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6528 - accuracy: 0.8343 - val_loss: 1.3088 - val_accuracy: 0.6968\n",
      "iteration:  7\n",
      "Test loss: 1.3087785945892334\n",
      "Test accuracy: 0.6967999935150146\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6820 - accuracy: 0.8221 - val_loss: 1.0888 - val_accuracy: 0.7221\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6724 - accuracy: 0.8257 - val_loss: 1.4292 - val_accuracy: 0.6526\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.6743 - accuracy: 0.8251 - val_loss: 1.2692 - val_accuracy: 0.6793\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6669 - accuracy: 0.8280 - val_loss: 1.1724 - val_accuracy: 0.6979\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6650 - accuracy: 0.8264 - val_loss: 1.3831 - val_accuracy: 0.6418\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6529 - accuracy: 0.8308 - val_loss: 1.2358 - val_accuracy: 0.6804\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6569 - accuracy: 0.8301 - val_loss: 1.2290 - val_accuracy: 0.7001\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6552 - accuracy: 0.8297 - val_loss: 1.2021 - val_accuracy: 0.6931\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6455 - accuracy: 0.8326 - val_loss: 1.1569 - val_accuracy: 0.6896\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6497 - accuracy: 0.8299 - val_loss: 1.3117 - val_accuracy: 0.6720\n",
      "iteration:  8\n",
      "Test loss: 1.3117145652770996\n",
      "Test accuracy: 0.671999990940094\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7031 - accuracy: 0.8144 - val_loss: 1.5291 - val_accuracy: 0.6444\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6937 - accuracy: 0.8154 - val_loss: 1.7473 - val_accuracy: 0.6129\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6876 - accuracy: 0.8173 - val_loss: 1.2999 - val_accuracy: 0.6621\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6795 - accuracy: 0.8200 - val_loss: 1.5600 - val_accuracy: 0.6217\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6752 - accuracy: 0.8202 - val_loss: 1.2478 - val_accuracy: 0.6680\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6706 - accuracy: 0.8234 - val_loss: 1.5060 - val_accuracy: 0.6445\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6734 - accuracy: 0.8226 - val_loss: 1.3427 - val_accuracy: 0.6603\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6712 - accuracy: 0.8216 - val_loss: 1.4510 - val_accuracy: 0.6504\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6701 - accuracy: 0.8225 - val_loss: 1.6584 - val_accuracy: 0.6195\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6660 - accuracy: 0.8235 - val_loss: 1.3408 - val_accuracy: 0.6558\n",
      "iteration:  9\n",
      "Test loss: 1.3408175603866577\n",
      "Test accuracy: 0.6557999849319458\n",
      "rate=9.0:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.8356999754905701\n",
      "x_true_pseudo.shape:  (50000, 32, 32, 3)\n",
      "Shape y_pseudo (45000, 10)\n",
      "Shape y_true (5000, 10)\n",
      "y_true_pseudo.shape:  (50000, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.2268 - accuracy: 0.6770 - val_loss: 0.9621 - val_accuracy: 0.7477\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8922 - accuracy: 0.7682 - val_loss: 0.8597 - val_accuracy: 0.7731\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8382 - accuracy: 0.7879 - val_loss: 0.9995 - val_accuracy: 0.7533\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8017 - accuracy: 0.8025 - val_loss: 0.7302 - val_accuracy: 0.8270\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7846 - accuracy: 0.8107 - val_loss: 0.7780 - val_accuracy: 0.8086\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7574 - accuracy: 0.8184 - val_loss: 0.8602 - val_accuracy: 0.7893\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7502 - accuracy: 0.8241 - val_loss: 0.8081 - val_accuracy: 0.8058\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7372 - accuracy: 0.8291 - val_loss: 0.7852 - val_accuracy: 0.8165\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7183 - accuracy: 0.8343 - val_loss: 0.8299 - val_accuracy: 0.8059\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 73s 47ms/step - loss: 0.7139 - accuracy: 0.8392 - val_loss: 0.7212 - val_accuracy: 0.8386\n",
      "iteration:  0\n",
      "Test loss: 0.7212490720748901\n",
      "Test accuracy: 0.8385999798774719\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7308 - accuracy: 0.8341 - val_loss: 0.8419 - val_accuracy: 0.7946\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7162 - accuracy: 0.8343 - val_loss: 0.9740 - val_accuracy: 0.7685\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7048 - accuracy: 0.8358 - val_loss: 1.0426 - val_accuracy: 0.7601\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6933 - accuracy: 0.8407 - val_loss: 0.7971 - val_accuracy: 0.8158\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6946 - accuracy: 0.8420 - val_loss: 0.9151 - val_accuracy: 0.7826\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6844 - accuracy: 0.8422 - val_loss: 0.8633 - val_accuracy: 0.7908\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6802 - accuracy: 0.8444 - val_loss: 0.7774 - val_accuracy: 0.8197\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6794 - accuracy: 0.8439 - val_loss: 0.7804 - val_accuracy: 0.8192\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6691 - accuracy: 0.8459 - val_loss: 0.7595 - val_accuracy: 0.8223\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6652 - accuracy: 0.8470 - val_loss: 0.7953 - val_accuracy: 0.8103\n",
      "iteration:  1\n",
      "Test loss: 0.7953081317901611\n",
      "Test accuracy: 0.8102999925613403\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7200 - accuracy: 0.8316 - val_loss: 1.1694 - val_accuracy: 0.7300\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.7136 - accuracy: 0.8327 - val_loss: 0.9629 - val_accuracy: 0.7652\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7000 - accuracy: 0.8346 - val_loss: 0.9090 - val_accuracy: 0.7755\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6921 - accuracy: 0.8381 - val_loss: 0.8758 - val_accuracy: 0.7875\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6863 - accuracy: 0.8381 - val_loss: 0.8964 - val_accuracy: 0.7804\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6784 - accuracy: 0.8393 - val_loss: 0.9440 - val_accuracy: 0.7671\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6764 - accuracy: 0.8399 - val_loss: 1.4392 - val_accuracy: 0.6828\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6807 - accuracy: 0.8415 - val_loss: 0.8644 - val_accuracy: 0.7946\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6767 - accuracy: 0.8402 - val_loss: 0.9139 - val_accuracy: 0.7835\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6691 - accuracy: 0.8430 - val_loss: 1.0037 - val_accuracy: 0.7661\n",
      "iteration:  2\n",
      "Test loss: 1.0036512302398681\n",
      "Test accuracy: 0.7660999894142151\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7152 - accuracy: 0.8274 - val_loss: 1.1699 - val_accuracy: 0.7340\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7033 - accuracy: 0.8284 - val_loss: 1.2143 - val_accuracy: 0.7155\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6917 - accuracy: 0.8310 - val_loss: 1.3179 - val_accuracy: 0.7027\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6825 - accuracy: 0.8311 - val_loss: 1.1717 - val_accuracy: 0.7245\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6855 - accuracy: 0.8321 - val_loss: 1.1481 - val_accuracy: 0.7337\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6845 - accuracy: 0.8317 - val_loss: 1.2456 - val_accuracy: 0.7032\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6746 - accuracy: 0.8357 - val_loss: 1.2508 - val_accuracy: 0.7039\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6763 - accuracy: 0.8350 - val_loss: 1.2105 - val_accuracy: 0.7178\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6719 - accuracy: 0.8365 - val_loss: 0.9766 - val_accuracy: 0.7558\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6726 - accuracy: 0.8358 - val_loss: 1.1908 - val_accuracy: 0.7208\n",
      "iteration:  3\n",
      "Test loss: 1.190794278717041\n",
      "Test accuracy: 0.72079998254776\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7103 - accuracy: 0.8198 - val_loss: 1.3363 - val_accuracy: 0.6810\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7037 - accuracy: 0.8240 - val_loss: 1.2130 - val_accuracy: 0.6910\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6939 - accuracy: 0.8242 - val_loss: 1.2745 - val_accuracy: 0.7009\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6889 - accuracy: 0.8267 - val_loss: 1.4472 - val_accuracy: 0.6516\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6824 - accuracy: 0.8279 - val_loss: 1.2166 - val_accuracy: 0.6926\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6801 - accuracy: 0.8274 - val_loss: 1.3822 - val_accuracy: 0.6781\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6727 - accuracy: 0.8284 - val_loss: 1.1021 - val_accuracy: 0.7244\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6740 - accuracy: 0.8280 - val_loss: 1.4600 - val_accuracy: 0.6773\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6699 - accuracy: 0.8296 - val_loss: 1.1547 - val_accuracy: 0.7100\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6670 - accuracy: 0.8303 - val_loss: 1.4405 - val_accuracy: 0.6755\n",
      "iteration:  4\n",
      "Test loss: 1.4405492961883546\n",
      "Test accuracy: 0.6754999756813049\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6778 - accuracy: 0.8283 - val_loss: 1.5113 - val_accuracy: 0.6301\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6735 - accuracy: 0.8283 - val_loss: 1.2760 - val_accuracy: 0.6871\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6651 - accuracy: 0.8316 - val_loss: 1.5042 - val_accuracy: 0.6418\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6653 - accuracy: 0.8314 - val_loss: 1.4074 - val_accuracy: 0.6405\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6603 - accuracy: 0.8303 - val_loss: 1.3371 - val_accuracy: 0.6596\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6561 - accuracy: 0.8352 - val_loss: 1.3124 - val_accuracy: 0.6745\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6549 - accuracy: 0.8324 - val_loss: 1.3180 - val_accuracy: 0.6641\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6490 - accuracy: 0.8343 - val_loss: 1.3269 - val_accuracy: 0.6564\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6500 - accuracy: 0.8337 - val_loss: 1.4241 - val_accuracy: 0.6521\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6467 - accuracy: 0.8371 - val_loss: 1.3004 - val_accuracy: 0.6798\n",
      "iteration:  5\n",
      "Test loss: 1.3003827560424805\n",
      "Test accuracy: 0.6797999739646912\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6908 - accuracy: 0.8227 - val_loss: 1.3075 - val_accuracy: 0.6745\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6790 - accuracy: 0.8251 - val_loss: 1.4363 - val_accuracy: 0.6465\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6720 - accuracy: 0.8270 - val_loss: 1.5361 - val_accuracy: 0.6353\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6670 - accuracy: 0.8289 - val_loss: 1.4072 - val_accuracy: 0.6446\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6644 - accuracy: 0.8288 - val_loss: 1.3857 - val_accuracy: 0.6374\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6600 - accuracy: 0.8292 - val_loss: 1.5967 - val_accuracy: 0.6257\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6596 - accuracy: 0.8288 - val_loss: 1.2794 - val_accuracy: 0.6725\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6558 - accuracy: 0.8294 - val_loss: 1.4362 - val_accuracy: 0.6423\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6546 - accuracy: 0.8328 - val_loss: 1.5037 - val_accuracy: 0.6430\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6548 - accuracy: 0.8323 - val_loss: 1.4735 - val_accuracy: 0.6322\n",
      "iteration:  6\n",
      "Test loss: 1.4735360172271728\n",
      "Test accuracy: 0.6322000026702881\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6971 - accuracy: 0.8210 - val_loss: 1.3640 - val_accuracy: 0.6525\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 46ms/step - loss: 0.6824 - accuracy: 0.8208 - val_loss: 1.5372 - val_accuracy: 0.5998\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6785 - accuracy: 0.8220 - val_loss: 1.5742 - val_accuracy: 0.5955\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6763 - accuracy: 0.8236 - val_loss: 1.7546 - val_accuracy: 0.5991\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6717 - accuracy: 0.8249 - val_loss: 1.4571 - val_accuracy: 0.6168\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6686 - accuracy: 0.8242 - val_loss: 1.5380 - val_accuracy: 0.6220\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6684 - accuracy: 0.8261 - val_loss: 1.6044 - val_accuracy: 0.6208\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6663 - accuracy: 0.8233 - val_loss: 1.4143 - val_accuracy: 0.6143\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6598 - accuracy: 0.8272 - val_loss: 1.5052 - val_accuracy: 0.6271\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6641 - accuracy: 0.8289 - val_loss: 1.4740 - val_accuracy: 0.6215\n",
      "iteration:  7\n",
      "Test loss: 1.4740113874435425\n",
      "Test accuracy: 0.6215000152587891\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7126 - accuracy: 0.8106 - val_loss: 1.6130 - val_accuracy: 0.5911\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6971 - accuracy: 0.8133 - val_loss: 1.5530 - val_accuracy: 0.5985\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7013 - accuracy: 0.8129 - val_loss: 1.5635 - val_accuracy: 0.5922\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6948 - accuracy: 0.8172 - val_loss: 1.6093 - val_accuracy: 0.5804\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6878 - accuracy: 0.8173 - val_loss: 1.5383 - val_accuracy: 0.6076\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6789 - accuracy: 0.8180 - val_loss: 1.6743 - val_accuracy: 0.6038\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6795 - accuracy: 0.8176 - val_loss: 1.9287 - val_accuracy: 0.5767\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6843 - accuracy: 0.8166 - val_loss: 1.3907 - val_accuracy: 0.6337\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6875 - accuracy: 0.8176 - val_loss: 1.4113 - val_accuracy: 0.6074\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6736 - accuracy: 0.8199 - val_loss: 1.6439 - val_accuracy: 0.6011\n",
      "iteration:  8\n",
      "Test loss: 1.6439020515441896\n",
      "Test accuracy: 0.6011000275611877\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7124 - accuracy: 0.8092 - val_loss: 1.7904 - val_accuracy: 0.5719\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7009 - accuracy: 0.8126 - val_loss: 1.5487 - val_accuracy: 0.5989\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 0.6891 - accuracy: 0.8164 - val_loss: 1.7384 - val_accuracy: 0.5683\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.6866 - accuracy: 0.8149 - val_loss: 1.7574 - val_accuracy: 0.5824\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.6840 - accuracy: 0.8181 - val_loss: 1.6795 - val_accuracy: 0.5960\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6847 - accuracy: 0.8159 - val_loss: 1.8019 - val_accuracy: 0.5651\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6777 - accuracy: 0.8173 - val_loss: 1.5507 - val_accuracy: 0.5862\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6781 - accuracy: 0.8197 - val_loss: 1.6092 - val_accuracy: 0.5915\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.6782 - accuracy: 0.8174 - val_loss: 1.6291 - val_accuracy: 0.5869\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.6731 - accuracy: 0.8203 - val_loss: 1.4221 - val_accuracy: 0.6206\n",
      "iteration:  9\n",
      "Test loss: 1.4220523723602294\n",
      "Test accuracy: 0.6205999851226807\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_augmentation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d5100a94fa3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrate7\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0maccuracy_7\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstns_full_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrate7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mteacher_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_supervised\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-384133ab5a68>\u001b[0m in \u001b[0;36mstns_full_dataset\u001b[0;34m(x_train, y_train, x_test, y_test, rate, teacher, student, teacher_path, accuracy_supervised)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy top 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resnet - Test1 - Accuracy as a function of rate - No data augmentation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_augmentation' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test1\n",
    "#Restnet20 on full CIFAR10 train dataset without data augmentation\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_cifar10_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "accuracy_supervised, teacher_path = train_model(init, teacher, x_train, y_train, x_test, y_test, batch_size=32, num_classes=10, epochs=100, \n",
    "                                  data_augmentation=False, model_name= 'keras_cifar10_trained_teacher_nda.h5')\n",
    "\n",
    "rate7=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,9])\n",
    "accuracy_7=stns_full_dataset(x_train,y_train,x_test,y_test,rate7,teacher, student,teacher_path,accuracy_supervised, data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test1\n",
    "#Restnet20 on full House numbers train dataset with data augmentation\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_house_numbers_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "accuracy_supervised, teacher_path = train_model(init, teacher, x_train, y_train, x_test, y_test, batch_size=32, num_classes=10, epochs=100, \n",
    "                                  data_augmentation=True, model_name= 'keras_housenumbers_trained_teacher_da.h5')\n",
    "\n",
    "rate8=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,9])\n",
    "accuracy_8=stns_full_dataset(x_train,y_train,x_test,y_test,rate8,teacher, student,teacher_path,accuracy_supervised, data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train is: (32, 32, 3, 73257)\n",
      "Shape of y_train is: (73257, 1)\n",
      "Shape of x_test is: (32, 32, 3, 26032)\n",
      "Shape of y_test is: (26032, 1)\n",
      "Shape of x_train is now: (73257, 32, 32, 3)\n",
      "Shape of x_test is now: (26032, 32, 32, 3)\n",
      "Shape of y_train is now: (73257,)\n",
      "Shape of y_test is now: (26032,)\n",
      "labels of y_train are [0 1 2 3 4 5 6 7 8 9]\n",
      "labels of y_test are [0 1 2 3 4 5 6 7 8 9]\n",
      "Learning rate:  0.001\n",
      "Learning rate:  0.001\n",
      "ResNet20v1\n",
      "Learning rate:  0.001\n",
      "Not using data augmentation.\n",
      "Train on 73257 samples, validate on 26032 samples\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 106s 1ms/step - loss: 0.8105 - accuracy: 0.7892 - val_loss: 0.5988 - val_accuracy: 0.8658\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      "  160/73257 [..............................] - ETA: 1:29 - loss: 0.4245 - accuracy: 0.9125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.4358 - accuracy: 0.9140 - val_loss: 0.4961 - val_accuracy: 0.8917\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.3889 - accuracy: 0.9262 - val_loss: 0.6137 - val_accuracy: 0.8504\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.3611 - accuracy: 0.9339 - val_loss: 0.3491 - val_accuracy: 0.9363\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.3401 - accuracy: 0.9381 - val_loss: 0.4580 - val_accuracy: 0.9034\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.3245 - accuracy: 0.9421 - val_loss: 0.3469 - val_accuracy: 0.9373\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.3157 - accuracy: 0.9456 - val_loss: 0.4070 - val_accuracy: 0.9159\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.3020 - accuracy: 0.9476 - val_loss: 0.3353 - val_accuracy: 0.9398\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2963 - accuracy: 0.9490 - val_loss: 0.3285 - val_accuracy: 0.9415\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2866 - accuracy: 0.9508 - val_loss: 0.4828 - val_accuracy: 0.8964\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2823 - accuracy: 0.9534 - val_loss: 0.3478 - val_accuracy: 0.9362\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2773 - accuracy: 0.9545 - val_loss: 0.3115 - val_accuracy: 0.9460\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2705 - accuracy: 0.9563 - val_loss: 0.3881 - val_accuracy: 0.9225\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2674 - accuracy: 0.9571 - val_loss: 0.3566 - val_accuracy: 0.9360\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.2643 - accuracy: 0.9579 - val_loss: 0.3303 - val_accuracy: 0.9423\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2598 - accuracy: 0.9600 - val_loss: 0.3129 - val_accuracy: 0.9455\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.2569 - accuracy: 0.9597 - val_loss: 0.3276 - val_accuracy: 0.9422\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2547 - accuracy: 0.9604 - val_loss: 0.3174 - val_accuracy: 0.9457\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2524 - accuracy: 0.9614 - val_loss: 0.3026 - val_accuracy: 0.9506\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2526 - accuracy: 0.9619 - val_loss: 0.3308 - val_accuracy: 0.9392\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2462 - accuracy: 0.9637 - val_loss: 0.3406 - val_accuracy: 0.9368\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2467 - accuracy: 0.9627 - val_loss: 0.3126 - val_accuracy: 0.9459\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2419 - accuracy: 0.9646 - val_loss: 0.4056 - val_accuracy: 0.9224\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.2404 - accuracy: 0.9655 - val_loss: 0.3208 - val_accuracy: 0.9444\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2402 - accuracy: 0.9645 - val_loss: 0.3046 - val_accuracy: 0.9516\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2392 - accuracy: 0.9656 - val_loss: 0.3355 - val_accuracy: 0.9426\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2366 - accuracy: 0.9668 - val_loss: 0.3202 - val_accuracy: 0.9467\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2335 - accuracy: 0.9675 - val_loss: 0.3893 - val_accuracy: 0.9272\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2332 - accuracy: 0.9674 - val_loss: 0.3462 - val_accuracy: 0.9397\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2331 - accuracy: 0.9674 - val_loss: 0.3505 - val_accuracy: 0.9370\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2312 - accuracy: 0.9670 - val_loss: 0.3327 - val_accuracy: 0.9453\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2289 - accuracy: 0.9679 - val_loss: 0.3362 - val_accuracy: 0.9436\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2283 - accuracy: 0.9681 - val_loss: 0.3528 - val_accuracy: 0.9392\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2284 - accuracy: 0.9685 - val_loss: 0.3483 - val_accuracy: 0.9421\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2251 - accuracy: 0.9696 - val_loss: 0.3368 - val_accuracy: 0.9411\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2250 - accuracy: 0.9687 - val_loss: 0.3204 - val_accuracy: 0.9476\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 103s 1ms/step - loss: 0.2245 - accuracy: 0.9693 - val_loss: 0.3944 - val_accuracy: 0.9256\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2205 - accuracy: 0.9710 - val_loss: 0.3275 - val_accuracy: 0.9457\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2220 - accuracy: 0.9705 - val_loss: 0.3361 - val_accuracy: 0.9443\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2188 - accuracy: 0.9704 - val_loss: 0.3384 - val_accuracy: 0.9412\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2192 - accuracy: 0.9704 - val_loss: 0.3615 - val_accuracy: 0.9360\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 103s 1ms/step - loss: 0.2205 - accuracy: 0.9708 - val_loss: 0.3315 - val_accuracy: 0.9499\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2177 - accuracy: 0.9716 - val_loss: 0.3543 - val_accuracy: 0.9432\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.2185 - accuracy: 0.9710 - val_loss: 0.3337 - val_accuracy: 0.9475\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2182 - accuracy: 0.9711 - val_loss: 0.3712 - val_accuracy: 0.9325\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2158 - accuracy: 0.9716 - val_loss: 0.3474 - val_accuracy: 0.9412\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2150 - accuracy: 0.9716 - val_loss: 0.3334 - val_accuracy: 0.9434\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.2151 - accuracy: 0.9719 - val_loss: 0.3718 - val_accuracy: 0.9374\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.2154 - accuracy: 0.9715 - val_loss: 0.3581 - val_accuracy: 0.9402\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2146 - accuracy: 0.9718 - val_loss: 0.3546 - val_accuracy: 0.9423\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2140 - accuracy: 0.9719 - val_loss: 0.3523 - val_accuracy: 0.9394\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2146 - accuracy: 0.9719 - val_loss: 0.3492 - val_accuracy: 0.9407\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2123 - accuracy: 0.9719 - val_loss: 0.3441 - val_accuracy: 0.9436\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2142 - accuracy: 0.9716 - val_loss: 0.3639 - val_accuracy: 0.9353\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2083 - accuracy: 0.9728 - val_loss: 0.3331 - val_accuracy: 0.9476\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2132 - accuracy: 0.9720 - val_loss: 0.3726 - val_accuracy: 0.9336\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2083 - accuracy: 0.9734 - val_loss: 0.3797 - val_accuracy: 0.9377\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2091 - accuracy: 0.9737 - val_loss: 0.3394 - val_accuracy: 0.9439\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2113 - accuracy: 0.9730 - val_loss: 0.4014 - val_accuracy: 0.9334\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2085 - accuracy: 0.9738 - val_loss: 0.3871 - val_accuracy: 0.9403\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2088 - accuracy: 0.9732 - val_loss: 0.3900 - val_accuracy: 0.9352\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2070 - accuracy: 0.9740 - val_loss: 0.3836 - val_accuracy: 0.9363\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2099 - accuracy: 0.9729 - val_loss: 0.3584 - val_accuracy: 0.9412\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2060 - accuracy: 0.9741 - val_loss: 0.3481 - val_accuracy: 0.9449\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2066 - accuracy: 0.9740 - val_loss: 0.3551 - val_accuracy: 0.9405\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2068 - accuracy: 0.9727 - val_loss: 0.3868 - val_accuracy: 0.9342\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2046 - accuracy: 0.9746 - val_loss: 0.3755 - val_accuracy: 0.9375\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2031 - accuracy: 0.9748 - val_loss: 0.3762 - val_accuracy: 0.9354\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2068 - accuracy: 0.9731 - val_loss: 0.3790 - val_accuracy: 0.9376\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2062 - accuracy: 0.9743 - val_loss: 0.3373 - val_accuracy: 0.9428\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2038 - accuracy: 0.9739 - val_loss: 0.3675 - val_accuracy: 0.9407\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2034 - accuracy: 0.9752 - val_loss: 0.3599 - val_accuracy: 0.9426\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2053 - accuracy: 0.9730 - val_loss: 0.3677 - val_accuracy: 0.9374\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2032 - accuracy: 0.9741 - val_loss: 0.4101 - val_accuracy: 0.9276\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2037 - accuracy: 0.9746 - val_loss: 0.3384 - val_accuracy: 0.9453\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2040 - accuracy: 0.9740 - val_loss: 0.3627 - val_accuracy: 0.9428\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2020 - accuracy: 0.9754 - val_loss: 0.3876 - val_accuracy: 0.9378\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.2033 - accuracy: 0.9750 - val_loss: 0.3539 - val_accuracy: 0.9464\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.2021 - accuracy: 0.9748 - val_loss: 0.3699 - val_accuracy: 0.9395\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 103s 1ms/step - loss: 0.2021 - accuracy: 0.9749 - val_loss: 0.3549 - val_accuracy: 0.9425\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.2021 - accuracy: 0.9749 - val_loss: 0.3513 - val_accuracy: 0.9446\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.1550 - accuracy: 0.9897 - val_loss: 0.3104 - val_accuracy: 0.9560\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.1345 - accuracy: 0.9955 - val_loss: 0.3217 - val_accuracy: 0.9546\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.1236 - accuracy: 0.9974 - val_loss: 0.3246 - val_accuracy: 0.9547\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.1143 - accuracy: 0.9984 - val_loss: 0.3331 - val_accuracy: 0.9545\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.1065 - accuracy: 0.9990 - val_loss: 0.3387 - val_accuracy: 0.9542\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.0996 - accuracy: 0.9992 - val_loss: 0.3406 - val_accuracy: 0.9538\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 103s 1ms/step - loss: 0.0934 - accuracy: 0.9993 - val_loss: 0.3463 - val_accuracy: 0.9543\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.0883 - accuracy: 0.9994 - val_loss: 0.3532 - val_accuracy: 0.9530\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 98s 1ms/step - loss: 0.0837 - accuracy: 0.9995 - val_loss: 0.3524 - val_accuracy: 0.9533\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 103s 1ms/step - loss: 0.0793 - accuracy: 0.9995 - val_loss: 0.3621 - val_accuracy: 0.9531\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.0755 - accuracy: 0.9994 - val_loss: 0.3449 - val_accuracy: 0.9538\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.0731 - accuracy: 0.9992 - val_loss: 0.3476 - val_accuracy: 0.9541\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.0703 - accuracy: 0.9994 - val_loss: 0.3477 - val_accuracy: 0.9536\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 101s 1ms/step - loss: 0.0676 - accuracy: 0.9995 - val_loss: 0.3484 - val_accuracy: 0.9531\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 99s 1ms/step - loss: 0.0657 - accuracy: 0.9993 - val_loss: 0.3647 - val_accuracy: 0.9512\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 102s 1ms/step - loss: 0.0635 - accuracy: 0.9995 - val_loss: 0.3646 - val_accuracy: 0.9520\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.0620 - accuracy: 0.9992 - val_loss: 0.3787 - val_accuracy: 0.9494\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.0600 - accuracy: 0.9995 - val_loss: 0.3661 - val_accuracy: 0.9532\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      "73257/73257 [==============================] - 100s 1ms/step - loss: 0.0582 - accuracy: 0.9994 - val_loss: 0.3594 - val_accuracy: 0.9517\n",
      "26032/26032 [==============================] - 9s 352us/step\n",
      "Supervised learning model with 100epochs \n",
      "\n",
      "Test loss: 0.3594213967024364\n",
      "Test accuracy: 0.9517132639884949\n",
      "Saved trained model at /home/ANDURAND/pandurand/saved_models/keras_housenumbers_trained_teacher_nda.h5 \n",
      "rate=0.05:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (3489, 10)\n",
      "Shape y_true (69768, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 110s 48ms/step - loss: 0.6280 - accuracy: 0.8509 - val_loss: 0.3247 - val_accuracy: 0.9304\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "   3/2290 [..............................] - ETA: 1:42 - loss: 0.4089 - accuracy: 0.8958"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANDURAND/pandurand/anaconda3/envs/gpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4593 - accuracy: 0.8936 - val_loss: 0.3239 - val_accuracy: 0.9325\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4431 - accuracy: 0.9032 - val_loss: 0.3284 - val_accuracy: 0.9310\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4312 - accuracy: 0.9082 - val_loss: 0.3424 - val_accuracy: 0.9309\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4185 - accuracy: 0.9122 - val_loss: 0.3508 - val_accuracy: 0.9317\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4163 - accuracy: 0.9171 - val_loss: 0.3546 - val_accuracy: 0.9320\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4122 - accuracy: 0.9180 - val_loss: 0.3503 - val_accuracy: 0.9374\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4109 - accuracy: 0.9199 - val_loss: 0.3439 - val_accuracy: 0.9371\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4070 - accuracy: 0.9205 - val_loss: 0.3349 - val_accuracy: 0.9403\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4037 - accuracy: 0.9215 - val_loss: 0.3610 - val_accuracy: 0.9307\n",
      "iteration:  0\n",
      "Test loss: 0.3610081751433046\n",
      "Test accuracy: 0.9307391047477722\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4034 - accuracy: 0.9225 - val_loss: 0.3305 - val_accuracy: 0.9425\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4001 - accuracy: 0.9241 - val_loss: 0.3414 - val_accuracy: 0.9383\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3950 - accuracy: 0.9254 - val_loss: 0.3535 - val_accuracy: 0.9382\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3953 - accuracy: 0.9260 - val_loss: 0.3155 - val_accuracy: 0.9468\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3952 - accuracy: 0.9256 - val_loss: 0.3915 - val_accuracy: 0.9206\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3941 - accuracy: 0.9259 - val_loss: 0.3434 - val_accuracy: 0.9364\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3868 - accuracy: 0.9272 - val_loss: 0.3425 - val_accuracy: 0.9371\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3882 - accuracy: 0.9280 - val_loss: 0.3395 - val_accuracy: 0.9369\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3880 - accuracy: 0.9281 - val_loss: 0.3455 - val_accuracy: 0.9379\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3854 - accuracy: 0.9271 - val_loss: 0.3468 - val_accuracy: 0.9354\n",
      "iteration:  1\n",
      "Test loss: 0.34675699062118814\n",
      "Test accuracy: 0.9354256391525269\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3841 - accuracy: 0.9293 - val_loss: 0.3291 - val_accuracy: 0.9444\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3802 - accuracy: 0.9287 - val_loss: 0.3457 - val_accuracy: 0.9366\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3810 - accuracy: 0.9297 - val_loss: 0.3491 - val_accuracy: 0.9365\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3783 - accuracy: 0.9301 - val_loss: 0.3080 - val_accuracy: 0.9480\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3799 - accuracy: 0.9295 - val_loss: 0.3443 - val_accuracy: 0.9363\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3787 - accuracy: 0.9298 - val_loss: 0.3420 - val_accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3738 - accuracy: 0.9320 - val_loss: 0.3571 - val_accuracy: 0.9344\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3756 - accuracy: 0.9309 - val_loss: 0.3517 - val_accuracy: 0.9334\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3723 - accuracy: 0.9315 - val_loss: 0.3178 - val_accuracy: 0.9453\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3733 - accuracy: 0.9320 - val_loss: 0.3294 - val_accuracy: 0.9425\n",
      "iteration:  2\n",
      "Test loss: 0.3293786886953546\n",
      "Test accuracy: 0.9425322413444519\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3746 - accuracy: 0.9318 - val_loss: 0.3413 - val_accuracy: 0.9390\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3723 - accuracy: 0.9320 - val_loss: 0.3257 - val_accuracy: 0.9430\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3678 - accuracy: 0.9319 - val_loss: 0.3469 - val_accuracy: 0.9362\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3699 - accuracy: 0.9323 - val_loss: 0.3394 - val_accuracy: 0.9393\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3680 - accuracy: 0.9317 - val_loss: 0.3407 - val_accuracy: 0.9397\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3676 - accuracy: 0.9324 - val_loss: 0.3543 - val_accuracy: 0.9348\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3679 - accuracy: 0.9327 - val_loss: 0.3650 - val_accuracy: 0.9294\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3667 - accuracy: 0.9323 - val_loss: 0.3826 - val_accuracy: 0.9232\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3659 - accuracy: 0.9329 - val_loss: 0.3757 - val_accuracy: 0.9286\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3640 - accuracy: 0.9341 - val_loss: 0.3552 - val_accuracy: 0.9316\n",
      "iteration:  3\n",
      "Test loss: 0.35524856050503933\n",
      "Test accuracy: 0.9315841794013977\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3690 - accuracy: 0.9317 - val_loss: 0.3212 - val_accuracy: 0.9415\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3660 - accuracy: 0.9333 - val_loss: 0.3148 - val_accuracy: 0.9457\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3660 - accuracy: 0.9325 - val_loss: 0.3354 - val_accuracy: 0.9401\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3642 - accuracy: 0.9335 - val_loss: 0.3142 - val_accuracy: 0.9463\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3667 - accuracy: 0.9335 - val_loss: 0.3358 - val_accuracy: 0.9401\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3639 - accuracy: 0.9335 - val_loss: 0.3687 - val_accuracy: 0.9307\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3679 - accuracy: 0.9330 - val_loss: 0.3357 - val_accuracy: 0.9400\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3656 - accuracy: 0.9325 - val_loss: 0.3400 - val_accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3628 - accuracy: 0.9343 - val_loss: 0.3411 - val_accuracy: 0.9351\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3635 - accuracy: 0.9336 - val_loss: 0.3139 - val_accuracy: 0.9450\n",
      "iteration:  4\n",
      "Test loss: 0.313872735729852\n",
      "Test accuracy: 0.9449523687362671\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3619 - accuracy: 0.9335 - val_loss: 0.3316 - val_accuracy: 0.9401\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3610 - accuracy: 0.9347 - val_loss: 0.3160 - val_accuracy: 0.9451\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3575 - accuracy: 0.9342 - val_loss: 0.3192 - val_accuracy: 0.9456\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3655 - accuracy: 0.9339 - val_loss: 0.3126 - val_accuracy: 0.9455\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3569 - accuracy: 0.9346 - val_loss: 0.3544 - val_accuracy: 0.9351\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3595 - accuracy: 0.9345 - val_loss: 0.3259 - val_accuracy: 0.9408\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3593 - accuracy: 0.9345 - val_loss: 0.3539 - val_accuracy: 0.9336\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3557 - accuracy: 0.9348 - val_loss: 0.3623 - val_accuracy: 0.9334\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3581 - accuracy: 0.9351 - val_loss: 0.3455 - val_accuracy: 0.9370\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3568 - accuracy: 0.9353 - val_loss: 0.3262 - val_accuracy: 0.9411\n",
      "iteration:  5\n",
      "Test loss: 0.3262059853973752\n",
      "Test accuracy: 0.9411493539810181\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3611 - accuracy: 0.9335 - val_loss: 0.3359 - val_accuracy: 0.9390\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3591 - accuracy: 0.9338 - val_loss: 0.3069 - val_accuracy: 0.9467\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3552 - accuracy: 0.9348 - val_loss: 0.3435 - val_accuracy: 0.9351\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3539 - accuracy: 0.9354 - val_loss: 0.3211 - val_accuracy: 0.9446\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3577 - accuracy: 0.9350 - val_loss: 0.3295 - val_accuracy: 0.9402\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3576 - accuracy: 0.9348 - val_loss: 0.3263 - val_accuracy: 0.9399\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3546 - accuracy: 0.9362 - val_loss: 0.3499 - val_accuracy: 0.9352\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3567 - accuracy: 0.9355 - val_loss: 0.3528 - val_accuracy: 0.9344\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3562 - accuracy: 0.9355 - val_loss: 0.3085 - val_accuracy: 0.9462\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3572 - accuracy: 0.9347 - val_loss: 0.3278 - val_accuracy: 0.9412\n",
      "iteration:  6\n",
      "Test loss: 0.32780493921345555\n",
      "Test accuracy: 0.9412261843681335\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3564 - accuracy: 0.9347 - val_loss: 0.3132 - val_accuracy: 0.9452\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3527 - accuracy: 0.9347 - val_loss: 0.3635 - val_accuracy: 0.9334\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3561 - accuracy: 0.9347 - val_loss: 0.3675 - val_accuracy: 0.9274\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3563 - accuracy: 0.9351 - val_loss: 0.3210 - val_accuracy: 0.9436\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3527 - accuracy: 0.9350 - val_loss: 0.3328 - val_accuracy: 0.9405\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3533 - accuracy: 0.9360 - val_loss: 0.3380 - val_accuracy: 0.9394\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3550 - accuracy: 0.9362 - val_loss: 0.3122 - val_accuracy: 0.9445\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3528 - accuracy: 0.9365 - val_loss: 0.3390 - val_accuracy: 0.9372\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3542 - accuracy: 0.9364 - val_loss: 0.3160 - val_accuracy: 0.9446\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3513 - accuracy: 0.9353 - val_loss: 0.3129 - val_accuracy: 0.9468\n",
      "iteration:  7\n",
      "Test loss: 0.31285393287066526\n",
      "Test accuracy: 0.9468346834182739\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3527 - accuracy: 0.9358 - val_loss: 0.3209 - val_accuracy: 0.9442\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3540 - accuracy: 0.9364 - val_loss: 0.3093 - val_accuracy: 0.9484\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3496 - accuracy: 0.9365 - val_loss: 0.3300 - val_accuracy: 0.9394\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3542 - accuracy: 0.9355 - val_loss: 0.3306 - val_accuracy: 0.9392\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3526 - accuracy: 0.9357 - val_loss: 0.3333 - val_accuracy: 0.9385\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3534 - accuracy: 0.9357 - val_loss: 0.3320 - val_accuracy: 0.9383\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3538 - accuracy: 0.9356 - val_loss: 0.3390 - val_accuracy: 0.9370\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3509 - accuracy: 0.9354 - val_loss: 0.3286 - val_accuracy: 0.9393\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3490 - accuracy: 0.9367 - val_loss: 0.3226 - val_accuracy: 0.9416\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3531 - accuracy: 0.9366 - val_loss: 0.3281 - val_accuracy: 0.9414\n",
      "iteration:  8\n",
      "Test loss: 0.3281457564016757\n",
      "Test accuracy: 0.9414182305335999\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3530 - accuracy: 0.9361 - val_loss: 0.3473 - val_accuracy: 0.9350\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3498 - accuracy: 0.9366 - val_loss: 0.3303 - val_accuracy: 0.9420\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3487 - accuracy: 0.9365 - val_loss: 0.3294 - val_accuracy: 0.9405\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3517 - accuracy: 0.9366 - val_loss: 0.3286 - val_accuracy: 0.9422\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3505 - accuracy: 0.9371 - val_loss: 0.3304 - val_accuracy: 0.9422\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3472 - accuracy: 0.9371 - val_loss: 0.3245 - val_accuracy: 0.9420\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3521 - accuracy: 0.9371 - val_loss: 0.3180 - val_accuracy: 0.9455\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3478 - accuracy: 0.9369 - val_loss: 0.3481 - val_accuracy: 0.9353\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3475 - accuracy: 0.9365 - val_loss: 0.3321 - val_accuracy: 0.9396\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3489 - accuracy: 0.9364 - val_loss: 0.3443 - val_accuracy: 0.9344\n",
      "iteration:  9\n",
      "Test loss: 0.3443211453297641\n",
      "Test accuracy: 0.9344268441200256\n",
      "rate=0.1:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (6660, 10)\n",
      "Shape y_true (66597, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.6204 - accuracy: 0.8509 - val_loss: 0.3546 - val_accuracy: 0.9270\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4670 - accuracy: 0.8949 - val_loss: 0.3512 - val_accuracy: 0.9292\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4508 - accuracy: 0.9025 - val_loss: 0.3656 - val_accuracy: 0.9260\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.4391 - accuracy: 0.9088 - val_loss: 0.3306 - val_accuracy: 0.9389\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4292 - accuracy: 0.9127 - val_loss: 0.3881 - val_accuracy: 0.9195\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.4175 - accuracy: 0.9163 - val_loss: 0.3543 - val_accuracy: 0.9320\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4161 - accuracy: 0.9181 - val_loss: 0.3945 - val_accuracy: 0.9180\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4145 - accuracy: 0.9186 - val_loss: 0.3404 - val_accuracy: 0.9393\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4096 - accuracy: 0.9203 - val_loss: 0.3347 - val_accuracy: 0.9403\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4075 - accuracy: 0.9208 - val_loss: 0.3367 - val_accuracy: 0.9402\n",
      "iteration:  0\n",
      "Test loss: 0.33673658225165604\n",
      "Test accuracy: 0.9401505589485168\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4013 - accuracy: 0.9227 - val_loss: 0.3329 - val_accuracy: 0.9399\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3961 - accuracy: 0.9244 - val_loss: 0.3295 - val_accuracy: 0.9415\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3975 - accuracy: 0.9236 - val_loss: 0.3621 - val_accuracy: 0.9313\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3948 - accuracy: 0.9248 - val_loss: 0.3708 - val_accuracy: 0.9278\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3910 - accuracy: 0.9259 - val_loss: 0.3258 - val_accuracy: 0.9447\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3892 - accuracy: 0.9261 - val_loss: 0.3454 - val_accuracy: 0.9367\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3897 - accuracy: 0.9260 - val_loss: 0.3129 - val_accuracy: 0.9468\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3843 - accuracy: 0.9270 - val_loss: 0.3310 - val_accuracy: 0.9432\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3852 - accuracy: 0.9263 - val_loss: 0.3254 - val_accuracy: 0.9426\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3857 - accuracy: 0.9278 - val_loss: 0.3199 - val_accuracy: 0.9433\n",
      "iteration:  1\n",
      "Test loss: 0.3199416871955956\n",
      "Test accuracy: 0.9432621598243713\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3823 - accuracy: 0.9281 - val_loss: 0.3313 - val_accuracy: 0.9417\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3826 - accuracy: 0.9287 - val_loss: 0.3088 - val_accuracy: 0.9482\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3764 - accuracy: 0.9290 - val_loss: 0.3212 - val_accuracy: 0.9435\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3742 - accuracy: 0.9293 - val_loss: 0.3384 - val_accuracy: 0.9365\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3733 - accuracy: 0.9308 - val_loss: 0.3218 - val_accuracy: 0.9436\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3759 - accuracy: 0.9299 - val_loss: 0.3413 - val_accuracy: 0.9382\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3750 - accuracy: 0.9306 - val_loss: 0.3310 - val_accuracy: 0.9409\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3733 - accuracy: 0.9289 - val_loss: 0.3489 - val_accuracy: 0.9342\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3721 - accuracy: 0.9299 - val_loss: 0.3280 - val_accuracy: 0.9406\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3705 - accuracy: 0.9315 - val_loss: 0.3170 - val_accuracy: 0.9453\n",
      "iteration:  2\n",
      "Test loss: 0.3169574000458814\n",
      "Test accuracy: 0.945259690284729\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3715 - accuracy: 0.9317 - val_loss: 0.3298 - val_accuracy: 0.9393\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3733 - accuracy: 0.9299 - val_loss: 0.3399 - val_accuracy: 0.9379\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3702 - accuracy: 0.9305 - val_loss: 0.3235 - val_accuracy: 0.9416\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3715 - accuracy: 0.9306 - val_loss: 0.3343 - val_accuracy: 0.9410\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3682 - accuracy: 0.9314 - val_loss: 0.3528 - val_accuracy: 0.9332\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3683 - accuracy: 0.9317 - val_loss: 0.3167 - val_accuracy: 0.9436\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3666 - accuracy: 0.9307 - val_loss: 0.4195 - val_accuracy: 0.9164\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 109s 47ms/step - loss: 0.3662 - accuracy: 0.9317 - val_loss: 0.3294 - val_accuracy: 0.9405\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3650 - accuracy: 0.9325 - val_loss: 0.3204 - val_accuracy: 0.9433\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3659 - accuracy: 0.9322 - val_loss: 0.3102 - val_accuracy: 0.9462\n",
      "iteration:  3\n",
      "Test loss: 0.310196936093889\n",
      "Test accuracy: 0.94618159532547\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3634 - accuracy: 0.9326 - val_loss: 0.3425 - val_accuracy: 0.9356\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3633 - accuracy: 0.9323 - val_loss: 0.3351 - val_accuracy: 0.9386\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3647 - accuracy: 0.9322 - val_loss: 0.3101 - val_accuracy: 0.9458\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3608 - accuracy: 0.9325 - val_loss: 0.3213 - val_accuracy: 0.9423\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3592 - accuracy: 0.9345 - val_loss: 0.3379 - val_accuracy: 0.9380\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3620 - accuracy: 0.9333 - val_loss: 0.3289 - val_accuracy: 0.9393\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3600 - accuracy: 0.9335 - val_loss: 0.3323 - val_accuracy: 0.9393\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3600 - accuracy: 0.9332 - val_loss: 0.3414 - val_accuracy: 0.9362\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3579 - accuracy: 0.9343 - val_loss: 0.3318 - val_accuracy: 0.9372\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3586 - accuracy: 0.9340 - val_loss: 0.3511 - val_accuracy: 0.9339\n",
      "iteration:  4\n",
      "Test loss: 0.35108413005541933\n",
      "Test accuracy: 0.9338890314102173\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3630 - accuracy: 0.9318 - val_loss: 0.3189 - val_accuracy: 0.9458\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3593 - accuracy: 0.9328 - val_loss: 0.3199 - val_accuracy: 0.9420\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3595 - accuracy: 0.9333 - val_loss: 0.3072 - val_accuracy: 0.9469\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3602 - accuracy: 0.9323 - val_loss: 0.3526 - val_accuracy: 0.9324\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3615 - accuracy: 0.9332 - val_loss: 0.3247 - val_accuracy: 0.9423\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3578 - accuracy: 0.9342 - val_loss: 0.3510 - val_accuracy: 0.9304\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3602 - accuracy: 0.9337 - val_loss: 0.3460 - val_accuracy: 0.9339\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3586 - accuracy: 0.9347 - val_loss: 0.3186 - val_accuracy: 0.9430\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3574 - accuracy: 0.9333 - val_loss: 0.3067 - val_accuracy: 0.9466\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3594 - accuracy: 0.9339 - val_loss: 0.3076 - val_accuracy: 0.9468\n",
      "iteration:  5\n",
      "Test loss: 0.30763251529966457\n",
      "Test accuracy: 0.9467578530311584\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3558 - accuracy: 0.9340 - val_loss: 0.3218 - val_accuracy: 0.9423\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3546 - accuracy: 0.9350 - val_loss: 0.3175 - val_accuracy: 0.9421\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3520 - accuracy: 0.9344 - val_loss: 0.3381 - val_accuracy: 0.9360\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3509 - accuracy: 0.9351 - val_loss: 0.3136 - val_accuracy: 0.9441\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3541 - accuracy: 0.9343 - val_loss: 0.3134 - val_accuracy: 0.9459\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3516 - accuracy: 0.9357 - val_loss: 0.3408 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3552 - accuracy: 0.9346 - val_loss: 0.3346 - val_accuracy: 0.9388\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3496 - accuracy: 0.9357 - val_loss: 0.3303 - val_accuracy: 0.9406\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3503 - accuracy: 0.9354 - val_loss: 0.3371 - val_accuracy: 0.9358\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3523 - accuracy: 0.9354 - val_loss: 0.3162 - val_accuracy: 0.9451\n",
      "iteration:  6\n",
      "Test loss: 0.31624857867943706\n",
      "Test accuracy: 0.945106029510498\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3564 - accuracy: 0.9348 - val_loss: 0.3254 - val_accuracy: 0.9411\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3547 - accuracy: 0.9351 - val_loss: 0.3167 - val_accuracy: 0.9442\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3541 - accuracy: 0.9347 - val_loss: 0.3397 - val_accuracy: 0.9355\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3556 - accuracy: 0.9346 - val_loss: 0.3312 - val_accuracy: 0.9398\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3511 - accuracy: 0.9362 - val_loss: 0.3156 - val_accuracy: 0.9439\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3550 - accuracy: 0.9351 - val_loss: 0.3475 - val_accuracy: 0.9365\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3507 - accuracy: 0.9354 - val_loss: 0.3267 - val_accuracy: 0.9398\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3498 - accuracy: 0.9352 - val_loss: 0.3264 - val_accuracy: 0.9417\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3518 - accuracy: 0.9361 - val_loss: 0.3163 - val_accuracy: 0.9463\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3513 - accuracy: 0.9353 - val_loss: 0.3088 - val_accuracy: 0.9468\n",
      "iteration:  7\n",
      "Test loss: 0.30884747502989907\n",
      "Test accuracy: 0.9468346834182739\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3508 - accuracy: 0.9358 - val_loss: 0.3223 - val_accuracy: 0.9421\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3511 - accuracy: 0.9349 - val_loss: 0.3676 - val_accuracy: 0.9290\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3511 - accuracy: 0.9354 - val_loss: 0.3585 - val_accuracy: 0.9273\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3514 - accuracy: 0.9352 - val_loss: 0.3406 - val_accuracy: 0.9358\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3506 - accuracy: 0.9355 - val_loss: 0.3198 - val_accuracy: 0.9428\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3520 - accuracy: 0.9355 - val_loss: 0.3121 - val_accuracy: 0.9450\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3531 - accuracy: 0.9344 - val_loss: 0.3048 - val_accuracy: 0.9458\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3468 - accuracy: 0.9371 - val_loss: 0.3092 - val_accuracy: 0.9459\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3545 - accuracy: 0.9342 - val_loss: 0.3336 - val_accuracy: 0.9363\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3485 - accuracy: 0.9364 - val_loss: 0.3082 - val_accuracy: 0.9487\n",
      "iteration:  8\n",
      "Test loss: 0.3081935183926455\n",
      "Test accuracy: 0.9486785531044006\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3454 - accuracy: 0.9368 - val_loss: 0.3480 - val_accuracy: 0.9339\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3488 - accuracy: 0.9367 - val_loss: 0.3260 - val_accuracy: 0.9410\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3457 - accuracy: 0.9371 - val_loss: 0.3229 - val_accuracy: 0.9453\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3454 - accuracy: 0.9370 - val_loss: 0.3349 - val_accuracy: 0.9388\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3499 - accuracy: 0.9358 - val_loss: 0.3178 - val_accuracy: 0.9447\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3507 - accuracy: 0.9364 - val_loss: 0.3104 - val_accuracy: 0.9444\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3464 - accuracy: 0.9358 - val_loss: 0.3133 - val_accuracy: 0.9434\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3463 - accuracy: 0.9361 - val_loss: 0.3294 - val_accuracy: 0.9397\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3480 - accuracy: 0.9352 - val_loss: 0.3360 - val_accuracy: 0.9371\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3439 - accuracy: 0.9367 - val_loss: 0.2946 - val_accuracy: 0.9503\n",
      "iteration:  9\n",
      "Test loss: 0.29460540338755387\n",
      "Test accuracy: 0.9502919316291809\n",
      "rate=0.2:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (12210, 10)\n",
      "Shape y_true (61047, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.6269 - accuracy: 0.8499 - val_loss: 0.3712 - val_accuracy: 0.9166\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4708 - accuracy: 0.8934 - val_loss: 0.3502 - val_accuracy: 0.9285\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.4456 - accuracy: 0.9049 - val_loss: 0.3597 - val_accuracy: 0.9270\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4357 - accuracy: 0.9092 - val_loss: 0.3356 - val_accuracy: 0.9362\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4263 - accuracy: 0.9136 - val_loss: 0.3658 - val_accuracy: 0.9301\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4230 - accuracy: 0.9149 - val_loss: 0.3480 - val_accuracy: 0.9343\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4184 - accuracy: 0.9179 - val_loss: 0.3373 - val_accuracy: 0.9390\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4109 - accuracy: 0.9204 - val_loss: 0.3670 - val_accuracy: 0.9271\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4053 - accuracy: 0.9224 - val_loss: 0.3443 - val_accuracy: 0.9370\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4050 - accuracy: 0.9216 - val_loss: 0.3392 - val_accuracy: 0.9408\n",
      "iteration:  0\n",
      "Test loss: 0.3391742631655642\n",
      "Test accuracy: 0.9408036470413208\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3993 - accuracy: 0.9225 - val_loss: 0.3461 - val_accuracy: 0.9359\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3983 - accuracy: 0.9242 - val_loss: 0.3391 - val_accuracy: 0.9370\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3931 - accuracy: 0.9247 - val_loss: 0.3484 - val_accuracy: 0.9361\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3897 - accuracy: 0.9252 - val_loss: 0.3391 - val_accuracy: 0.9363\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3878 - accuracy: 0.9259 - val_loss: 0.3308 - val_accuracy: 0.9396\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3884 - accuracy: 0.9265 - val_loss: 0.3573 - val_accuracy: 0.9336\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3833 - accuracy: 0.9277 - val_loss: 0.4071 - val_accuracy: 0.9188\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3824 - accuracy: 0.9257 - val_loss: 0.3758 - val_accuracy: 0.9301\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3776 - accuracy: 0.9279 - val_loss: 0.3392 - val_accuracy: 0.9369\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3823 - accuracy: 0.9280 - val_loss: 0.3193 - val_accuracy: 0.9457\n",
      "iteration:  1\n",
      "Test loss: 0.3193246843848554\n",
      "Test accuracy: 0.9457206726074219\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3793 - accuracy: 0.9288 - val_loss: 0.3418 - val_accuracy: 0.9393\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3768 - accuracy: 0.9291 - val_loss: 0.3205 - val_accuracy: 0.9431\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3741 - accuracy: 0.9302 - val_loss: 0.3177 - val_accuracy: 0.9444\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3751 - accuracy: 0.9291 - val_loss: 0.3619 - val_accuracy: 0.9337\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3718 - accuracy: 0.9297 - val_loss: 0.3878 - val_accuracy: 0.9268\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3686 - accuracy: 0.9299 - val_loss: 0.3104 - val_accuracy: 0.9465\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3718 - accuracy: 0.9307 - val_loss: 0.3389 - val_accuracy: 0.9384\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3681 - accuracy: 0.9319 - val_loss: 0.3460 - val_accuracy: 0.9359\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3667 - accuracy: 0.9320 - val_loss: 0.3413 - val_accuracy: 0.9363\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3665 - accuracy: 0.9317 - val_loss: 0.3180 - val_accuracy: 0.9445\n",
      "iteration:  2\n",
      "Test loss: 0.31802534326205756\n",
      "Test accuracy: 0.9444913864135742\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3679 - accuracy: 0.9314 - val_loss: 0.3412 - val_accuracy: 0.9389\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3663 - accuracy: 0.9315 - val_loss: 0.3283 - val_accuracy: 0.9407\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3692 - accuracy: 0.9307 - val_loss: 0.3229 - val_accuracy: 0.9424\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3662 - accuracy: 0.9319 - val_loss: 0.3367 - val_accuracy: 0.9379\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3668 - accuracy: 0.9310 - val_loss: 0.3276 - val_accuracy: 0.9431\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3612 - accuracy: 0.9332 - val_loss: 0.3701 - val_accuracy: 0.9285\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3650 - accuracy: 0.9322 - val_loss: 0.3070 - val_accuracy: 0.9476\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3628 - accuracy: 0.9320 - val_loss: 0.3068 - val_accuracy: 0.9480\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3619 - accuracy: 0.9322 - val_loss: 0.3332 - val_accuracy: 0.9389\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3626 - accuracy: 0.9326 - val_loss: 0.3412 - val_accuracy: 0.9373\n",
      "iteration:  3\n",
      "Test loss: 0.34118878256160934\n",
      "Test accuracy: 0.9373079538345337\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3655 - accuracy: 0.9317 - val_loss: 0.3329 - val_accuracy: 0.9405\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3662 - accuracy: 0.9319 - val_loss: 0.3370 - val_accuracy: 0.9392\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3622 - accuracy: 0.9329 - val_loss: 0.3290 - val_accuracy: 0.9402\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3596 - accuracy: 0.9327 - val_loss: 0.3320 - val_accuracy: 0.9398\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3617 - accuracy: 0.9319 - val_loss: 0.3644 - val_accuracy: 0.9270\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3570 - accuracy: 0.9330 - val_loss: 0.3261 - val_accuracy: 0.9421\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3580 - accuracy: 0.9323 - val_loss: 0.3429 - val_accuracy: 0.9369\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3595 - accuracy: 0.9328 - val_loss: 0.3096 - val_accuracy: 0.9467\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3582 - accuracy: 0.9332 - val_loss: 0.3263 - val_accuracy: 0.9416\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3585 - accuracy: 0.9326 - val_loss: 0.3284 - val_accuracy: 0.9383\n",
      "iteration:  4\n",
      "Test loss: 0.328414420326502\n",
      "Test accuracy: 0.9383066892623901\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3628 - accuracy: 0.9336 - val_loss: 0.3271 - val_accuracy: 0.9378\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3576 - accuracy: 0.9347 - val_loss: 0.3420 - val_accuracy: 0.9377\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3568 - accuracy: 0.9339 - val_loss: 0.3348 - val_accuracy: 0.9387\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3577 - accuracy: 0.9322 - val_loss: 0.3367 - val_accuracy: 0.9350\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3574 - accuracy: 0.9327 - val_loss: 0.3179 - val_accuracy: 0.9408\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3578 - accuracy: 0.9329 - val_loss: 0.3398 - val_accuracy: 0.9364\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3575 - accuracy: 0.9340 - val_loss: 0.3415 - val_accuracy: 0.9379\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3576 - accuracy: 0.9328 - val_loss: 0.3456 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 47ms/step - loss: 0.3548 - accuracy: 0.9343 - val_loss: 0.3152 - val_accuracy: 0.9447\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3534 - accuracy: 0.9341 - val_loss: 0.3200 - val_accuracy: 0.9457\n",
      "iteration:  5\n",
      "Test loss: 0.3199808571737239\n",
      "Test accuracy: 0.9457206726074219\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3596 - accuracy: 0.9336 - val_loss: 0.3343 - val_accuracy: 0.9396\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3613 - accuracy: 0.9332 - val_loss: 0.3279 - val_accuracy: 0.9399\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3562 - accuracy: 0.9336 - val_loss: 0.3223 - val_accuracy: 0.9412\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3581 - accuracy: 0.9333 - val_loss: 0.3314 - val_accuracy: 0.9398\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3545 - accuracy: 0.9338 - val_loss: 0.3247 - val_accuracy: 0.9398\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3570 - accuracy: 0.9332 - val_loss: 0.3288 - val_accuracy: 0.9416\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3561 - accuracy: 0.9342 - val_loss: 0.3086 - val_accuracy: 0.9453\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3542 - accuracy: 0.9335 - val_loss: 0.3466 - val_accuracy: 0.9362\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3483 - accuracy: 0.9356 - val_loss: 0.3234 - val_accuracy: 0.9421\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3523 - accuracy: 0.9349 - val_loss: 0.3400 - val_accuracy: 0.9390\n",
      "iteration:  6\n",
      "Test loss: 0.3399587483926646\n",
      "Test accuracy: 0.9390365481376648\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3510 - accuracy: 0.9344 - val_loss: 0.3131 - val_accuracy: 0.9450\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3520 - accuracy: 0.9341 - val_loss: 0.3236 - val_accuracy: 0.9408\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3538 - accuracy: 0.9339 - val_loss: 0.3391 - val_accuracy: 0.9360\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3525 - accuracy: 0.9343 - val_loss: 0.3183 - val_accuracy: 0.9435\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3510 - accuracy: 0.9344 - val_loss: 0.3244 - val_accuracy: 0.9410\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3542 - accuracy: 0.9341 - val_loss: 0.3476 - val_accuracy: 0.9337\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3507 - accuracy: 0.9341 - val_loss: 0.3259 - val_accuracy: 0.9389\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3526 - accuracy: 0.9340 - val_loss: 0.3243 - val_accuracy: 0.9427\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3465 - accuracy: 0.9341 - val_loss: 0.3406 - val_accuracy: 0.9383\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3517 - accuracy: 0.9347 - val_loss: 0.3114 - val_accuracy: 0.9436\n",
      "iteration:  7\n",
      "Test loss: 0.3113936660988754\n",
      "Test accuracy: 0.9436078667640686\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3506 - accuracy: 0.9339 - val_loss: 0.3399 - val_accuracy: 0.9349\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3481 - accuracy: 0.9351 - val_loss: 0.3176 - val_accuracy: 0.9432\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3478 - accuracy: 0.9348 - val_loss: 0.3202 - val_accuracy: 0.9418\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3474 - accuracy: 0.9355 - val_loss: 0.3396 - val_accuracy: 0.9352\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3483 - accuracy: 0.9354 - val_loss: 0.3231 - val_accuracy: 0.9405\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3469 - accuracy: 0.9358 - val_loss: 0.3330 - val_accuracy: 0.9398\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3447 - accuracy: 0.9353 - val_loss: 0.3058 - val_accuracy: 0.9453\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3456 - accuracy: 0.9351 - val_loss: 0.3060 - val_accuracy: 0.9476\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3478 - accuracy: 0.9353 - val_loss: 0.3476 - val_accuracy: 0.9327\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3471 - accuracy: 0.9359 - val_loss: 0.3423 - val_accuracy: 0.9357\n",
      "iteration:  8\n",
      "Test loss: 0.34226566224319477\n",
      "Test accuracy: 0.9356561303138733\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3539 - accuracy: 0.9349 - val_loss: 0.3269 - val_accuracy: 0.9418\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3542 - accuracy: 0.9332 - val_loss: 0.3094 - val_accuracy: 0.9445\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3530 - accuracy: 0.9342 - val_loss: 0.3207 - val_accuracy: 0.9422\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3508 - accuracy: 0.9346 - val_loss: 0.3044 - val_accuracy: 0.9480\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3509 - accuracy: 0.9338 - val_loss: 0.3215 - val_accuracy: 0.9422\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3499 - accuracy: 0.9336 - val_loss: 0.3599 - val_accuracy: 0.9292\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3497 - accuracy: 0.9349 - val_loss: 0.3120 - val_accuracy: 0.9455\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3481 - accuracy: 0.9349 - val_loss: 0.3272 - val_accuracy: 0.9392\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3469 - accuracy: 0.9355 - val_loss: 0.3319 - val_accuracy: 0.9404\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3493 - accuracy: 0.9338 - val_loss: 0.3117 - val_accuracy: 0.9435\n",
      "iteration:  9\n",
      "Test loss: 0.3117481545209152\n",
      "Test accuracy: 0.9434542059898376\n",
      "rate=0.5:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (24419, 10)\n",
      "Shape y_true (48838, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.6269 - accuracy: 0.8481 - val_loss: 0.4113 - val_accuracy: 0.9015\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4717 - accuracy: 0.8941 - val_loss: 0.3349 - val_accuracy: 0.9338\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4476 - accuracy: 0.9034 - val_loss: 0.3768 - val_accuracy: 0.9168\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.4363 - accuracy: 0.9098 - val_loss: 0.3472 - val_accuracy: 0.9333\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4263 - accuracy: 0.9132 - val_loss: 0.3597 - val_accuracy: 0.9302\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4230 - accuracy: 0.9160 - val_loss: 0.3327 - val_accuracy: 0.9355\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4169 - accuracy: 0.9178 - val_loss: 0.3673 - val_accuracy: 0.9283\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4129 - accuracy: 0.9185 - val_loss: 0.3322 - val_accuracy: 0.9382\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4095 - accuracy: 0.9197 - val_loss: 0.3790 - val_accuracy: 0.9263\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4048 - accuracy: 0.9229 - val_loss: 0.3432 - val_accuracy: 0.9387\n",
      "iteration:  0\n",
      "Test loss: 0.3431677465597837\n",
      "Test accuracy: 0.9387292265892029\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3963 - accuracy: 0.9228 - val_loss: 0.3425 - val_accuracy: 0.9390\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3946 - accuracy: 0.9244 - val_loss: 0.3614 - val_accuracy: 0.9301\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3897 - accuracy: 0.9250 - val_loss: 0.3424 - val_accuracy: 0.9365\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3913 - accuracy: 0.9243 - val_loss: 0.3431 - val_accuracy: 0.9354\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3895 - accuracy: 0.9241 - val_loss: 0.3407 - val_accuracy: 0.9359\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3856 - accuracy: 0.9252 - val_loss: 0.3509 - val_accuracy: 0.9330\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3823 - accuracy: 0.9259 - val_loss: 0.3562 - val_accuracy: 0.9329\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3811 - accuracy: 0.9261 - val_loss: 0.3303 - val_accuracy: 0.9396\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3821 - accuracy: 0.9255 - val_loss: 0.3343 - val_accuracy: 0.9410\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3811 - accuracy: 0.9266 - val_loss: 0.3394 - val_accuracy: 0.9375\n",
      "iteration:  1\n",
      "Test loss: 0.3394218923566381\n",
      "Test accuracy: 0.9374616146087646\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3752 - accuracy: 0.9283 - val_loss: 0.3425 - val_accuracy: 0.9361\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3725 - accuracy: 0.9287 - val_loss: 0.3492 - val_accuracy: 0.9363\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3733 - accuracy: 0.9277 - val_loss: 0.3269 - val_accuracy: 0.9415\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3739 - accuracy: 0.9283 - val_loss: 0.3295 - val_accuracy: 0.9408\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3701 - accuracy: 0.9291 - val_loss: 0.3395 - val_accuracy: 0.9377\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3646 - accuracy: 0.9303 - val_loss: 0.3332 - val_accuracy: 0.9393\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3674 - accuracy: 0.9302 - val_loss: 0.3210 - val_accuracy: 0.9420\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3646 - accuracy: 0.9299 - val_loss: 0.3339 - val_accuracy: 0.9376\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3605 - accuracy: 0.9317 - val_loss: 0.3288 - val_accuracy: 0.9393\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3614 - accuracy: 0.9302 - val_loss: 0.3284 - val_accuracy: 0.9405\n",
      "iteration:  2\n",
      "Test loss: 0.3283624987781378\n",
      "Test accuracy: 0.9405347108840942\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3650 - accuracy: 0.9286 - val_loss: 0.3488 - val_accuracy: 0.9335\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3592 - accuracy: 0.9308 - val_loss: 0.3688 - val_accuracy: 0.9284\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3641 - accuracy: 0.9308 - val_loss: 0.3399 - val_accuracy: 0.9336\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3630 - accuracy: 0.9304 - val_loss: 0.3655 - val_accuracy: 0.9271\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3558 - accuracy: 0.9321 - val_loss: 0.3195 - val_accuracy: 0.9439\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3584 - accuracy: 0.9313 - val_loss: 0.3379 - val_accuracy: 0.9364\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3593 - accuracy: 0.9308 - val_loss: 0.3407 - val_accuracy: 0.9369\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3546 - accuracy: 0.9326 - val_loss: 0.3560 - val_accuracy: 0.9310\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3538 - accuracy: 0.9319 - val_loss: 0.3645 - val_accuracy: 0.9307\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3551 - accuracy: 0.9322 - val_loss: 0.3357 - val_accuracy: 0.9360\n",
      "iteration:  3\n",
      "Test loss: 0.3356626980377621\n",
      "Test accuracy: 0.9360018372535706\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3652 - accuracy: 0.9285 - val_loss: 0.3507 - val_accuracy: 0.9329\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3640 - accuracy: 0.9300 - val_loss: 0.3364 - val_accuracy: 0.9386\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3620 - accuracy: 0.9293 - val_loss: 0.3442 - val_accuracy: 0.9341\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3608 - accuracy: 0.9305 - val_loss: 0.3287 - val_accuracy: 0.9388\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3632 - accuracy: 0.9299 - val_loss: 0.3389 - val_accuracy: 0.9379\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3620 - accuracy: 0.9304 - val_loss: 0.3366 - val_accuracy: 0.9356\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3607 - accuracy: 0.9311 - val_loss: 0.3388 - val_accuracy: 0.9365\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3579 - accuracy: 0.9304 - val_loss: 0.3314 - val_accuracy: 0.9377\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3584 - accuracy: 0.9306 - val_loss: 0.3261 - val_accuracy: 0.9406\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3572 - accuracy: 0.9306 - val_loss: 0.3258 - val_accuracy: 0.9408\n",
      "iteration:  4\n",
      "Test loss: 0.3257661010582314\n",
      "Test accuracy: 0.9407652020454407\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3539 - accuracy: 0.9315 - val_loss: 0.3423 - val_accuracy: 0.9352\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3522 - accuracy: 0.9318 - val_loss: 0.3494 - val_accuracy: 0.9315\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3487 - accuracy: 0.9326 - val_loss: 0.3246 - val_accuracy: 0.9418\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3515 - accuracy: 0.9319 - val_loss: 0.3211 - val_accuracy: 0.9416\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3493 - accuracy: 0.9325 - val_loss: 0.3265 - val_accuracy: 0.9410\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3500 - accuracy: 0.9319 - val_loss: 0.3361 - val_accuracy: 0.9364\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3471 - accuracy: 0.9336 - val_loss: 0.3451 - val_accuracy: 0.9337\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3499 - accuracy: 0.9339 - val_loss: 0.3244 - val_accuracy: 0.9411\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3481 - accuracy: 0.9336 - val_loss: 0.3229 - val_accuracy: 0.9406\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3483 - accuracy: 0.9326 - val_loss: 0.3421 - val_accuracy: 0.9351\n",
      "iteration:  5\n",
      "Test loss: 0.3420533377038385\n",
      "Test accuracy: 0.9351183176040649\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3533 - accuracy: 0.9314 - val_loss: 0.3492 - val_accuracy: 0.9310\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3525 - accuracy: 0.9312 - val_loss: 0.3235 - val_accuracy: 0.9412\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3515 - accuracy: 0.9320 - val_loss: 0.3152 - val_accuracy: 0.9413\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3499 - accuracy: 0.9320 - val_loss: 0.3329 - val_accuracy: 0.9377\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3516 - accuracy: 0.9316 - val_loss: 0.3423 - val_accuracy: 0.9360\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3498 - accuracy: 0.9332 - val_loss: 0.3215 - val_accuracy: 0.9413\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3501 - accuracy: 0.9323 - val_loss: 0.3412 - val_accuracy: 0.9355\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3511 - accuracy: 0.9318 - val_loss: 0.3264 - val_accuracy: 0.9371\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3506 - accuracy: 0.9332 - val_loss: 0.3411 - val_accuracy: 0.9330\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3484 - accuracy: 0.9335 - val_loss: 0.3406 - val_accuracy: 0.9359\n",
      "iteration:  6\n",
      "Test loss: 0.3406272256528225\n",
      "Test accuracy: 0.9359250068664551\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3521 - accuracy: 0.9315 - val_loss: 0.3041 - val_accuracy: 0.9469\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3468 - accuracy: 0.9328 - val_loss: 0.3373 - val_accuracy: 0.9359\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3462 - accuracy: 0.9324 - val_loss: 0.3236 - val_accuracy: 0.9377\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3495 - accuracy: 0.9327 - val_loss: 0.3558 - val_accuracy: 0.9311\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3489 - accuracy: 0.9327 - val_loss: 0.3626 - val_accuracy: 0.9267\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3452 - accuracy: 0.9335 - val_loss: 0.3350 - val_accuracy: 0.9380\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3452 - accuracy: 0.9335 - val_loss: 0.3205 - val_accuracy: 0.9398\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3453 - accuracy: 0.9339 - val_loss: 0.3485 - val_accuracy: 0.9313\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3413 - accuracy: 0.9342 - val_loss: 0.3364 - val_accuracy: 0.9351\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3461 - accuracy: 0.9338 - val_loss: 0.3248 - val_accuracy: 0.9398\n",
      "iteration:  7\n",
      "Test loss: 0.3247871172889728\n",
      "Test accuracy: 0.9397664666175842\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3520 - accuracy: 0.9317 - val_loss: 0.3213 - val_accuracy: 0.9397\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3453 - accuracy: 0.9347 - val_loss: 0.3187 - val_accuracy: 0.9415\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3421 - accuracy: 0.9331 - val_loss: 0.3327 - val_accuracy: 0.9390\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3432 - accuracy: 0.9336 - val_loss: 0.3496 - val_accuracy: 0.9284\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3437 - accuracy: 0.9333 - val_loss: 0.3302 - val_accuracy: 0.9366\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3459 - accuracy: 0.9321 - val_loss: 0.3106 - val_accuracy: 0.9451\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3395 - accuracy: 0.9342 - val_loss: 0.3295 - val_accuracy: 0.9370\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3459 - accuracy: 0.9332 - val_loss: 0.3368 - val_accuracy: 0.9364\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3405 - accuracy: 0.9340 - val_loss: 0.3475 - val_accuracy: 0.9322\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3405 - accuracy: 0.9345 - val_loss: 0.3278 - val_accuracy: 0.9393\n",
      "iteration:  8\n",
      "Test loss: 0.3278028026461308\n",
      "Test accuracy: 0.9392670392990112\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3436 - accuracy: 0.9337 - val_loss: 0.3340 - val_accuracy: 0.9360\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3404 - accuracy: 0.9349 - val_loss: 0.3280 - val_accuracy: 0.9367\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3435 - accuracy: 0.9337 - val_loss: 0.3749 - val_accuracy: 0.9256\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3387 - accuracy: 0.9344 - val_loss: 0.3261 - val_accuracy: 0.9391\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3384 - accuracy: 0.9354 - val_loss: 0.3108 - val_accuracy: 0.9418\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3394 - accuracy: 0.9344 - val_loss: 0.3111 - val_accuracy: 0.9438\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3395 - accuracy: 0.9335 - val_loss: 0.3110 - val_accuracy: 0.9422\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3369 - accuracy: 0.9359 - val_loss: 0.3297 - val_accuracy: 0.9378\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3403 - accuracy: 0.9344 - val_loss: 0.3448 - val_accuracy: 0.9345\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3360 - accuracy: 0.9358 - val_loss: 0.3402 - val_accuracy: 0.9345\n",
      "iteration:  9\n",
      "Test loss: 0.34015004186631714\n",
      "Test accuracy: 0.9344652891159058\n",
      "rate=0.75:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (31396, 10)\n",
      "Shape y_true (41861, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.6349 - accuracy: 0.8473 - val_loss: 0.3741 - val_accuracy: 0.9171\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4752 - accuracy: 0.8927 - val_loss: 0.3485 - val_accuracy: 0.9267\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4508 - accuracy: 0.9023 - val_loss: 0.3770 - val_accuracy: 0.9234\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.4390 - accuracy: 0.9082 - val_loss: 0.3827 - val_accuracy: 0.9201\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4281 - accuracy: 0.9119 - val_loss: 0.3318 - val_accuracy: 0.9378\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4212 - accuracy: 0.9140 - val_loss: 0.3703 - val_accuracy: 0.9249\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4162 - accuracy: 0.9168 - val_loss: 0.3465 - val_accuracy: 0.9360\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4086 - accuracy: 0.9197 - val_loss: 0.3605 - val_accuracy: 0.9320\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4049 - accuracy: 0.9212 - val_loss: 0.3670 - val_accuracy: 0.9315\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4049 - accuracy: 0.9215 - val_loss: 0.3452 - val_accuracy: 0.9354\n",
      "iteration:  0\n",
      "Test loss: 0.3452004278336263\n",
      "Test accuracy: 0.9354256391525269\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3946 - accuracy: 0.9225 - val_loss: 0.3630 - val_accuracy: 0.9280\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3847 - accuracy: 0.9237 - val_loss: 0.3504 - val_accuracy: 0.9339\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3859 - accuracy: 0.9231 - val_loss: 0.3399 - val_accuracy: 0.9380\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3838 - accuracy: 0.9252 - val_loss: 0.3934 - val_accuracy: 0.9215\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3832 - accuracy: 0.9251 - val_loss: 0.3642 - val_accuracy: 0.9295\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3806 - accuracy: 0.9249 - val_loss: 0.3440 - val_accuracy: 0.9366\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3775 - accuracy: 0.9247 - val_loss: 0.3764 - val_accuracy: 0.9252\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3740 - accuracy: 0.9272 - val_loss: 0.3370 - val_accuracy: 0.9402\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3739 - accuracy: 0.9269 - val_loss: 0.3545 - val_accuracy: 0.9340\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3753 - accuracy: 0.9264 - val_loss: 0.3265 - val_accuracy: 0.9426\n",
      "iteration:  1\n",
      "Test loss: 0.3264965460244761\n",
      "Test accuracy: 0.9426090717315674\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3726 - accuracy: 0.9271 - val_loss: 0.3268 - val_accuracy: 0.9414\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3698 - accuracy: 0.9272 - val_loss: 0.3486 - val_accuracy: 0.9346\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3666 - accuracy: 0.9287 - val_loss: 0.3403 - val_accuracy: 0.9373\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3658 - accuracy: 0.9289 - val_loss: 0.3538 - val_accuracy: 0.9345\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3637 - accuracy: 0.9285 - val_loss: 0.3322 - val_accuracy: 0.9385\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3621 - accuracy: 0.9302 - val_loss: 0.3786 - val_accuracy: 0.9277\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3571 - accuracy: 0.9305 - val_loss: 0.3504 - val_accuracy: 0.9373\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3581 - accuracy: 0.9312 - val_loss: 0.3568 - val_accuracy: 0.9310\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3571 - accuracy: 0.9315 - val_loss: 0.3424 - val_accuracy: 0.9377\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3562 - accuracy: 0.9309 - val_loss: 0.3237 - val_accuracy: 0.9403\n",
      "iteration:  2\n",
      "Test loss: 0.3237039011610602\n",
      "Test accuracy: 0.9403426647186279\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3656 - accuracy: 0.9292 - val_loss: 0.3420 - val_accuracy: 0.9358\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3612 - accuracy: 0.9296 - val_loss: 0.3293 - val_accuracy: 0.9378\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3582 - accuracy: 0.9295 - val_loss: 0.3313 - val_accuracy: 0.9405\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3600 - accuracy: 0.9295 - val_loss: 0.3598 - val_accuracy: 0.9289\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3542 - accuracy: 0.9311 - val_loss: 0.3613 - val_accuracy: 0.9274\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3559 - accuracy: 0.9310 - val_loss: 0.3507 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3559 - accuracy: 0.9306 - val_loss: 0.3353 - val_accuracy: 0.9383\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3560 - accuracy: 0.9316 - val_loss: 0.3132 - val_accuracy: 0.9429\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3516 - accuracy: 0.9309 - val_loss: 0.3271 - val_accuracy: 0.9406\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3523 - accuracy: 0.9319 - val_loss: 0.3418 - val_accuracy: 0.9368\n",
      "iteration:  3\n",
      "Test loss: 0.3417660838019357\n",
      "Test accuracy: 0.9368085265159607\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3586 - accuracy: 0.9296 - val_loss: 0.3295 - val_accuracy: 0.9378\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3618 - accuracy: 0.9285 - val_loss: 0.3320 - val_accuracy: 0.9391\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3538 - accuracy: 0.9306 - val_loss: 0.3307 - val_accuracy: 0.9382\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3565 - accuracy: 0.9300 - val_loss: 0.3435 - val_accuracy: 0.9347\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3537 - accuracy: 0.9302 - val_loss: 0.3188 - val_accuracy: 0.9411\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3550 - accuracy: 0.9299 - val_loss: 0.3392 - val_accuracy: 0.9370\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3556 - accuracy: 0.9305 - val_loss: 0.3363 - val_accuracy: 0.9347\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3525 - accuracy: 0.9307 - val_loss: 0.3365 - val_accuracy: 0.9359\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3526 - accuracy: 0.9304 - val_loss: 0.3611 - val_accuracy: 0.9301\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3506 - accuracy: 0.9310 - val_loss: 0.3614 - val_accuracy: 0.9258\n",
      "iteration:  4\n",
      "Test loss: 0.3614185792156429\n",
      "Test accuracy: 0.9258220791816711\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3630 - accuracy: 0.9280 - val_loss: 0.3707 - val_accuracy: 0.9252\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3628 - accuracy: 0.9273 - val_loss: 0.3446 - val_accuracy: 0.9311\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3620 - accuracy: 0.9269 - val_loss: 0.3262 - val_accuracy: 0.9390\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3634 - accuracy: 0.9276 - val_loss: 0.3345 - val_accuracy: 0.9352\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3579 - accuracy: 0.9285 - val_loss: 0.3657 - val_accuracy: 0.9264\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3581 - accuracy: 0.9274 - val_loss: 0.3331 - val_accuracy: 0.9374\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3601 - accuracy: 0.9272 - val_loss: 0.3764 - val_accuracy: 0.9211\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3546 - accuracy: 0.9276 - val_loss: 0.3617 - val_accuracy: 0.9274\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3558 - accuracy: 0.9287 - val_loss: 0.3403 - val_accuracy: 0.9340\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3552 - accuracy: 0.9288 - val_loss: 0.3443 - val_accuracy: 0.9347\n",
      "iteration:  5\n",
      "Test loss: 0.34425365692204907\n",
      "Test accuracy: 0.9347341656684875\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3566 - accuracy: 0.9286 - val_loss: 0.3608 - val_accuracy: 0.9285\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3546 - accuracy: 0.9291 - val_loss: 0.3413 - val_accuracy: 0.9333\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3522 - accuracy: 0.9290 - val_loss: 0.3767 - val_accuracy: 0.9219\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3528 - accuracy: 0.9297 - val_loss: 0.3224 - val_accuracy: 0.9401\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3535 - accuracy: 0.9295 - val_loss: 0.3700 - val_accuracy: 0.9271\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3509 - accuracy: 0.9304 - val_loss: 0.3372 - val_accuracy: 0.9332\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3495 - accuracy: 0.9287 - val_loss: 0.3121 - val_accuracy: 0.9430\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3502 - accuracy: 0.9299 - val_loss: 0.3367 - val_accuracy: 0.9344\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3487 - accuracy: 0.9301 - val_loss: 0.3293 - val_accuracy: 0.9363\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3511 - accuracy: 0.9293 - val_loss: 0.3463 - val_accuracy: 0.9324\n",
      "iteration:  6\n",
      "Test loss: 0.3463465502347108\n",
      "Test accuracy: 0.9323909282684326\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3559 - accuracy: 0.9296 - val_loss: 0.3311 - val_accuracy: 0.9374\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 47ms/step - loss: 0.3519 - accuracy: 0.9302 - val_loss: 0.3345 - val_accuracy: 0.9362\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3503 - accuracy: 0.9302 - val_loss: 0.3191 - val_accuracy: 0.9394\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3506 - accuracy: 0.9299 - val_loss: 0.3209 - val_accuracy: 0.9393\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3498 - accuracy: 0.9306 - val_loss: 0.3616 - val_accuracy: 0.9288\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3491 - accuracy: 0.9312 - val_loss: 0.3690 - val_accuracy: 0.9277\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3445 - accuracy: 0.9312 - val_loss: 0.3271 - val_accuracy: 0.9363\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3461 - accuracy: 0.9309 - val_loss: 0.3262 - val_accuracy: 0.9383\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3467 - accuracy: 0.9311 - val_loss: 0.3563 - val_accuracy: 0.9288\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3467 - accuracy: 0.9301 - val_loss: 0.3358 - val_accuracy: 0.9357\n",
      "iteration:  7\n",
      "Test loss: 0.3358420468660755\n",
      "Test accuracy: 0.9356561303138733\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3481 - accuracy: 0.9304 - val_loss: 0.3415 - val_accuracy: 0.9330\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3470 - accuracy: 0.9314 - val_loss: 0.3316 - val_accuracy: 0.9365\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3494 - accuracy: 0.9302 - val_loss: 0.3334 - val_accuracy: 0.9341\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3415 - accuracy: 0.9325 - val_loss: 0.3389 - val_accuracy: 0.9338\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3472 - accuracy: 0.9312 - val_loss: 0.3388 - val_accuracy: 0.9362\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3439 - accuracy: 0.9325 - val_loss: 0.3425 - val_accuracy: 0.9309\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3436 - accuracy: 0.9320 - val_loss: 0.3618 - val_accuracy: 0.9309\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3407 - accuracy: 0.9324 - val_loss: 0.3697 - val_accuracy: 0.9262\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3419 - accuracy: 0.9334 - val_loss: 0.3533 - val_accuracy: 0.9284\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3442 - accuracy: 0.9315 - val_loss: 0.3509 - val_accuracy: 0.9294\n",
      "iteration:  8\n",
      "Test loss: 0.35086284442298166\n",
      "Test accuracy: 0.9294329881668091\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3519 - accuracy: 0.9295 - val_loss: 0.3551 - val_accuracy: 0.9297\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3490 - accuracy: 0.9298 - val_loss: 0.3157 - val_accuracy: 0.9415\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3443 - accuracy: 0.9297 - val_loss: 0.3234 - val_accuracy: 0.9377\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3480 - accuracy: 0.9291 - val_loss: 0.3138 - val_accuracy: 0.9434\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3449 - accuracy: 0.9305 - val_loss: 0.3417 - val_accuracy: 0.9322\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3453 - accuracy: 0.9293 - val_loss: 0.3327 - val_accuracy: 0.9378\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3434 - accuracy: 0.9309 - val_loss: 0.3468 - val_accuracy: 0.9322\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3433 - accuracy: 0.9316 - val_loss: 0.3309 - val_accuracy: 0.9346\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3453 - accuracy: 0.9312 - val_loss: 0.3765 - val_accuracy: 0.9218\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3417 - accuracy: 0.9308 - val_loss: 0.3383 - val_accuracy: 0.9314\n",
      "iteration:  9\n",
      "Test loss: 0.33832666146000545\n",
      "Test accuracy: 0.9314305186271667\n",
      "rate=1.0:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (36629, 10)\n",
      "Shape y_true (36628, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.6303 - accuracy: 0.8492 - val_loss: 0.3868 - val_accuracy: 0.9166\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4748 - accuracy: 0.8924 - val_loss: 0.3504 - val_accuracy: 0.9287\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.4512 - accuracy: 0.9035 - val_loss: 0.3628 - val_accuracy: 0.9263\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4357 - accuracy: 0.9086 - val_loss: 0.3427 - val_accuracy: 0.9324\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4276 - accuracy: 0.9114 - val_loss: 0.3679 - val_accuracy: 0.9294\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4224 - accuracy: 0.9149 - val_loss: 0.3810 - val_accuracy: 0.9232\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4174 - accuracy: 0.9174 - val_loss: 0.3443 - val_accuracy: 0.9352\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4127 - accuracy: 0.9196 - val_loss: 0.3627 - val_accuracy: 0.9291\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.4096 - accuracy: 0.9204 - val_loss: 0.3425 - val_accuracy: 0.9374\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4017 - accuracy: 0.9228 - val_loss: 0.3560 - val_accuracy: 0.9337\n",
      "iteration:  0\n",
      "Test loss: 0.3560249056683117\n",
      "Test accuracy: 0.9336585998535156\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3946 - accuracy: 0.9228 - val_loss: 0.3572 - val_accuracy: 0.9322\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3902 - accuracy: 0.9235 - val_loss: 0.3527 - val_accuracy: 0.9325\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3865 - accuracy: 0.9240 - val_loss: 0.3562 - val_accuracy: 0.9344\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3825 - accuracy: 0.9254 - val_loss: 0.3599 - val_accuracy: 0.9309\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3793 - accuracy: 0.9244 - val_loss: 0.3784 - val_accuracy: 0.9251\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3776 - accuracy: 0.9258 - val_loss: 0.3348 - val_accuracy: 0.9391\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3782 - accuracy: 0.9263 - val_loss: 0.3542 - val_accuracy: 0.9320\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3732 - accuracy: 0.9263 - val_loss: 0.3391 - val_accuracy: 0.9387\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3724 - accuracy: 0.9263 - val_loss: 0.3477 - val_accuracy: 0.9369\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3729 - accuracy: 0.9282 - val_loss: 0.3485 - val_accuracy: 0.9363\n",
      "iteration:  1\n",
      "Test loss: 0.3485034625672752\n",
      "Test accuracy: 0.9363475441932678\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3718 - accuracy: 0.9275 - val_loss: 0.3415 - val_accuracy: 0.9373\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3693 - accuracy: 0.9273 - val_loss: 0.3394 - val_accuracy: 0.9403\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3655 - accuracy: 0.9286 - val_loss: 0.3369 - val_accuracy: 0.9420\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3614 - accuracy: 0.9298 - val_loss: 0.3788 - val_accuracy: 0.9294\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3601 - accuracy: 0.9282 - val_loss: 0.3571 - val_accuracy: 0.9346\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3602 - accuracy: 0.9282 - val_loss: 0.3331 - val_accuracy: 0.9402\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3598 - accuracy: 0.9297 - val_loss: 0.3574 - val_accuracy: 0.9318\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3595 - accuracy: 0.9285 - val_loss: 0.3810 - val_accuracy: 0.9259\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3559 - accuracy: 0.9300 - val_loss: 0.3404 - val_accuracy: 0.9368\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3582 - accuracy: 0.9296 - val_loss: 0.3518 - val_accuracy: 0.9343\n",
      "iteration:  2\n",
      "Test loss: 0.3518180815021026\n",
      "Test accuracy: 0.9342731833457947\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3757 - accuracy: 0.9254 - val_loss: 0.3507 - val_accuracy: 0.9346\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3704 - accuracy: 0.9266 - val_loss: 0.3425 - val_accuracy: 0.9349\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3717 - accuracy: 0.9265 - val_loss: 0.3669 - val_accuracy: 0.9311\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3708 - accuracy: 0.9250 - val_loss: 0.3855 - val_accuracy: 0.9249\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3654 - accuracy: 0.9277 - val_loss: 0.3463 - val_accuracy: 0.9346\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3686 - accuracy: 0.9260 - val_loss: 0.3416 - val_accuracy: 0.9346\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3643 - accuracy: 0.9262 - val_loss: 0.3288 - val_accuracy: 0.9389\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3639 - accuracy: 0.9269 - val_loss: 0.3526 - val_accuracy: 0.9322\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3640 - accuracy: 0.9275 - val_loss: 0.3425 - val_accuracy: 0.9365\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3616 - accuracy: 0.9279 - val_loss: 0.3638 - val_accuracy: 0.9303\n",
      "iteration:  3\n",
      "Test loss: 0.3638234978705443\n",
      "Test accuracy: 0.9302781224250793\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3640 - accuracy: 0.9271 - val_loss: 0.3561 - val_accuracy: 0.9322\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3614 - accuracy: 0.9271 - val_loss: 0.3550 - val_accuracy: 0.9322\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3649 - accuracy: 0.9273 - val_loss: 0.3624 - val_accuracy: 0.9296\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3621 - accuracy: 0.9265 - val_loss: 0.3346 - val_accuracy: 0.9393\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3566 - accuracy: 0.9292 - val_loss: 0.3658 - val_accuracy: 0.9281\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3568 - accuracy: 0.9288 - val_loss: 0.3658 - val_accuracy: 0.9293\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3572 - accuracy: 0.9286 - val_loss: 0.3255 - val_accuracy: 0.9420\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3549 - accuracy: 0.9284 - val_loss: 0.3659 - val_accuracy: 0.9285\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3552 - accuracy: 0.9293 - val_loss: 0.4058 - val_accuracy: 0.9193\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3568 - accuracy: 0.9290 - val_loss: 0.3650 - val_accuracy: 0.9281\n",
      "iteration:  4\n",
      "Test loss: 0.3649511203838406\n",
      "Test accuracy: 0.9280501008033752\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3712 - accuracy: 0.9245 - val_loss: 0.3424 - val_accuracy: 0.9346\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3659 - accuracy: 0.9261 - val_loss: 0.3399 - val_accuracy: 0.9357\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3647 - accuracy: 0.9266 - val_loss: 0.3646 - val_accuracy: 0.9281\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3670 - accuracy: 0.9255 - val_loss: 0.3363 - val_accuracy: 0.9381\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3632 - accuracy: 0.9251 - val_loss: 0.3541 - val_accuracy: 0.9310\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3624 - accuracy: 0.9263 - val_loss: 0.3391 - val_accuracy: 0.9356\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3628 - accuracy: 0.9266 - val_loss: 0.3524 - val_accuracy: 0.9300\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3610 - accuracy: 0.9274 - val_loss: 0.3675 - val_accuracy: 0.9274\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3626 - accuracy: 0.9263 - val_loss: 0.3553 - val_accuracy: 0.9327\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3595 - accuracy: 0.9271 - val_loss: 0.3344 - val_accuracy: 0.9372\n",
      "iteration:  5\n",
      "Test loss: 0.33435319293091176\n",
      "Test accuracy: 0.9371926784515381\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3502 - accuracy: 0.9287 - val_loss: 0.3357 - val_accuracy: 0.9350\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3507 - accuracy: 0.9298 - val_loss: 0.3503 - val_accuracy: 0.9317\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3477 - accuracy: 0.9298 - val_loss: 0.3815 - val_accuracy: 0.9207\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3468 - accuracy: 0.9301 - val_loss: 0.3353 - val_accuracy: 0.9353\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3440 - accuracy: 0.9302 - val_loss: 0.3448 - val_accuracy: 0.9324\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3432 - accuracy: 0.9317 - val_loss: 0.3069 - val_accuracy: 0.9457\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3442 - accuracy: 0.9303 - val_loss: 0.3591 - val_accuracy: 0.9290\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3443 - accuracy: 0.9314 - val_loss: 0.3249 - val_accuracy: 0.9388\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3421 - accuracy: 0.9312 - val_loss: 0.3222 - val_accuracy: 0.9393\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3416 - accuracy: 0.9306 - val_loss: 0.3398 - val_accuracy: 0.9325\n",
      "iteration:  6\n",
      "Test loss: 0.3397571574610366\n",
      "Test accuracy: 0.9325445890426636\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3503 - accuracy: 0.9283 - val_loss: 0.3401 - val_accuracy: 0.9337\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3469 - accuracy: 0.9309 - val_loss: 0.3326 - val_accuracy: 0.9375\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3483 - accuracy: 0.9302 - val_loss: 0.3129 - val_accuracy: 0.9413\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3476 - accuracy: 0.9293 - val_loss: 0.3600 - val_accuracy: 0.9269\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3474 - accuracy: 0.9299 - val_loss: 0.3568 - val_accuracy: 0.9271\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3395 - accuracy: 0.9305 - val_loss: 0.3502 - val_accuracy: 0.9350\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3446 - accuracy: 0.9310 - val_loss: 0.3286 - val_accuracy: 0.9388\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3427 - accuracy: 0.9296 - val_loss: 0.3609 - val_accuracy: 0.9286\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3459 - accuracy: 0.9306 - val_loss: 0.3480 - val_accuracy: 0.9315\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3448 - accuracy: 0.9309 - val_loss: 0.3523 - val_accuracy: 0.9291\n",
      "iteration:  7\n",
      "Test loss: 0.35230085715770576\n",
      "Test accuracy: 0.9291256666183472\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3519 - accuracy: 0.9287 - val_loss: 0.3322 - val_accuracy: 0.9367\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3469 - accuracy: 0.9303 - val_loss: 0.3575 - val_accuracy: 0.9276\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3424 - accuracy: 0.9306 - val_loss: 0.3465 - val_accuracy: 0.9304\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3450 - accuracy: 0.9298 - val_loss: 0.3684 - val_accuracy: 0.9209\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3444 - accuracy: 0.9305 - val_loss: 0.3443 - val_accuracy: 0.9326\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3413 - accuracy: 0.9312 - val_loss: 0.3441 - val_accuracy: 0.9316\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3420 - accuracy: 0.9312 - val_loss: 0.3320 - val_accuracy: 0.9362\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3430 - accuracy: 0.9301 - val_loss: 0.4076 - val_accuracy: 0.9149\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3399 - accuracy: 0.9316 - val_loss: 0.3398 - val_accuracy: 0.9349\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3440 - accuracy: 0.9300 - val_loss: 0.3288 - val_accuracy: 0.9387\n",
      "iteration:  8\n",
      "Test loss: 0.32879060640537305\n",
      "Test accuracy: 0.9387292265892029\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3422 - accuracy: 0.9312 - val_loss: 0.3348 - val_accuracy: 0.9357\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3391 - accuracy: 0.9319 - val_loss: 0.3666 - val_accuracy: 0.9270\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3371 - accuracy: 0.9328 - val_loss: 0.3442 - val_accuracy: 0.9315\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3385 - accuracy: 0.9326 - val_loss: 0.3422 - val_accuracy: 0.9342\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3404 - accuracy: 0.9318 - val_loss: 0.3631 - val_accuracy: 0.9305\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3374 - accuracy: 0.9317 - val_loss: 0.3427 - val_accuracy: 0.9337\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3370 - accuracy: 0.9316 - val_loss: 0.3288 - val_accuracy: 0.9367\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3377 - accuracy: 0.9319 - val_loss: 0.3660 - val_accuracy: 0.9265\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3404 - accuracy: 0.9315 - val_loss: 0.3413 - val_accuracy: 0.9330\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3336 - accuracy: 0.9335 - val_loss: 0.3281 - val_accuracy: 0.9359\n",
      "iteration:  9\n",
      "Test loss: 0.32807312857247456\n",
      "Test accuracy: 0.9359250068664551\n",
      "rate=2.5:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (52327, 10)\n",
      "Shape y_true (20930, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.6212 - accuracy: 0.8511 - val_loss: 0.3639 - val_accuracy: 0.9209\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4700 - accuracy: 0.8948 - val_loss: 0.3545 - val_accuracy: 0.9269\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4462 - accuracy: 0.9032 - val_loss: 0.3763 - val_accuracy: 0.9226\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4369 - accuracy: 0.9082 - val_loss: 0.3294 - val_accuracy: 0.9367\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4274 - accuracy: 0.9132 - val_loss: 0.3358 - val_accuracy: 0.9363\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.4228 - accuracy: 0.9155 - val_loss: 0.3306 - val_accuracy: 0.9377\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4151 - accuracy: 0.9175 - val_loss: 0.3358 - val_accuracy: 0.9380\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.4107 - accuracy: 0.9198 - val_loss: 0.3636 - val_accuracy: 0.9291\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.4072 - accuracy: 0.9214 - val_loss: 0.3455 - val_accuracy: 0.9352\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4053 - accuracy: 0.9216 - val_loss: 0.3319 - val_accuracy: 0.9401\n",
      "iteration:  0\n",
      "Test loss: 0.33189569769018473\n",
      "Test accuracy: 0.9401121735572815\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3676 - accuracy: 0.9278 - val_loss: 0.3594 - val_accuracy: 0.9322\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3669 - accuracy: 0.9278 - val_loss: 0.3735 - val_accuracy: 0.9284\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3547 - accuracy: 0.9303 - val_loss: 0.3899 - val_accuracy: 0.9295\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3549 - accuracy: 0.9309 - val_loss: 0.4007 - val_accuracy: 0.9262\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3564 - accuracy: 0.9300 - val_loss: 0.3698 - val_accuracy: 0.9272\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3507 - accuracy: 0.9308 - val_loss: 0.3672 - val_accuracy: 0.9309\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3528 - accuracy: 0.9313 - val_loss: 0.3591 - val_accuracy: 0.9338\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3499 - accuracy: 0.9319 - val_loss: 0.3341 - val_accuracy: 0.9425\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3490 - accuracy: 0.9320 - val_loss: 0.3393 - val_accuracy: 0.9380\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3441 - accuracy: 0.9337 - val_loss: 0.3703 - val_accuracy: 0.9288\n",
      "iteration:  1\n",
      "Test loss: 0.3702689862518715\n",
      "Test accuracy: 0.92881840467453\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3690 - accuracy: 0.9257 - val_loss: 0.3704 - val_accuracy: 0.9290\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3651 - accuracy: 0.9262 - val_loss: 0.3534 - val_accuracy: 0.9328\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3630 - accuracy: 0.9266 - val_loss: 0.3399 - val_accuracy: 0.9363\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3586 - accuracy: 0.9268 - val_loss: 0.3388 - val_accuracy: 0.9375\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3569 - accuracy: 0.9287 - val_loss: 0.3777 - val_accuracy: 0.9228\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3570 - accuracy: 0.9295 - val_loss: 0.3666 - val_accuracy: 0.9289\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3542 - accuracy: 0.9290 - val_loss: 0.3445 - val_accuracy: 0.9362\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3509 - accuracy: 0.9292 - val_loss: 0.3553 - val_accuracy: 0.9350\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3494 - accuracy: 0.9305 - val_loss: 0.3388 - val_accuracy: 0.9390\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3501 - accuracy: 0.9300 - val_loss: 0.3719 - val_accuracy: 0.9268\n",
      "iteration:  2\n",
      "Test loss: 0.37190230612968767\n",
      "Test accuracy: 0.9268208146095276\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3656 - accuracy: 0.9246 - val_loss: 0.3431 - val_accuracy: 0.9342\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3578 - accuracy: 0.9260 - val_loss: 0.3844 - val_accuracy: 0.9251\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3597 - accuracy: 0.9255 - val_loss: 0.4076 - val_accuracy: 0.9188\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3569 - accuracy: 0.9268 - val_loss: 0.3396 - val_accuracy: 0.9353\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3558 - accuracy: 0.9266 - val_loss: 0.3719 - val_accuracy: 0.9271\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3530 - accuracy: 0.9267 - val_loss: 0.3392 - val_accuracy: 0.9340\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3544 - accuracy: 0.9280 - val_loss: 0.3605 - val_accuracy: 0.9269\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3516 - accuracy: 0.9280 - val_loss: 0.3461 - val_accuracy: 0.9333\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3532 - accuracy: 0.9268 - val_loss: 0.3680 - val_accuracy: 0.9270\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3499 - accuracy: 0.9272 - val_loss: 0.3605 - val_accuracy: 0.9280\n",
      "iteration:  3\n",
      "Test loss: 0.36053601190328743\n",
      "Test accuracy: 0.9280116558074951\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3539 - accuracy: 0.9271 - val_loss: 0.3679 - val_accuracy: 0.9253\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3467 - accuracy: 0.9289 - val_loss: 0.3583 - val_accuracy: 0.9277\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3482 - accuracy: 0.9285 - val_loss: 0.3720 - val_accuracy: 0.9234\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3459 - accuracy: 0.9295 - val_loss: 0.4117 - val_accuracy: 0.9157\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3441 - accuracy: 0.9296 - val_loss: 0.3830 - val_accuracy: 0.9202\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3412 - accuracy: 0.9294 - val_loss: 0.3631 - val_accuracy: 0.9271\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3398 - accuracy: 0.9308 - val_loss: 0.3521 - val_accuracy: 0.9305\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3389 - accuracy: 0.9314 - val_loss: 0.3358 - val_accuracy: 0.9359\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3389 - accuracy: 0.9292 - val_loss: 0.3589 - val_accuracy: 0.9279\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3390 - accuracy: 0.9303 - val_loss: 0.3577 - val_accuracy: 0.9291\n",
      "iteration:  4\n",
      "Test loss: 0.35771189379999807\n",
      "Test accuracy: 0.9291256666183472\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3492 - accuracy: 0.9289 - val_loss: 0.3628 - val_accuracy: 0.9266\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3414 - accuracy: 0.9302 - val_loss: 0.3710 - val_accuracy: 0.9229\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3415 - accuracy: 0.9287 - val_loss: 0.3559 - val_accuracy: 0.9294\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3394 - accuracy: 0.9299 - val_loss: 0.3632 - val_accuracy: 0.9246\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3370 - accuracy: 0.9312 - val_loss: 0.3681 - val_accuracy: 0.9252\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3406 - accuracy: 0.9309 - val_loss: 0.3735 - val_accuracy: 0.9217\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3389 - accuracy: 0.9304 - val_loss: 0.3349 - val_accuracy: 0.9355\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3375 - accuracy: 0.9299 - val_loss: 0.3574 - val_accuracy: 0.9277\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3342 - accuracy: 0.9304 - val_loss: 0.3702 - val_accuracy: 0.9242\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3335 - accuracy: 0.9311 - val_loss: 0.3588 - val_accuracy: 0.9291\n",
      "iteration:  5\n",
      "Test loss: 0.3587813687218283\n",
      "Test accuracy: 0.9291256666183472\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3430 - accuracy: 0.9284 - val_loss: 0.3742 - val_accuracy: 0.9226\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3395 - accuracy: 0.9289 - val_loss: 0.3993 - val_accuracy: 0.9187\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3374 - accuracy: 0.9301 - val_loss: 0.4185 - val_accuracy: 0.9105\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3384 - accuracy: 0.9304 - val_loss: 0.3562 - val_accuracy: 0.9271\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3340 - accuracy: 0.9305 - val_loss: 0.3239 - val_accuracy: 0.9368\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3381 - accuracy: 0.9288 - val_loss: 0.3568 - val_accuracy: 0.9294\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3319 - accuracy: 0.9314 - val_loss: 0.3618 - val_accuracy: 0.9266\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3302 - accuracy: 0.9318 - val_loss: 0.3876 - val_accuracy: 0.9186\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3263 - accuracy: 0.9320 - val_loss: 0.3604 - val_accuracy: 0.9266\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3315 - accuracy: 0.9318 - val_loss: 0.3825 - val_accuracy: 0.9190\n",
      "iteration:  6\n",
      "Test loss: 0.382475469950598\n",
      "Test accuracy: 0.9190227389335632\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3402 - accuracy: 0.9274 - val_loss: 0.3769 - val_accuracy: 0.9196\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3375 - accuracy: 0.9280 - val_loss: 0.3487 - val_accuracy: 0.9289\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3393 - accuracy: 0.9286 - val_loss: 0.3666 - val_accuracy: 0.9234\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3329 - accuracy: 0.9294 - val_loss: 0.4016 - val_accuracy: 0.9124\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3327 - accuracy: 0.9298 - val_loss: 0.3458 - val_accuracy: 0.9321\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3309 - accuracy: 0.9314 - val_loss: 0.3988 - val_accuracy: 0.9191\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3347 - accuracy: 0.9304 - val_loss: 0.3758 - val_accuracy: 0.9171\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3336 - accuracy: 0.9306 - val_loss: 0.3822 - val_accuracy: 0.9179\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3298 - accuracy: 0.9312 - val_loss: 0.3685 - val_accuracy: 0.9266\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3312 - accuracy: 0.9305 - val_loss: 0.3732 - val_accuracy: 0.9186\n",
      "iteration:  7\n",
      "Test loss: 0.37321225071088915\n",
      "Test accuracy: 0.9186385869979858\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3479 - accuracy: 0.9257 - val_loss: 0.3327 - val_accuracy: 0.9329\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3412 - accuracy: 0.9276 - val_loss: 0.3418 - val_accuracy: 0.9309\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3381 - accuracy: 0.9273 - val_loss: 0.3953 - val_accuracy: 0.9146\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3351 - accuracy: 0.9284 - val_loss: 0.3987 - val_accuracy: 0.9234\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3364 - accuracy: 0.9287 - val_loss: 0.3945 - val_accuracy: 0.9141\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3348 - accuracy: 0.9293 - val_loss: 0.3784 - val_accuracy: 0.9196\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3333 - accuracy: 0.9282 - val_loss: 0.4230 - val_accuracy: 0.9098\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3337 - accuracy: 0.9291 - val_loss: 0.3788 - val_accuracy: 0.9211\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3344 - accuracy: 0.9294 - val_loss: 0.3737 - val_accuracy: 0.9243\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3307 - accuracy: 0.9300 - val_loss: 0.3632 - val_accuracy: 0.9259\n",
      "iteration:  8\n",
      "Test loss: 0.3632418071804196\n",
      "Test accuracy: 0.925937294960022\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3403 - accuracy: 0.9285 - val_loss: 0.3651 - val_accuracy: 0.9216\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3354 - accuracy: 0.9293 - val_loss: 0.3392 - val_accuracy: 0.9333\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3336 - accuracy: 0.9296 - val_loss: 0.3453 - val_accuracy: 0.9291\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3315 - accuracy: 0.9307 - val_loss: 0.3630 - val_accuracy: 0.9248\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3335 - accuracy: 0.9298 - val_loss: 0.3557 - val_accuracy: 0.9273\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3322 - accuracy: 0.9303 - val_loss: 0.3683 - val_accuracy: 0.9208\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3319 - accuracy: 0.9304 - val_loss: 0.3581 - val_accuracy: 0.9254\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3308 - accuracy: 0.9301 - val_loss: 0.3530 - val_accuracy: 0.9261\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3301 - accuracy: 0.9303 - val_loss: 0.3745 - val_accuracy: 0.9196\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3297 - accuracy: 0.9297 - val_loss: 0.3594 - val_accuracy: 0.9289\n",
      "iteration:  9\n",
      "Test loss: 0.35939788090702796\n",
      "Test accuracy: 0.9288567900657654\n",
      "rate=5.0:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (61048, 10)\n",
      "Shape y_true (12209, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.6314 - accuracy: 0.8494 - val_loss: 0.3377 - val_accuracy: 0.9271\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4733 - accuracy: 0.8940 - val_loss: 0.3315 - val_accuracy: 0.9311\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4469 - accuracy: 0.9035 - val_loss: 0.3647 - val_accuracy: 0.9261\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4395 - accuracy: 0.9086 - val_loss: 0.3821 - val_accuracy: 0.9184\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4262 - accuracy: 0.9133 - val_loss: 0.3380 - val_accuracy: 0.9371\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4218 - accuracy: 0.9162 - val_loss: 0.3441 - val_accuracy: 0.9362\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4137 - accuracy: 0.9190 - val_loss: 0.3244 - val_accuracy: 0.9416\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.4089 - accuracy: 0.9196 - val_loss: 0.3307 - val_accuracy: 0.9413\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4057 - accuracy: 0.9211 - val_loss: 0.3393 - val_accuracy: 0.9370\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4014 - accuracy: 0.9223 - val_loss: 0.3267 - val_accuracy: 0.9426\n",
      "iteration:  0\n",
      "Test loss: 0.3267352516547111\n",
      "Test accuracy: 0.942570686340332\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3654 - accuracy: 0.9280 - val_loss: 0.3614 - val_accuracy: 0.9328\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3626 - accuracy: 0.9269 - val_loss: 0.3928 - val_accuracy: 0.9241\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3544 - accuracy: 0.9297 - val_loss: 0.3630 - val_accuracy: 0.9320\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3519 - accuracy: 0.9286 - val_loss: 0.3501 - val_accuracy: 0.9397\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3506 - accuracy: 0.9303 - val_loss: 0.3676 - val_accuracy: 0.9320\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3465 - accuracy: 0.9306 - val_loss: 0.3624 - val_accuracy: 0.9329\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3423 - accuracy: 0.9318 - val_loss: 0.3421 - val_accuracy: 0.9389\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3421 - accuracy: 0.9318 - val_loss: 0.3765 - val_accuracy: 0.9291\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3422 - accuracy: 0.9310 - val_loss: 0.3702 - val_accuracy: 0.9318\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3403 - accuracy: 0.9325 - val_loss: 0.3677 - val_accuracy: 0.9308\n",
      "iteration:  1\n",
      "Test loss: 0.36766898047726215\n",
      "Test accuracy: 0.9308159351348877\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3559 - accuracy: 0.9274 - val_loss: 0.4126 - val_accuracy: 0.9211\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3518 - accuracy: 0.9275 - val_loss: 0.3502 - val_accuracy: 0.9358\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3486 - accuracy: 0.9294 - val_loss: 0.3801 - val_accuracy: 0.9276\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3461 - accuracy: 0.9286 - val_loss: 0.4092 - val_accuracy: 0.9154\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3427 - accuracy: 0.9288 - val_loss: 0.3847 - val_accuracy: 0.9253\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3403 - accuracy: 0.9297 - val_loss: 0.3876 - val_accuracy: 0.9271\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3424 - accuracy: 0.9285 - val_loss: 0.3558 - val_accuracy: 0.9355\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3435 - accuracy: 0.9295 - val_loss: 0.4056 - val_accuracy: 0.9182\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3384 - accuracy: 0.9302 - val_loss: 0.3812 - val_accuracy: 0.9264\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3396 - accuracy: 0.9305 - val_loss: 0.3773 - val_accuracy: 0.9297\n",
      "iteration:  2\n",
      "Test loss: 0.37733982720474707\n",
      "Test accuracy: 0.9297019243240356\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3379 - accuracy: 0.9305 - val_loss: 0.4048 - val_accuracy: 0.9174\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3336 - accuracy: 0.9310 - val_loss: 0.3955 - val_accuracy: 0.9243\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3289 - accuracy: 0.9324 - val_loss: 0.3798 - val_accuracy: 0.9250\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3266 - accuracy: 0.9324 - val_loss: 0.4156 - val_accuracy: 0.9138\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3300 - accuracy: 0.9321 - val_loss: 0.4039 - val_accuracy: 0.9201\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3251 - accuracy: 0.9333 - val_loss: 0.3660 - val_accuracy: 0.9305\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3246 - accuracy: 0.9331 - val_loss: 0.4341 - val_accuracy: 0.9109\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3233 - accuracy: 0.9336 - val_loss: 0.4452 - val_accuracy: 0.9125\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3213 - accuracy: 0.9329 - val_loss: 0.3847 - val_accuracy: 0.9268\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3198 - accuracy: 0.9343 - val_loss: 0.4000 - val_accuracy: 0.9218\n",
      "iteration:  3\n",
      "Test loss: 0.3999997844304786\n",
      "Test accuracy: 0.921826958656311\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3342 - accuracy: 0.9296 - val_loss: 0.4073 - val_accuracy: 0.9138\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3251 - accuracy: 0.9307 - val_loss: 0.3709 - val_accuracy: 0.9330\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3270 - accuracy: 0.9319 - val_loss: 0.3649 - val_accuracy: 0.9296\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3231 - accuracy: 0.9318 - val_loss: 0.3801 - val_accuracy: 0.9280\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3212 - accuracy: 0.9328 - val_loss: 0.3599 - val_accuracy: 0.9305\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3166 - accuracy: 0.9332 - val_loss: 0.3951 - val_accuracy: 0.9194\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3188 - accuracy: 0.9329 - val_loss: 0.4233 - val_accuracy: 0.9176\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3180 - accuracy: 0.9332 - val_loss: 0.4072 - val_accuracy: 0.9184\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3158 - accuracy: 0.9343 - val_loss: 0.3767 - val_accuracy: 0.9272\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3157 - accuracy: 0.9344 - val_loss: 0.3689 - val_accuracy: 0.9325\n",
      "iteration:  4\n",
      "Test loss: 0.3688611485883731\n",
      "Test accuracy: 0.9325445890426636\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3239 - accuracy: 0.9310 - val_loss: 0.3931 - val_accuracy: 0.9189\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3177 - accuracy: 0.9330 - val_loss: 0.4210 - val_accuracy: 0.9141\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3116 - accuracy: 0.9333 - val_loss: 0.4483 - val_accuracy: 0.9134\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3134 - accuracy: 0.9338 - val_loss: 0.3919 - val_accuracy: 0.9255\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3117 - accuracy: 0.9343 - val_loss: 0.3601 - val_accuracy: 0.9315\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3096 - accuracy: 0.9356 - val_loss: 0.3882 - val_accuracy: 0.9236\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3098 - accuracy: 0.9334 - val_loss: 0.3778 - val_accuracy: 0.9266\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3099 - accuracy: 0.9347 - val_loss: 0.3924 - val_accuracy: 0.9209\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3074 - accuracy: 0.9347 - val_loss: 0.4234 - val_accuracy: 0.9106\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3089 - accuracy: 0.9349 - val_loss: 0.3972 - val_accuracy: 0.9236\n",
      "iteration:  5\n",
      "Test loss: 0.3971553189106418\n",
      "Test accuracy: 0.9235556125640869\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3246 - accuracy: 0.9301 - val_loss: 0.4533 - val_accuracy: 0.9133\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3261 - accuracy: 0.9303 - val_loss: 0.3626 - val_accuracy: 0.9249\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3187 - accuracy: 0.9308 - val_loss: 0.3773 - val_accuracy: 0.9272\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3166 - accuracy: 0.9302 - val_loss: 0.4223 - val_accuracy: 0.9192\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3203 - accuracy: 0.9314 - val_loss: 0.3757 - val_accuracy: 0.9227\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3166 - accuracy: 0.9316 - val_loss: 0.3652 - val_accuracy: 0.9273\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3148 - accuracy: 0.9311 - val_loss: 0.3678 - val_accuracy: 0.9274\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3154 - accuracy: 0.9319 - val_loss: 0.3702 - val_accuracy: 0.9266\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3169 - accuracy: 0.9311 - val_loss: 0.3987 - val_accuracy: 0.9231\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3139 - accuracy: 0.9322 - val_loss: 0.4239 - val_accuracy: 0.9071\n",
      "iteration:  6\n",
      "Test loss: 0.4239489809823461\n",
      "Test accuracy: 0.9071143269538879\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3429 - accuracy: 0.9237 - val_loss: 0.3859 - val_accuracy: 0.9178\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3399 - accuracy: 0.9235 - val_loss: 0.4374 - val_accuracy: 0.9036\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3331 - accuracy: 0.9251 - val_loss: 0.3976 - val_accuracy: 0.9159\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3333 - accuracy: 0.9261 - val_loss: 0.3884 - val_accuracy: 0.9195\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3338 - accuracy: 0.9256 - val_loss: 0.4017 - val_accuracy: 0.9126\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3311 - accuracy: 0.9260 - val_loss: 0.4257 - val_accuracy: 0.9105\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3295 - accuracy: 0.9265 - val_loss: 0.3957 - val_accuracy: 0.9153\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3293 - accuracy: 0.9262 - val_loss: 0.4080 - val_accuracy: 0.9159\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3283 - accuracy: 0.9274 - val_loss: 0.3822 - val_accuracy: 0.9218\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3267 - accuracy: 0.9276 - val_loss: 0.4061 - val_accuracy: 0.9094\n",
      "iteration:  7\n",
      "Test loss: 0.40607595540807523\n",
      "Test accuracy: 0.9093807339668274\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3324 - accuracy: 0.9230 - val_loss: 0.4496 - val_accuracy: 0.8987\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3287 - accuracy: 0.9244 - val_loss: 0.3936 - val_accuracy: 0.9127\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3274 - accuracy: 0.9246 - val_loss: 0.3886 - val_accuracy: 0.9181\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3263 - accuracy: 0.9250 - val_loss: 0.3976 - val_accuracy: 0.9148\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3277 - accuracy: 0.9243 - val_loss: 0.4506 - val_accuracy: 0.9001\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3225 - accuracy: 0.9261 - val_loss: 0.4310 - val_accuracy: 0.9047\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3238 - accuracy: 0.9256 - val_loss: 0.4007 - val_accuracy: 0.9143\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3219 - accuracy: 0.9265 - val_loss: 0.4249 - val_accuracy: 0.9086\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3222 - accuracy: 0.9256 - val_loss: 0.4550 - val_accuracy: 0.8937\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3203 - accuracy: 0.9266 - val_loss: 0.3865 - val_accuracy: 0.9184\n",
      "iteration:  8\n",
      "Test loss: 0.3864533117964873\n",
      "Test accuracy: 0.9184080958366394\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3176 - accuracy: 0.9281 - val_loss: 0.4252 - val_accuracy: 0.9103\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3150 - accuracy: 0.9295 - val_loss: 0.3871 - val_accuracy: 0.9211\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3136 - accuracy: 0.9306 - val_loss: 0.4013 - val_accuracy: 0.9154\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 47ms/step - loss: 0.3110 - accuracy: 0.9309 - val_loss: 0.3741 - val_accuracy: 0.9237\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3070 - accuracy: 0.9328 - val_loss: 0.4224 - val_accuracy: 0.9082\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3076 - accuracy: 0.9320 - val_loss: 0.4108 - val_accuracy: 0.9128\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3073 - accuracy: 0.9313 - val_loss: 0.3981 - val_accuracy: 0.9163\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3055 - accuracy: 0.9317 - val_loss: 0.4095 - val_accuracy: 0.9096\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3075 - accuracy: 0.9311 - val_loss: 0.4369 - val_accuracy: 0.9064\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3078 - accuracy: 0.9320 - val_loss: 0.4072 - val_accuracy: 0.9115\n",
      "iteration:  9\n",
      "Test loss: 0.4071932436959027\n",
      "Test accuracy: 0.9114551544189453\n",
      "rate=7.5:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (64639, 10)\n",
      "Shape y_true (8618, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.6303 - accuracy: 0.8490 - val_loss: 0.3388 - val_accuracy: 0.9289\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.4774 - accuracy: 0.8935 - val_loss: 0.3381 - val_accuracy: 0.9316\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.4510 - accuracy: 0.9030 - val_loss: 0.3211 - val_accuracy: 0.9392\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.4384 - accuracy: 0.9082 - val_loss: 0.3475 - val_accuracy: 0.9324\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.4241 - accuracy: 0.9138 - val_loss: 0.3440 - val_accuracy: 0.9344\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4214 - accuracy: 0.9149 - val_loss: 0.3285 - val_accuracy: 0.9410\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4144 - accuracy: 0.9181 - val_loss: 0.3611 - val_accuracy: 0.9285\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4118 - accuracy: 0.9201 - val_loss: 0.3499 - val_accuracy: 0.9325\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4087 - accuracy: 0.9212 - val_loss: 0.3488 - val_accuracy: 0.9337\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4034 - accuracy: 0.9222 - val_loss: 0.3557 - val_accuracy: 0.9315\n",
      "iteration:  0\n",
      "Test loss: 0.3556816362041451\n",
      "Test accuracy: 0.9314689636230469\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3801 - accuracy: 0.9223 - val_loss: 0.3870 - val_accuracy: 0.9264\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3693 - accuracy: 0.9247 - val_loss: 0.3541 - val_accuracy: 0.9345\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 102s 45ms/step - loss: 0.3684 - accuracy: 0.9247 - val_loss: 0.3830 - val_accuracy: 0.9250\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3665 - accuracy: 0.9262 - val_loss: 0.3524 - val_accuracy: 0.9361\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3629 - accuracy: 0.9261 - val_loss: 0.3934 - val_accuracy: 0.9243\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3607 - accuracy: 0.9263 - val_loss: 0.4090 - val_accuracy: 0.9183\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3560 - accuracy: 0.9267 - val_loss: 0.3968 - val_accuracy: 0.9301\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3577 - accuracy: 0.9268 - val_loss: 0.3847 - val_accuracy: 0.9255\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3525 - accuracy: 0.9285 - val_loss: 0.4149 - val_accuracy: 0.9204\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3515 - accuracy: 0.9268 - val_loss: 0.3637 - val_accuracy: 0.9344\n",
      "iteration:  1\n",
      "Test loss: 0.36367824856826114\n",
      "Test accuracy: 0.9343884587287903\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3515 - accuracy: 0.9280 - val_loss: 0.3984 - val_accuracy: 0.9192\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3486 - accuracy: 0.9271 - val_loss: 0.3571 - val_accuracy: 0.9322\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3430 - accuracy: 0.9282 - val_loss: 0.4246 - val_accuracy: 0.9135\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3410 - accuracy: 0.9291 - val_loss: 0.3700 - val_accuracy: 0.9292\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3389 - accuracy: 0.9286 - val_loss: 0.4355 - val_accuracy: 0.9120\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3360 - accuracy: 0.9299 - val_loss: 0.3815 - val_accuracy: 0.9277\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3359 - accuracy: 0.9302 - val_loss: 0.3758 - val_accuracy: 0.9318\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3336 - accuracy: 0.9308 - val_loss: 0.4106 - val_accuracy: 0.9218\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3329 - accuracy: 0.9310 - val_loss: 0.3847 - val_accuracy: 0.9268\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3307 - accuracy: 0.9303 - val_loss: 0.4031 - val_accuracy: 0.9238\n",
      "iteration:  2\n",
      "Test loss: 0.40305009029562955\n",
      "Test accuracy: 0.9238245487213135\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3511 - accuracy: 0.9248 - val_loss: 0.4464 - val_accuracy: 0.9094\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3475 - accuracy: 0.9267 - val_loss: 0.3563 - val_accuracy: 0.9347\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3423 - accuracy: 0.9286 - val_loss: 0.3789 - val_accuracy: 0.9266\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3393 - accuracy: 0.9275 - val_loss: 0.3705 - val_accuracy: 0.9306\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3415 - accuracy: 0.9272 - val_loss: 0.3740 - val_accuracy: 0.9301\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3375 - accuracy: 0.9277 - val_loss: 0.3917 - val_accuracy: 0.9224\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3350 - accuracy: 0.9289 - val_loss: 0.3919 - val_accuracy: 0.9233\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3357 - accuracy: 0.9281 - val_loss: 0.4198 - val_accuracy: 0.9132\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3324 - accuracy: 0.9296 - val_loss: 0.4057 - val_accuracy: 0.9178\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3320 - accuracy: 0.9291 - val_loss: 0.3853 - val_accuracy: 0.9267\n",
      "iteration:  3\n",
      "Test loss: 0.3852509588882391\n",
      "Test accuracy: 0.9267055988311768\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3271 - accuracy: 0.9314 - val_loss: 0.4120 - val_accuracy: 0.9208\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3274 - accuracy: 0.9302 - val_loss: 0.3943 - val_accuracy: 0.9176\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3226 - accuracy: 0.9314 - val_loss: 0.3718 - val_accuracy: 0.9302\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3233 - accuracy: 0.9319 - val_loss: 0.4113 - val_accuracy: 0.9179\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3211 - accuracy: 0.9321 - val_loss: 0.3895 - val_accuracy: 0.9237\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3157 - accuracy: 0.9335 - val_loss: 0.3688 - val_accuracy: 0.9327\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3202 - accuracy: 0.9326 - val_loss: 0.3969 - val_accuracy: 0.9222\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3156 - accuracy: 0.9337 - val_loss: 0.4557 - val_accuracy: 0.9115\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3166 - accuracy: 0.9325 - val_loss: 0.4132 - val_accuracy: 0.9213\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3167 - accuracy: 0.9333 - val_loss: 0.4187 - val_accuracy: 0.9185\n",
      "iteration:  4\n",
      "Test loss: 0.4187134421249072\n",
      "Test accuracy: 0.918523371219635\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3354 - accuracy: 0.9269 - val_loss: 0.3988 - val_accuracy: 0.9192\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3328 - accuracy: 0.9279 - val_loss: 0.4165 - val_accuracy: 0.9108\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3276 - accuracy: 0.9291 - val_loss: 0.3992 - val_accuracy: 0.9165\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3284 - accuracy: 0.9292 - val_loss: 0.4118 - val_accuracy: 0.9136\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3273 - accuracy: 0.9300 - val_loss: 0.4081 - val_accuracy: 0.9204\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3265 - accuracy: 0.9292 - val_loss: 0.3639 - val_accuracy: 0.9264\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3205 - accuracy: 0.9308 - val_loss: 0.3972 - val_accuracy: 0.9196\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3217 - accuracy: 0.9300 - val_loss: 0.3946 - val_accuracy: 0.9209\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3214 - accuracy: 0.9302 - val_loss: 0.3853 - val_accuracy: 0.9245\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3183 - accuracy: 0.9312 - val_loss: 0.4170 - val_accuracy: 0.9142\n",
      "iteration:  5\n",
      "Test loss: 0.4169853329521128\n",
      "Test accuracy: 0.914220929145813\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3296 - accuracy: 0.9272 - val_loss: 0.4000 - val_accuracy: 0.9206\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3255 - accuracy: 0.9290 - val_loss: 0.4162 - val_accuracy: 0.9142\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3267 - accuracy: 0.9286 - val_loss: 0.3961 - val_accuracy: 0.9181\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3236 - accuracy: 0.9291 - val_loss: 0.3938 - val_accuracy: 0.9201\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3233 - accuracy: 0.9295 - val_loss: 0.4339 - val_accuracy: 0.9093\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3219 - accuracy: 0.9293 - val_loss: 0.3933 - val_accuracy: 0.9217\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3220 - accuracy: 0.9292 - val_loss: 0.3838 - val_accuracy: 0.9254\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3192 - accuracy: 0.9309 - val_loss: 0.5019 - val_accuracy: 0.8932\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3194 - accuracy: 0.9303 - val_loss: 0.4603 - val_accuracy: 0.9032\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3199 - accuracy: 0.9293 - val_loss: 0.4160 - val_accuracy: 0.9141\n",
      "iteration:  6\n",
      "Test loss: 0.4160297924507302\n",
      "Test accuracy: 0.9141441583633423\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3280 - accuracy: 0.9280 - val_loss: 0.3856 - val_accuracy: 0.9223\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3232 - accuracy: 0.9283 - val_loss: 0.4081 - val_accuracy: 0.9147\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3196 - accuracy: 0.9289 - val_loss: 0.4053 - val_accuracy: 0.9202\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3170 - accuracy: 0.9303 - val_loss: 0.3777 - val_accuracy: 0.9214\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3184 - accuracy: 0.9298 - val_loss: 0.4284 - val_accuracy: 0.9072\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3160 - accuracy: 0.9304 - val_loss: 0.4467 - val_accuracy: 0.9064\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3172 - accuracy: 0.9302 - val_loss: 0.3871 - val_accuracy: 0.9224\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3147 - accuracy: 0.9303 - val_loss: 0.4566 - val_accuracy: 0.8979\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3134 - accuracy: 0.9314 - val_loss: 0.4738 - val_accuracy: 0.8979\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3133 - accuracy: 0.9303 - val_loss: 0.4044 - val_accuracy: 0.9154\n",
      "iteration:  7\n",
      "Test loss: 0.40443120259372606\n",
      "Test accuracy: 0.9154118299484253\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3170 - accuracy: 0.9308 - val_loss: 0.4325 - val_accuracy: 0.9087\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3128 - accuracy: 0.9324 - val_loss: 0.4656 - val_accuracy: 0.8992\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3100 - accuracy: 0.9320 - val_loss: 0.3923 - val_accuracy: 0.9191\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3091 - accuracy: 0.9335 - val_loss: 0.4484 - val_accuracy: 0.9063\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3067 - accuracy: 0.9324 - val_loss: 0.3930 - val_accuracy: 0.9180\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3059 - accuracy: 0.9324 - val_loss: 0.4519 - val_accuracy: 0.9053\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3061 - accuracy: 0.9323 - val_loss: 0.3939 - val_accuracy: 0.9179\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3047 - accuracy: 0.9325 - val_loss: 0.4116 - val_accuracy: 0.9151\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3086 - accuracy: 0.9333 - val_loss: 0.4413 - val_accuracy: 0.9051\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3019 - accuracy: 0.9341 - val_loss: 0.4102 - val_accuracy: 0.9112\n",
      "iteration:  8\n",
      "Test loss: 0.41020467385567255\n",
      "Test accuracy: 0.9112246632575989\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3216 - accuracy: 0.9285 - val_loss: 0.4342 - val_accuracy: 0.9073\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3154 - accuracy: 0.9305 - val_loss: 0.4152 - val_accuracy: 0.9112\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3152 - accuracy: 0.9302 - val_loss: 0.4618 - val_accuracy: 0.9032\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3080 - accuracy: 0.9323 - val_loss: 0.4288 - val_accuracy: 0.9066\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3097 - accuracy: 0.9314 - val_loss: 0.3943 - val_accuracy: 0.9167\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3105 - accuracy: 0.9311 - val_loss: 0.3859 - val_accuracy: 0.9196\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3087 - accuracy: 0.9311 - val_loss: 0.4361 - val_accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3065 - accuracy: 0.9325 - val_loss: 0.4097 - val_accuracy: 0.9134\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3059 - accuracy: 0.9319 - val_loss: 0.4003 - val_accuracy: 0.9177\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3086 - accuracy: 0.9317 - val_loss: 0.4157 - val_accuracy: 0.9128\n",
      "iteration:  9\n",
      "Test loss: 0.41574252702263725\n",
      "Test accuracy: 0.9127612113952637\n",
      "rate=9.0:\n",
      "\n",
      "Learning rate:  0.001\n",
      "Original model with labelled data only predicting on test data:  0.9517132639884949\n",
      "x_true_pseudo.shape:  (73257, 32, 32, 3)\n",
      "Shape y_pseudo (65932, 10)\n",
      "Shape y_true (7325, 10)\n",
      "y_true_pseudo.shape:  (73257, 10)\n",
      "0\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.6304 - accuracy: 0.8489 - val_loss: 0.3516 - val_accuracy: 0.9254\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4751 - accuracy: 0.8928 - val_loss: 0.3351 - val_accuracy: 0.9343\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4525 - accuracy: 0.9023 - val_loss: 0.3690 - val_accuracy: 0.9264\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4399 - accuracy: 0.9075 - val_loss: 0.3913 - val_accuracy: 0.9186\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4272 - accuracy: 0.9134 - val_loss: 0.3374 - val_accuracy: 0.9386\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.4263 - accuracy: 0.9140 - val_loss: 0.3361 - val_accuracy: 0.9385\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.4163 - accuracy: 0.9172 - val_loss: 0.3552 - val_accuracy: 0.9327\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4144 - accuracy: 0.9188 - val_loss: 0.3506 - val_accuracy: 0.9324\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.4069 - accuracy: 0.9221 - val_loss: 0.3578 - val_accuracy: 0.9315\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.4072 - accuracy: 0.9218 - val_loss: 0.3421 - val_accuracy: 0.9366\n",
      "iteration:  0\n",
      "Test loss: 0.3420711921249787\n",
      "Test accuracy: 0.9365780353546143\n",
      "1\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3715 - accuracy: 0.9240 - val_loss: 0.3507 - val_accuracy: 0.9385\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3632 - accuracy: 0.9253 - val_loss: 0.3789 - val_accuracy: 0.9298\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3569 - accuracy: 0.9263 - val_loss: 0.3599 - val_accuracy: 0.9341\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3553 - accuracy: 0.9270 - val_loss: 0.3409 - val_accuracy: 0.9393\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3550 - accuracy: 0.9261 - val_loss: 0.3844 - val_accuracy: 0.9267\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3506 - accuracy: 0.9279 - val_loss: 0.3422 - val_accuracy: 0.9360\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3463 - accuracy: 0.9287 - val_loss: 0.3702 - val_accuracy: 0.9302\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3490 - accuracy: 0.9280 - val_loss: 0.3746 - val_accuracy: 0.9296\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3439 - accuracy: 0.9294 - val_loss: 0.3761 - val_accuracy: 0.9279\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3430 - accuracy: 0.9286 - val_loss: 0.3646 - val_accuracy: 0.9351\n",
      "iteration:  1\n",
      "Test loss: 0.36464176179993424\n",
      "Test accuracy: 0.9350798726081848\n",
      "2\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3483 - accuracy: 0.9287 - val_loss: 0.3586 - val_accuracy: 0.9352\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3402 - accuracy: 0.9294 - val_loss: 0.3799 - val_accuracy: 0.9292\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3397 - accuracy: 0.9294 - val_loss: 0.4206 - val_accuracy: 0.9159\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3337 - accuracy: 0.9308 - val_loss: 0.3614 - val_accuracy: 0.9343\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3321 - accuracy: 0.9314 - val_loss: 0.3711 - val_accuracy: 0.9297\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3342 - accuracy: 0.9310 - val_loss: 0.3666 - val_accuracy: 0.9292\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3284 - accuracy: 0.9322 - val_loss: 0.3970 - val_accuracy: 0.9271\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3300 - accuracy: 0.9317 - val_loss: 0.3807 - val_accuracy: 0.9276\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3274 - accuracy: 0.9326 - val_loss: 0.3748 - val_accuracy: 0.9277\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3268 - accuracy: 0.9322 - val_loss: 0.4212 - val_accuracy: 0.9167\n",
      "iteration:  2\n",
      "Test loss: 0.4212496008382814\n",
      "Test accuracy: 0.9166794419288635\n",
      "3\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3619 - accuracy: 0.9216 - val_loss: 0.4021 - val_accuracy: 0.9176\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3541 - accuracy: 0.9234 - val_loss: 0.4087 - val_accuracy: 0.9186\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3516 - accuracy: 0.9247 - val_loss: 0.4159 - val_accuracy: 0.9203\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3487 - accuracy: 0.9242 - val_loss: 0.4239 - val_accuracy: 0.9140\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3423 - accuracy: 0.9247 - val_loss: 0.4085 - val_accuracy: 0.9182\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3461 - accuracy: 0.9248 - val_loss: 0.4060 - val_accuracy: 0.9204\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3430 - accuracy: 0.9253 - val_loss: 0.4391 - val_accuracy: 0.9174\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3478 - accuracy: 0.9247 - val_loss: 0.3854 - val_accuracy: 0.9265\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3408 - accuracy: 0.9270 - val_loss: 0.4163 - val_accuracy: 0.9156\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3399 - accuracy: 0.9263 - val_loss: 0.3998 - val_accuracy: 0.9217\n",
      "iteration:  3\n",
      "Test loss: 0.3997723097788753\n",
      "Test accuracy: 0.9217117428779602\n",
      "4\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 110s 48ms/step - loss: 0.3308 - accuracy: 0.9284 - val_loss: 0.4018 - val_accuracy: 0.9196\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3256 - accuracy: 0.9303 - val_loss: 0.4021 - val_accuracy: 0.9274\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3214 - accuracy: 0.9307 - val_loss: 0.4112 - val_accuracy: 0.9188\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3209 - accuracy: 0.9311 - val_loss: 0.3731 - val_accuracy: 0.9285\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3164 - accuracy: 0.9323 - val_loss: 0.3870 - val_accuracy: 0.9230\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3190 - accuracy: 0.9309 - val_loss: 0.3963 - val_accuracy: 0.9221\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3182 - accuracy: 0.9309 - val_loss: 0.4106 - val_accuracy: 0.9204\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3204 - accuracy: 0.9314 - val_loss: 0.4229 - val_accuracy: 0.9149\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3158 - accuracy: 0.9329 - val_loss: 0.4300 - val_accuracy: 0.9187\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3165 - accuracy: 0.9317 - val_loss: 0.4388 - val_accuracy: 0.9124\n",
      "iteration:  4\n",
      "Test loss: 0.4388245713164928\n",
      "Test accuracy: 0.9124155044555664\n",
      "5\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3332 - accuracy: 0.9272 - val_loss: 0.4257 - val_accuracy: 0.9142\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3342 - accuracy: 0.9268 - val_loss: 0.4577 - val_accuracy: 0.9083\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3286 - accuracy: 0.9293 - val_loss: 0.4291 - val_accuracy: 0.9151\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3253 - accuracy: 0.9288 - val_loss: 0.3912 - val_accuracy: 0.9241\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 103s 45ms/step - loss: 0.3251 - accuracy: 0.9292 - val_loss: 0.3999 - val_accuracy: 0.9218\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3235 - accuracy: 0.9295 - val_loss: 0.4638 - val_accuracy: 0.9092\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3202 - accuracy: 0.9295 - val_loss: 0.4941 - val_accuracy: 0.8991\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3213 - accuracy: 0.9287 - val_loss: 0.4810 - val_accuracy: 0.8978\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3239 - accuracy: 0.9309 - val_loss: 0.4316 - val_accuracy: 0.9137\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3170 - accuracy: 0.9309 - val_loss: 0.4473 - val_accuracy: 0.9116\n",
      "iteration:  5\n",
      "Test loss: 0.44725059363910885\n",
      "Test accuracy: 0.9116088151931763\n",
      "6\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3302 - accuracy: 0.9273 - val_loss: 0.4551 - val_accuracy: 0.9054\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3214 - accuracy: 0.9290 - val_loss: 0.4684 - val_accuracy: 0.9022\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3216 - accuracy: 0.9271 - val_loss: 0.4655 - val_accuracy: 0.9077\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3194 - accuracy: 0.9290 - val_loss: 0.4358 - val_accuracy: 0.9103\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3179 - accuracy: 0.9287 - val_loss: 0.4663 - val_accuracy: 0.9024\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3178 - accuracy: 0.9290 - val_loss: 0.4292 - val_accuracy: 0.9135\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3177 - accuracy: 0.9286 - val_loss: 0.4534 - val_accuracy: 0.9073\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3138 - accuracy: 0.9291 - val_loss: 0.4189 - val_accuracy: 0.9109\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3165 - accuracy: 0.9295 - val_loss: 0.4152 - val_accuracy: 0.9185\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3134 - accuracy: 0.9288 - val_loss: 0.4529 - val_accuracy: 0.9088\n",
      "iteration:  6\n",
      "Test loss: 0.4529411217498867\n",
      "Test accuracy: 0.9087661504745483\n",
      "7\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3202 - accuracy: 0.9275 - val_loss: 0.4233 - val_accuracy: 0.9148\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3143 - accuracy: 0.9280 - val_loss: 0.4724 - val_accuracy: 0.8979\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3147 - accuracy: 0.9290 - val_loss: 0.4592 - val_accuracy: 0.9078\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3144 - accuracy: 0.9283 - val_loss: 0.4767 - val_accuracy: 0.8963\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3131 - accuracy: 0.9291 - val_loss: 0.4255 - val_accuracy: 0.9087\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3139 - accuracy: 0.9291 - val_loss: 0.4399 - val_accuracy: 0.9097\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3081 - accuracy: 0.9311 - val_loss: 0.4209 - val_accuracy: 0.9121\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 107s 47ms/step - loss: 0.3091 - accuracy: 0.9311 - val_loss: 0.5296 - val_accuracy: 0.8905\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3092 - accuracy: 0.9300 - val_loss: 0.4520 - val_accuracy: 0.9014\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 108s 47ms/step - loss: 0.3096 - accuracy: 0.9296 - val_loss: 0.4716 - val_accuracy: 0.9004\n",
      "iteration:  7\n",
      "Test loss: 0.471568818061573\n",
      "Test accuracy: 0.9004302620887756\n",
      "8\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3195 - accuracy: 0.9267 - val_loss: 0.4671 - val_accuracy: 0.8998\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3151 - accuracy: 0.9283 - val_loss: 0.4240 - val_accuracy: 0.9087\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3114 - accuracy: 0.9292 - val_loss: 0.4628 - val_accuracy: 0.9039\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3128 - accuracy: 0.9281 - val_loss: 0.4624 - val_accuracy: 0.9001\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3132 - accuracy: 0.9293 - val_loss: 0.4895 - val_accuracy: 0.8994\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3094 - accuracy: 0.9290 - val_loss: 0.5477 - val_accuracy: 0.8791\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3054 - accuracy: 0.9310 - val_loss: 0.4843 - val_accuracy: 0.8986\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3063 - accuracy: 0.9301 - val_loss: 0.4632 - val_accuracy: 0.9060\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3074 - accuracy: 0.9301 - val_loss: 0.4323 - val_accuracy: 0.9088\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3058 - accuracy: 0.9308 - val_loss: 0.4528 - val_accuracy: 0.8999\n",
      "iteration:  8\n",
      "Test loss: 0.45282883531818674\n",
      "Test accuracy: 0.8999308347702026\n",
      "9\n",
      "Epoch 1/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 46ms/step - loss: 0.3146 - accuracy: 0.9282 - val_loss: 0.4643 - val_accuracy: 0.8975\n",
      "Epoch 2/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3146 - accuracy: 0.9275 - val_loss: 0.4719 - val_accuracy: 0.9009\n",
      "Epoch 3/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3129 - accuracy: 0.9291 - val_loss: 0.4855 - val_accuracy: 0.8958\n",
      "Epoch 4/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3077 - accuracy: 0.9288 - val_loss: 0.4458 - val_accuracy: 0.9024\n",
      "Epoch 5/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3066 - accuracy: 0.9305 - val_loss: 0.4441 - val_accuracy: 0.9009\n",
      "Epoch 6/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3061 - accuracy: 0.9298 - val_loss: 0.4685 - val_accuracy: 0.8899\n",
      "Epoch 7/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 106s 46ms/step - loss: 0.3074 - accuracy: 0.9299 - val_loss: 0.4422 - val_accuracy: 0.9125\n",
      "Epoch 8/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 105s 46ms/step - loss: 0.3051 - accuracy: 0.9298 - val_loss: 0.4586 - val_accuracy: 0.8989\n",
      "Epoch 9/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3056 - accuracy: 0.9303 - val_loss: 0.5153 - val_accuracy: 0.8926\n",
      "Epoch 10/10\n",
      "Learning rate:  0.001\n",
      "2290/2290 [==============================] - 104s 45ms/step - loss: 0.3018 - accuracy: 0.9314 - val_loss: 0.4627 - val_accuracy: 0.9014\n",
      "iteration:  9\n",
      "Test loss: 0.46270581781314646\n",
      "Test accuracy: 0.9013906121253967\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAEWCAYAAAAQKVIQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5wU9f3H8dfnGkfvoNJBLKgIgoKAgiEaKyqYiC127Kap0ZjERGNMlPzUKKJoNNFYYk3QqGgQVEDUQwWkI9LbIb0ed/f5/TFzuKx7bdnbvdt7Px+PfdxO/8zczHz2+535zpi7IyIikq4yUh2AiIhIVVKiExGRtKZEJyIiaU2JTkRE0poSnYiIpDUlOhERSWtKdCIRzOw+M/vGzBYnebl/N7NbkrnMcLk/NbO1ZrbVzOone/nVjZkdYmaFqY6jtqnq/b/cRGdmi81sR3ggrA4DalBVAVVEGNP345y2fbguJR83s20R3cftQ1yrzWxAVL+/m9kCMys2s+HxzjtqnieHcd+YiPlJwMy6AtcAXd29YxUu52oz+19kP3e/xN3vrapllhJHfeDPwHHu3sDdtyVgni+Y2a/3PbpS5391uO/fENV/nZn1rarllhLLyWa2MJnLrI4q+z9Pxf5f0RLdGe7eAOgB9ARuq6qAqpq7Lw0P6gbhOgEcGdHvwwQv8jNgBPBlAud5MbA+/JtUZpaV7GUmUQdgtbuvT3UgSbI/kOnu8yoycjX6368HbjezeqkORGoIdy/zAywGvh/RfS/w34juOsBIYCmwBngUqBsOawG8AWwk2Dk/BDIi5nsTMAPYBPwLyI2Y7+nAF+G0U4DuYf9ngGJgB7AVuKW8dShn/Rw4MKpfXeABYBmwGngIqBMO2w94O4zrG+C9sP9LYVzbw7hujJpnHjB8X2IN59M4XMYPgULg8Kjhg4Cp4TZdCpwf9q8P/DVcp03A+0AWcDKwMGoeq4EB4fc/Ac+F/58twIVAf+DjcD4rgfuBrIjpjwTeAzaE8/oF0B7YBjSKGK9/OH1GjPUsdRlAJvAwkB8Onw4cXMr2ugqYG8a+ELislPFOD/ep4vD/92gFt82zwPPh/GcAPSLG7Qj8B1gXfv5C8ENxZ/i/20qQWAFeAH4dMe11wFfhPvYq0Drsn0uwz14ZDt8A3F/G/lIXGAWsApYD9wHZwBHh/8PDON6KMe0hYZxXhvvNO+E+8wrBsb4RmFCy7YEbgd3ArnCeL4X920Vsh0XA1fuw/18N/A94F/hlRP91QN+y1rmU+WUBD4bbeSFwA1BY3v4DNI/aX7aG/co8NmIsO+a2DIdPBS6MXveI7tOABeG0D0SOH477HsFxsikcrzfBj+4V4TKHR8yrrHPeyeG6/4rgmFsBXFDO//y3wNfhdvsSOC3sn9T9f8/8KrBjLSZMdEBbYCbwYMTwB4CxQDOgIfA6cE847B6CE0Z2+DkOsIj5fgIcEE47h/AAAI4C1gJ9CE5qF4fj14mOaV8/xE50jwIvA00IEss44I5w2P0EB0YWkAMcH+skGGM5iUp0VwJLACM42O+NGHZguPMMC+NrSVBaBfgbwYlqv3CbHhf+rcjJfBdwKkENQF3gGODocPouBAdByf+uKcHBcD3Bj6BGwNHhsPeASyOWMxq4r5T1LGsZZwIfhfPOAA4DWpUynyFAp3B7fZ/g5HRYKePutS0quG22AyeGcd4PTAyHZYf79J+AeuF26xfrhBV9oIfbejXQneDAHgO8G3WgvxqufyeCE92gUtbpXoIfmC2A1sCnwO3hsEOIOKnHmPaQcFlPRKxDFsHx2CCMZTQwNdZ6hN2ZBOeMXxIcLwcR/AAbGOf+X5Lo+hCcBBuF/SMTXanrHGN+Pw3jO4DgeJnE3omu1P2nlP2j1P02xrLL25alJjqC43grwQ+0bOAWgoQTmeh2A+eHy7mP4Lxxf/h/GEKQJHIrcM47OZzX7eGyziZIYA1i/c/DfucS1BhkABeF47dI9v6/Z/4V2LEWhxt0S7iA8UCTcJgR/CrsEjH+scDX4fc7CX7JHVjKfCP/ifcCj0acAO+KGn8e4cFBFSa6cKcoANpE9DsBmBMR50tA5xjzSkaimwT8Kfx+KcGvxsyw+/fA8zGmyQ531O+UeqjYyfydcmK6tWS5YUwflTLexcD48HsOwYmqewXXO3IZpwKzCE4qVsnt9zZwVSnD4kl0b0QMOwrYGLHPrCB2abW8A/1Z4M6IYU0ISg778e2B3jti+Fjgp6Ws0wrgexHdZwJzw+8VTXQHlDHOfmFsudHrEXYPBBZETfN7YHSc+3/kyX4s8Pvwe2SiK3WdY8xvCnBJRPeQcrbJnv0n1v5R1n5bgXWL3pZlJboRwISIYRkEhYPIRDczYvjR4f+ycUS/beH/uLxz3skEpcKMiOGbCWsvov/npazbXOAHyd7/Sz4VvUZ3lrs3JKgWO4TglxIEv4DqAdPMbKOZbQx3hJbh8PsIftG8Y2aLzOzWqPmujvi+neCXDQTXSn5RMs9wvu0IfnWVy8zeiri55IIKrmOJAwgSw6yIZf8baBUOv5sguUwws4Vm9vNKzr+0mA+KiHldKeN0IagaeTbs9QpBCerEsLsdQXE+2v4EO/OiOMNbFhVHt3AbrzGzzQTVFCX7RGkxlMR7tJm1IUhWy919RqwRy1nGWwQl1MeANWb2SGk3SJnZEDP7xMzWh//L70XMJxFK24fbEfzgK45jngcQ/PoGwN03EpxY2lRguXuYmRGcHJZE9F4SNZ/yFLv7yoh5ZpnZyPB43kxwAjOCartYOgAdo47ln4dxRcdb7jEQ5TfAT8xsz7LjWOcD2Hv/jpyu0vtPOftt9LiV3Zalxh3uZyuixlkT8X0HsMvdN0X1a0D55zyA/Kh9OeY+F7Ful5vZjIj5HUjFj7uE7P+RKtW8wN3fB/5OcE0Ogl9RJUX5JuGnsYc3ebj7Fnf/hbt3Bs4Afm5mgyuwqGXA3RHzbOLu9dz9+ZJQyonzFP/25pJnyxo3hlUE9cddotapeTjvTe7+E3fvQFBF+Gsz61+RuMqJeX5EzKXtEBeHf981s9XAfIIE9uOw/zKC6pLS1qlzjGHbCH6sAGBm2QRVyXuFF9X9OMFNNl3cvRFByd3KiQF33wq8RlCdchHB9dbSlLoMD/yfu/ckqN44EvhJ9AzCuwpfAu4iqNpsQlB9atHjlqIi26Y0ywhO8LGOsfL2k5UECaJkuY0JqmmiT2Rl8uDn7urIeRFcK63MfKJjvRQ4ieAXf2OCH77w7TaNHn8ZQWkq8lhu6O5nx4i3IsdA5PjTCX5Y/zKiX2XXeRXBj5LIcYMVKn//ifV/LOvYiFbettxr/2PvHwerCC4llcSaQeV+wEQq85xXAXttBzM7iOAa3wigWbjdFlL2douUkP0/Ujzt6B4ATjSzHmGGfxy438xahUG1MbMfhN9PN7MDw19Zm4Gi8FOex4GrzayPBeqb2Wlm1jAcvobYJ+195u67gSeBB82sRbj8dmZ2Iuz5hdcpXKdN7L1O34nLzHLMLJfgn5xtZrnhtJUSTnMRwQXhHhGf84GzzKwR8DRwupmdbWaZZtbSzLqH6/R0uE6tw2EDzCyT4DpSMzMbHJ7If0/5+0VDYJO7bzWzwwiuG5b4N3CgmV0TrnsjMzs6YvjTwBUE1SFl/QgpdRlm1tfMeltwF+A2gmqXWPtVXYJfqmuBYjMbQlArUVHxbJsSkwiq++8ys3pmVtfM+oXD1gDtwnnG8jxwpZkdHu47fya46Wl1KeOX5XngDjNrHh6jtwP/jGM+JRoS3EzwDcENTn+IGh59DEyCPe31csNSTHczO2ofYoh0B0FVWGRCqMw6vwj8zMz2N7MWBNe6SpS3/6wBWkXVJpR1bEQrb1t+AZwTbrdDgEsiho0F+pjZqeFx8HOC2p1KK++cVwHR//MGBFWN+UCGmV1NUKKLHD9Z+z8QR6Jz93yCk9Vvwl6/JMjWU8Pi9/+Ag8NhXcPurQQ3Dzzi7hMrsIw8gh3kYYILpgvZ+598D0FJaqOZ3VTZdaiAnxL8qsgjSGZv8+0/6lBgIsFJ7ANgpLtPDYfdDdwdxnV92O8DglLvUQTbbQfBhfTKGkhQlTDa3VeXfAguIK8AfuTuXxFcj/gVwXbLI7hRA4K7o74CPic4sO4iuL61jqA09CzBHWqrCUrqZfkZcIWZbSW4u+1fJQPcfQNBVepwghPEPCCybeEEghPIJHdfFc8yCOrs/05wEXoRQTXHX6NnEK7bTQQ3SH0DnAW8Wc66RU9f2W1TMu1ugurZI8NplwJDw8FvE1xnXmtmy2NM+wbBPj6WYD/cj+BHTjx+C8wmuKb5BTCZ4DpzvP5GcAJbTXATx6So4WMIqqc3mtkLEduhH8H/KZ/gGnxC2uJ60DTiZYJ9qkRl1vlhghtXZhHcLflixLzL23+mE/yPloTr24yy99to5W3LewlqbPIJtuueZB0eO+cR7Pfr+PZGwV1lLK8sZZ3zyhP9P/+M4OaWPILSYqfwe4lk7v/At3dAiiSNmU0h+NGzLyULEQmFpbrVBG2eP0p1PNWNHgEmSRVezzyI4MYUEYmTmZ1iZo3D6r07CG7KmJbisKolJTpJGjN7geABAje6+45UxyNSwx1P0Ch7LTAYONvdC1IbUvWkqksREUlrKtGJiEhaqy4PaU24Fi1aeMeOHVMdhohIjTJt2rR17t6y/DFrjrRNdB07diQvL6/8EUVEZA8zW1L+WDWLqi5FRCStKdGJiEhaU6ITEZG0pkQnIiJpTYlORETSmhKdiIikNSU6ERFJa2nbji5uP/0pfPFFqqMQEYlfjx7wwAOpjqLaUIlORETSmkp00fQrSEQkrahEJyIiaU2JTkRE0lrSEp2ZnWxm88xsoZndGmN4BzMbb2YzzGyimbWNGFZkZl+En7HJillERGq+pCQ6M8sERgGnAN2A88ysW9RoI4Gn3b07cCdwT8SwHe7eI/wMSUbMf3xzDqMmLEzGokREpAolq0R3DLDQ3ReFr3p/ATgzapxuwPjw+4QYw5Pqg/n5jJu1OpUhiIhIAiQr0bUBlkV0Lw/7RZoODAu/nw00NLPmYXeumeWZ2VQzO6u0hZjZiHC8vPz8/H0KuKCwmOUbduzTPEREJPWSlegsRj+P6r4JGGhmnwMDgRVAYTisvbv3Bs4HHjCzLrEW4u5j3L23u/du2XLfXpC7q7CY9dsK2LarsPyRRUSk2kpWolsOtIvobgusjBzB3Ve6+1B37wncHvbbVDIs/LsImAj0rOqAC4qKg8BVqhMRqdGSleg+BbqaWSczywGGA3vdPWlmLcysJJ7bgCfD/k3NrE7JOEB/YHZVB1xQWJLotlf1okREpAolJdG5eyFwPTAOmAO86O6zzOxOMyu5i3IQMM/M5gOtgbvD/ocCeWY2neAmlT+5e9IS3bL1SnQiIjVZ0h4B5u5vAm9G9fttxPeXgZdjTDcFOKLKA4yiqksRkfSgJ6PEUFTsFBUH98osU9WliEiNpkQXQ0m1JahEJyJS0ynRxVCS6MyU6EREajoluhh2FRUB0LZpXTbt2M3mnbtTHJGIiMRLiS6GkhJdl5YNAFi+XqU6EZGaSokuhpJE17lFkOh0Q4qISM2lRBdDSdOCLq3qA7pOJyJSkynRxVBSomvVMJcGdbL0dBQRkRpMiS6GkkSXk5VB26Z1WaZrdCIiNZYSXQx7El1mkOhUohMRqbmU6GLYVRRZoqvH8g07cI9+q5CIiNQESnQxlJTo6oRVl1t3FbJph9rSiYjUREp0Mex9ja4egK7TiYjUUEp0MUReo2vXrC6g99KJiNRUSXtNT01SEHGNrmn9oESntnQiIjWTSnQxRFZdNq6bTaPcLD0dRUSkhlKiiyEy0QF77rwUEZGaR4kuhj1Vl5klia4uy9arRCciUhMp0cWwq3DvRNeumdrSiYjUVEp0MRQUFpOdaWRkGADtmtZlx+4i1mzeleLIRESkspToYigoLN5TmgPo3bEZAJMXrktVSCIiEicluhgKior23IgC0G3/RrRsWIcJ89amMCoREYmHEl0MBYXFeyW6jAxj0EEt+WB+PoXhjSoiIlIzKNHFEJ3oAE44pBWbdxby+bKNKYpKRETioUQXQ0HR3tfoAAZ0bUFmhjFhrqovRURqEiW6GIISXeZe/RrlZtO7Q1MmzMtPUVQiIhIPJboYdsWouoSg+nLOqs2s3rQzBVGJiEg8lOhiKCgspk5mjER3cCsAJuruSxGRGkOJLoaCotgluoNaN+CAxrlqZiAiUoMo0cUQ665LADNj0CGtmLRg3Z4HP4uISPWW1ERnZieb2TwzW2hmt8YY3sHMxpvZDDObaGZto4Y3MrMVZvZwVcYZ/WSUSCcc3IptBUXkLV5flSGIiEiCJC3RmVkmMAo4BegGnGdm3aJGGwk87e7dgTuBe6KG3wW8X9WxllZ1CdCvS3NyMjNUfSkiUkMks0R3DLDQ3Re5ewHwAnBm1DjdgPHh9wmRw82sF9AaeKeqAy2t6hKgfp0s+nRupmYGIiI1RDITXRtgWUT38rBfpOnAsPD72UBDM2tuZhnAX4Cby1qAmY0wszwzy8vPjz8RlZXoAAYd3IqFa7fqHXUiIjVAMhOdxegX/YK3m4CBZvY5MBBYARQC1wJvuvsyyuDuY9y9t7v3btmyZdyBlnWNDuCEg4N5q5mBiEj1l5XEZS0H2kV0twVWRo7g7iuBoQBm1gAY5u6bzOxY4DgzuxZoAOSY2VZ3/84NLYmwq6iYOmWU6Dq1qE+H5vWYMC+fi47tWBUhiIhIgiQz0X0KdDWzTgQlteHA+ZEjmFkLYL27FwO3AU8CuPsFEeNcAvSuqiTn7uVWXZoZJxzcihc+XcrO3UXkZmeWOq6IiKRW0qou3b0QuB4YB8wBXnT3WWZ2p5kNCUcbBMwzs/kEN57cnaz4SuwuCmpTy6q6BBh0cEt27i5m6qJvkhGWiIjEKZklOtz9TeDNqH6/jfj+MvByOfP4O/D3KggPCJoWAGWW6AD6dm5ObnYGE+flMyh8NJiIiFQ/ejJKlN2FFUt0udmZ9OvSgvfmrsU9+p4aERGpLpToolS0RAfB3ZdL12/n63XbqjosERGJkxJdlJJnWJZ3jQ7YU2WpxuMiItWXEl2UXRWsugRo16weB7ZqoPZ0IiLVmBJdlJISXVnt6CKdcHBLPl60nm27CqsyLBERiZMSXZTKXKOD4G0GBUXFTPlKzQxERKojJboo316jq1gj8N4dm1E/J1NvMxARqaaU6KIUVOIaXcl4A7q24P15+WpmICJSDSnRRSkoKgIqnuggqL5csXEHC9ZuraqwREQkTkp0USrTvKDEnmYGc/e9+rKwqJh/fbqUFRt37PO8REREie47KtO8oMR+jXM5dP9GCblO9/K05fzylZmcMHIi97w5h03bd+/zPEVEajMluiiVbV5Q4oSDW5K3eAObd8afmAqLihn9/lccun8jzuh+AGM+XMTx901gzAdfsXN3UdzzFRGpzRKS6MxsTCLmUx1UtnlBiRMOaUVhsTN5wbq4l/3GjFUs+WY7Pxnclb/86Ej+e8Nx9GjXhD++OZfBf3mfVz9bTnGxbngREamMCp/NzaxZKZ/mwKlVGGNSxXONDqBnuyY0ys2Ku/qyuNgZNWEhB7VuwEndWgPQ7YBG/OOyY3j2ij40rZ/Nz1+czukPTeKD+XrkmIhIRVXmNT35wBLAIvp52J0276mpbPOCElmZGRx3UEsmhM0MzKz8iSK8M3sNC9Zu5cHhPcjI2Hva/ge2YOx1A3h9xkruGzePHz/5CQMObMGtpxzC4W0aV2o5IiK1TWXO5ouAQe7eKeLT2d07AWuqKL6kizfRQdDMIH/LLj5ftrFS07kHpbkOzetx2hH7xxwnI8M4s0cbxv9iIL85vRuzVm7i9Icm8dMXPmfZ+u2VjlVEpLaozNn8AaBpKcPuTUAs1UJudiatGtYhK6NyJTKA7x/aiub1c7j9tS/ZVVjxm0fen5/PzBWbuHZQF7LKqTKtk5XJ5QM68f4tJ3DtoC689eVqBv/lff7wxmw2bCuodMwiIunO0vVpHr179/a8vLykL3f8nDVc/o88rhjQiV+f3q1C0/zw0Sms2LCDiTefUOmS5KpNO7j/3fm8PG059etkce2gA7m0f0dysyv2CDMRkUhmNs3de6c6jkRS84IEG3xoay7q24EnJn1doZtGPl70DZ8u3sCI4zvHVV26f+O63HvOkbz1k+M5umMz/vz2XE4YOZGX8pZRpDs0RUSU6KrC7acdyoGtGvCLl6azvpzqxIcnLKRFgxyGH9N+n5Z58H4NefKSo3n+yr60aliHm1+ewWl//ZAJ89bqGZwiUqsp0VWB3OxM/jq8J5u27+aWl2eUmmimL9vIhwvWccVxnRNW1Xhsl+b8+7r+PHx+T3bsLuLSpz7l/Mc/Zsbyyt0gIyKSLuJKdGY21Mz+z8z+YmZnJzqodNDtgEbccvLB/G/OGp77ZGnMcR6esJDGdbO5sG+HhC7bzDi9+wG8+7OB/O6Mbsxbs4UhD0/mhuc/Z+k3ukNTRGqXSic6M3sEuBqYCXwJXGVmoxIdWDq4rH8njuvagrvemM3CtVv2GjZ39Wbenb2GS/p1pEGdyjRnrLicrAwu6d+J928exA3fO5B3Z69m8P9N5HdjZ5VbpSoiki7iKdENBH7g7k+5+1MET0UZlNCo0kRGhvGXHx5J3exMbnz+i72aHDwy4Svq52Ryaf+OVR5Hw9xsfnHSwbx/8wmc06stT3+0mIH3TmDUhIXsKNAzNEUkvcWT6OYBkXdOtANmJCac9NOqUS73nnMks1dt5i/vzAfg63XbeGPGSi48tgNN6uUkLZbWjXK5Z2h3xv30ePp0bs594+YxaOQE/vXpUgrDZ3yKiKSbeBJdc2COmU00s4nAbKClmY01s7EJjS5NnNitNRf0ac+YDxYxacE6Rk9cSHZmBlcM6JySeLq2bsgTF/fmxauO5YAmdfnlKzM55cEPGT9nje7QFJG0U+kG42Y2sKzh7v7+PkWUIKlqMF6aHQVFnP7Qh2zeWciGbQVc0Kc9vz/z8FSHhbvz9peruXfcPL5et41jOjXjtlMOoWf70h6CIyLpLB0bjMf1ZBQzaw0cHXZ+4u77/sbRBKtuiQ7gyxWbOPuRyQBMvPkE2jSpm+KIvrW7qJgXPl3Gg/+bz7qtBZx6xH7c/IND6NSifqpDE5EkUqIDzOxHwH3ARII3FxwH3OzuLyc8un1QHRMdwLhZq9mys5BzerVNdSgxbd1VyOMfLOLxDxdRUFjM+X3ac+PgrrRoUCfVoYlIEijRAWY2HTixpBRnZi2B/7n7kVUQX9yqa6KrKdZu2cmD/1vAC58uIzcrg6sGduGK4zpRL6dqmkKISPWQjokunptRMqKqKr+p6HzM7GQzm2dmC83s1hjDO5jZeDObEd7s0jai/zQz+8LMZpnZ1XHELZXQqmEud599BO/87HgGdG3B/707n4H3TeTZj5foDk0RqVHiKdHdB3QHng97nQvMdPdbypkuE5gPnAgsBz4FznP32RHjvAS84e7/MLPvAZe6+0VmlhPGusvMGhA0VO/n7itLW55KdIk1bcl67nlzLnlLNtClZX1uOfkQTurWutIvmBWR6k0lOsDdbwYeI0h2RwJjyktyoWOAhe6+yN0LgBeAM6PG6QaMD79PKBnu7gXuvivsXyeeuGXf9OrQjJeuPpYxF/XCgauemcYPH/2IaUs2pDo0EZEyxfMIsD+7+6vu/nN3/5m7v2Zmf67ApG2AZRHdy8N+kaYDw8LvZwMNzax5uNx2ZjYjnMefyyrNSdUwM046bD/e+enx/PHsI1iyfjvDRk/hqmfy+Cp/a6rDExGJKZ6S0Ykx+p1Sgeli1XFF15veBAw0s88JHjW2AigEcPdl7t4dOBC4OGzisPcCzEaYWZ6Z5eXnl/8uOIlPVmYG5/dpz/s3D+LnJx7EpAXrOOn+D7j9tZms3bIz1eGJiOylwonOzK4xs5nAweHNIiWfr6nYI8CWEzwurERbYK9SmbuvdPeh7t4TuD3styl6HGAWQbMGooaNcffe7t67ZcuWFV01iVO9nCxuHNyV9285gQv7tOdfny5j0H0Tuf/d+WzdVZjq8EREgErcjGJmjYGmwD1A5B2TW9x9fQWmzyK4GWUwQUntU+B8d58VMU4LYL27F5vZ3UCRu/82vPvyG3ffYWZNgY+BYe4+s7Tl6WaU5Pt63TZGjpvHf2euokWDHH4yuCvDj2lPdqYuqYrUFLX6ZhR33+Tui939PHdfEvEpN8mF0xcC1wPjgDnAi+4+y8zuNLMh4WiDgHlmNh9oDdwd9j8U+Dhsw/c+MLKsJCep0alFfUZdcBSvXduPzi0b8Jv/zOKk+z/gs6W6YUVEUieuR4DVBCrRpZa7897ctfzu9Vms2bSLu846jHOPbl/+hCKSUrW6RCdSGWbG4ENb8/r1A+jTuRm/fGUmv/n3lxQUqrG5iCRXPM0Lrg+vk4mUq0m9HJ665GiuOr4zz0xdwoVPfEz+ll3lTygikiDxlOj2Az41sxfDR3rp0RhSpqzMDG479VAeHN6DGSs2MuThScxYvjHVYYlILRHPk1F+DXQF/gZcAiwwsz+aWZcExyZp5swebXj56n5kmHHOox/xyrTlqQ5JRGqBuK7ReXAHy+rwU0jQ7OBlM7s3gbFJGjq8TWNev2EAvdo35RcvTef3r89itx4SLSJVKJ5rdDea2TTgXmAycIS7XwP04tvHd4mUqln9HJ65/Bgu69+JpyYv5sd/+4T12wpSHZaIpKl4SnQtgKHu/gN3f8nddwO4ezFwekKjk7SVlZnBb8/oxl9+eCTTlm7gjIcm8eWKTeVPKCJSSfEkujeBPY3EzayhmfUBcPc5iQpMaodhvdry8tXHUuzOOY9O4T9frEh1SCKSZuJJdKOByEfVbwv7icSle9smjL1+AN3bNOEnL3zBPW/Ooag4PR9kICLJF0+iM494nEpYZZmVuJCkNmrZsA7/vKIPPz62A499sIhLnvqEjdt13U5E9l08iW5ReENKdvj5CbAo0YFJ7ZOTlcGdZx7On4cdwceL1jPk4cnMXb051WGJSA0XT6K7GuhH8NG62v4AABcvSURBVAaC5UAfYEQig5La7dyj2/PCVX3ZubuIoY9M4c2Zq1IdkojUYPE0GF/r7sPdvZW7t3b38919bVUEJ7XXUe2b8sYNAzhkv4Zc++xn3Ddurq7biUhcKn1tzcxygcuBw4Dckv7uflkC4xKhVaNcnh/Rl9+NncWoCV8xe+VmHhjek8Z1s1MdmojUIPFUXT5D8LzLHxC8G64tsCWRQYmUqJOVyT1Du3P32Yfz4YJ1nDVqMgvWaHcTkYqLJ9Ed6O6/Aba5+z+A04AjEhuWyN4u6NOB50f0ZcvOQs4aNZlxs1anOiQRqSHiSXS7w78bzexwoDHQMWERiZTi6I7NeP2G/hzYqgFXPTON+9+dT7Gu24lIOeJJdGPC99H9GhgLzAb+nNCoREqxf+O6/OuqYxl2VFseHL+AEc9MY8vO3eVPKCK1VqUSnZllAJvdfYO7f+DuncO7Lx+rovhEviM3O5ORP+zO787oxoR5azlr1GQW5W8tf0IRqZUqlejCp6BcX0WxiFSYmXFJ/0788/I+bNi+mzMfnsx7c9ekOiwRqYbiqbp818xuMrN2Ztas5JPwyEQq4NguzRl7fX/aN6/H5f/I4+H3FhDxhDoREayyJwUz+zpGb3f3zokJKTF69+7teXl5qQ5DkmRHQRG3vTqDf3+xklMO34+RPzyS+nX0CFaRyjKzae7eO9VxJFKlzwTu3qkqAhHZF3VzMrn/3B4c3qYxf3xzDovytzHmx73o0Lx+qkMTkRSL58koP47V392f3vdwROJnZlxxXGcO2a8R1z//GWc8NImHzj+KgQe1THVoIpJC8VyjOzricxzwO2BIAmMS2ScDurbg9esHcECTulz61Cc8+v5Xum4nUovFU3V5Q2S3mTUmeCyYSLXRrlk9Xr22Hze/PIM/vTWXL1ds4t5zulMvR9ftRGqbeEp00bYDXRMwH5GEqpeTxcPn9eSXJx/Cf2euYtjoj1i2fnuqwxKRJKt0ojOz181sbPh5A5gH/CfxoYnsOzPjmkFdeOqSo1mxYTtDHp7E5IXrUh2WiCRRPM0LBkZ0FgJL3H15QqNKADUvkGiL121jxDN5fJW/jV+deiiX9e+ImaU6LJFqJR2bF8RTdbkU+Njd33f3ycA3ZtYxoVGJVIGOLerz6rX9+f6hrbjrjdn8/MXp7NxdlOqwRKSKxZPoXgKKI7qLwn4i1V6DOlmMvqAXvzjxIP79xQrOeXQKKzbuSHVYIlKF4kl0We5eUNIRfs8pbyIzO9nM5pnZQjO7NcbwDmY23sxmmNlEM2sb9u9hZh+Z2axw2LlxxCyyR0aGccPgrjx+UW+WrNvOkIcm8fGib1IdlohUkXgSXb6Z7Wk3Z2ZnAmVe3TezTGAUcArQDTjPzLpFjTYSeNrduwN3AveE/bcDP3b3w4CTgQfMrEkccYvs5fvdWvPv6/vTuF42FzzxMf+Ysljt7UTSUDyJ7mrgV2a21MyWAr8EripnmmOAhe6+KCwBvgCcGTVON2B8+H1CyXB3n+/uC8LvK4G1gB51IQnRpWUD/n1dfwYd3JI7xs7il6/M0HU7kTRT6UTn7l+5e1+CxHSYu/dz94XlTNYGWBbRvTzsF2k6MCz8fjbQ0MyaR45gZscQVJN+FWshZjbCzPLMLC8/P79iKyS1XqPcbMZc1JsbB3flxbzlnDtmKqs37Ux1WCKSIPG0o/ujmTVx963uvsXMmprZH8qbLEa/6Dqim4CBZvY5MBBYQdB8oWS5+xM8geXS8L14352h+xh37+3uvVu2VKFPKi4jw/j5iQfx6IW9WLhmC6c/NIm8xetTHZaIJEA8VZenuPvGkg533wCcWs40y4F2Ed1tgZWRI7j7Sncf6u49gdvDfpsAzKwR8F/g1+4+NY6YRSrk5MP347Xr+tOgTibnPT6V5z5emuqQRGQfxZPoMs2sTkmHmdUF6pQxPsCnQFcz62RmOcBwYGzkCGbWwsxK4rkNeDLsnwO8RnCjipoxSJU7qHVD/nPdAPof2IJfvTaTX702k4LCmJUIIlIDxJPo/gmMN7PLzewy4F2gzFf0uHshcD0wDpgDvOjus8zszog7OAcB88xsPtAauDvs/yPgeOASM/si/PSII26RCmtcL5u/XXw01w7qwnMfL+W8x6eydrOu24nURJV+BBgEbeKA7xNce3vH3cclOrB9pUeASaK8MWMlN780g0Z1s3j0wl70bN801SGJVBk9Aizk7m+7+03u/gtgq5mNSnBcItXG6d0P4NVr+5GTlcG5j03lxbxl5U8kItVGXIkufFrJn81sMfAHYG5CoxKpZg7dvxFjrxvAMZ2accvLM7jjP1+yu0jX7URqggq/hdLMDiK4ieQ84BvgXwRVnydUUWwi1UrT+jn8/dKjuXfcPMZ8sIg5q7fwyAVH0aJBefdiiUgqVaZENxcYDJzh7gPc/SGCBzqL1BpZmRn86tRDeXB4D6Yv28iQhyYxc/mmVIclImWoTKIbBqwGJpjZ42Y2mNgNwUXS3pk92vDKNf0wM855dAqvflbtXskoIqEKJzp3f83dzwUOASYCPwNam9loMzupiuITqbYOb9OYsdf3p2f7Jvz8xenc9cZsCnXdTqTaiedZl9vc/Vl3P53gCSdfAN957Y5IbdC8QR2eubwPl/bvyN8mfc2Pn/yE9dsKyp9QRJImrrsuS7j7end/zN2/l6iARGqa7MwM7jjjMEb+8EjylmxgyMOTmL1yc6rDEpHQPiU6EfnWOb3a8tJVx1JY5AwdPZmx01eWP5GIVDklOpEEOrJdE16/YQBHtGnMjc9/zj1vzaGoWC9zFUklJTqRBGvZsA7PXtGXC/u257H3F3HJU5+wcbuu24mkihKdSBXIycrgD2cdwZ+GHsHURd9w5qjJzFu9JdVhidRKSnQiVWj4Me15YcSx7Cgo4uxHJvPWzFWpDkmk1lGiE6livTo05fUbBnDwfg255tnPGDluHsW6bieSNEp0IknQulEuL4zoy7m92/HwhIVc8XQem3bsTnVYIrWCEp1IktTJyuRPw47grrMO54P5+Zw9ajIL1+q6nUhVU6ITSSIz46K+HXjuyr5s3rmbs0ZN4d3Za1IdlkhaU6ITSYFjOjVj7PUD6NyyPlc+nccD/5uv63YiVUSJTiRFDmhSlxevOpZhR7Xlgf8t4Kp/TmPLTl23E0k0JTqRFMrNzmTkD7tzxxndeG/uWs5+ZAqL8remOiyRtKJEJ5JiZsal/TvxzOXHsH5bAWeOmsyEuWtTHZZI2jD39Lwu0Lt3b8/Ly0t1GCKVsnzDdq56ZhqzV23mppMO5tpBXTDT+433VWFRMVt2FrJlZyGbd+4OPjsK2bJzN1t2FrK7qJihR7WlZcM6qQ415cxsmrv3TnUciZSV6gBE5Fttm9bj5av7ceurM7hv3DxmrdzEfeccSf06tfdQdXe2FxSxOUxKW8IkFSSsb7u37NW9e6/Etr2gqNzlPDN1CX+/9BgObNUgCWslyVR7jx6RaqpuTiYPnNuDww9ozD1vzeGrtdsY8+NedGheP9WhxaWgsHhPyakkWZUkopJk9W1i2v3tODu/TVblvQEiO9NolJtNw9wsGtUN/rZulBt052bTMDebRnWzaFgyTvi3cTju1+u2ceXTeQwbPYUxF/WiT+fmSdo6kgyquhSpxj5ckM/1z30OwEPn9eT4g1omdfnFxc62gsIYJaeIEtOOMFntlcS+LV3t3F1c7nIik090worubpibTaOSv3WD4XWyMva5infZ+u1c8tQnLFu/g5E/OpIhRx6wT/OrqdKx6lKJTqSaW/rNdkY8k8f8NVu49ZRDuPK4zhU+qe/cXRRVOvpusoouXUUmqy27CinvFFEnK+M7SahRRAmqJCnFTGJ1s2mQk0VGRvW4DrlxewEjnp7GJ4vXc9sphzDi+Ipv63ShRFeDKNFJOtleUMjNL83gvzNXcVr3/Tm6Q9M9JaayqgQLCssuTWUYpSehkqS1VxKLLl1lUScrM0lbITl27i7ippem88aMVVzUtwN3nNGNrMzac4N6OiY6XaMTqQHq5WTx8Pk9Oez9RowcN4//zlgV9s/cqyTVpF4O7ZvXj6oKjK4G/La0VT8ns9aVWMqTm53JX4f3pE3Tujz2/iJWbdrBX8/rSb0cnS5rKpXoRGqYjdsLcIcGuVlk16KSRio889Fi7hg7i8PbNOZvFx9dK5ofpGOJTkeJSA3TpF4OTevnKMklwUXHdmTMRb1ZsGYrZz8ymYVr9dSamihpR4qZnWxm88xsoZndGmN4BzMbb2YzzGyimbWNGPa2mW00szeSFa+ICMD3u7XmhRF92bm7iGGjp/Dp4vWpDkkqKSmJzswygVHAKUA34Dwz6xY12kjgaXfvDtwJ3BMx7D7gomTEKiIS7ch2TXjt2v40b5DDBU98zBszVqY6JKmEZJXojgEWuvsidy8AXgDOjBqnGzA+/D4hcri7jwf0hkoRSZl2zerxytX9OLJtY65/7nPGfPAV6XqPQ7pJVqJrAyyL6F4e9os0HRgWfj8baGhmlXo8gZmNMLM8M8vLz8+PO1gRkVia1s/hmcv7cFr3/fnjm3O5Y+yscp/aIqmXrEQX6/7l6L3jJmCgmX0ODARWAIWVWYi7j3H33u7eu2XL5D5BQkRqh9zsTB4a3pOrju/M0x8t4apnprG9oFKnKkmyZCW65UC7iO62wF6V3O6+0t2HuntP4Paw36YkxSciUmEZGcZtpx7KnWcexntz13DemKnkb9mV6rCkFMlKdJ8CXc2sk5nlAMOBsZEjmFkLMyuJ5zbgySTFJiISlx8f25HHLurNvDVbGDp6Ml/ppbnVUlISnbsXAtcD44A5wIvuPsvM7jSzIeFog4B5ZjYfaA3cXTK9mX0IvAQMNrPlZvaDZMQtIlKeE7u15oURx7J9l5ofVFd6MoqISAIs/SZ4+8HyjTu4/0c9OK37/qkOKS56MoqIiMTUvnk9XrmmH93bNOa65z7j8Q8WqflBNaFEJyKSIE3r5/DPK/pw2hH7c/ebc/idmh9UC3oct4hIAuVmZ/LQecHbD8Z8sIiVm3by1+E9qZuTXq8zqklUohMRSbCMDONXpx7K74ccxv/mrGH441NZt1XND1JFiU5EpIpc3K8jj13Yi3mrNzP0kSksUvODlFCiExGpQicdth/PX9mXbbsKGTp6CnlqfpB0SnQiIlWsZ/umvHptP5rWy+H8Jz7mzZmrUh1SraJEJyKSBB2a1+eVa/pxRNj84IkP1fwgWZToRESSpFn9HJ69og+nHL4ff/jvHH7/+mw1P0gCJToRkSTKzc7k4fOO4ooBnfj7lMVc889p7CgoSnVYaU2JTkQkyTIyjF+f3o07zujGu3PWcJ6aH1QpJToRkRS5tH8nHr2wF3NWqflBVVKiExFJoR8cth/Pj+jL1l2FDBs9hWlL1Pwg0ZToRERS7Kj2TXn1mn40qZfDeY9/zFtqfpBQSnQiItVAxxZB84PDD2jEtWHzA0kMJToRkWqiWf0cnruyLycfVtL8QG8/SAQlOhGRaiQ3O5NR5x/F5QM68dTkxVz7rJof7CslOhGRaiYjw/jN6d347endeGf2Gs5/YirfqPlB3JToRESqqcsGdGL0Bb2YvXIzQ0dP4et121IdUo2kRCciUo2dfHjQ/GDLzkKGPjKZaUs2pDqkGkeJTkSkmitpftC4bjbnPz6Vt79U84PKUKITEakBSpofdDugEdc8+xl/m/R1qkOqMZToRERqiOYN6vD8lX05qVtr7npjtpofVJASnYhIDZKbnckjF/Tisv5B84Prnv2MnbvV/KAsSnQiIjVMZobx2zO68ZvTuzFu9mrOe1zND8qiRCciUkNdPqATj5x/FLNXbmbY6CksVvODmJToRERqsFOO2J/nruzLph27GTp6ipofxKBEJyJSw/Xq0JRXr+1Pw9wszn98KuPnrEl1SNWKEp2ISBro1KI+r17Tj76dm9OuWb1Uh1OtZKU6ABERSYzmDerwj8uOSXUY1U5SS3RmdrKZzTOzhWZ2a4zhHcxsvJnNMLOJZtY2YtjFZrYg/FyczLhFRKTmSlqiM7NMYBRwCtANOM/MukWNNhJ42t27A3cC94TTNgPuAPoAxwB3mFnTZMUuIiI1VzJLdMcAC919kbsXAC8AZ0aN0w0YH36fEDH8B8C77r7e3TcA7wInJyFmERGp4ZKZ6NoAyyK6l4f9Ik0HhoXfzwYamlnzCk6LmY0wszwzy8vPz09Y4CIiUnMlM9FZjH7RD2m7CRhoZp8DA4EVQGEFp8Xdx7h7b3fv3bJly32NV0RE0kAy77pcDrSL6G4LrIwcwd1XAkMBzKwBMMzdN5nZcmBQ1LQTqzJYERFJD8ks0X0KdDWzTmaWAwwHxkaOYGYtzKwkptuAJ8Pv44CTzKxpeBPKSWE/ERGRMiUt0bl7IXA9QYKaA7zo7rPM7E4zGxKONgiYZ2bzgdbA3eG064G7CJLlp8CdYT8REZEymXt6vsvIzPKBJXFM2gJYl+BwajJtj71pe+xN22Nv6bA9Orh7Wt3kkLaJLl5mlufuvVMdR3Wh7bE3bY+9aXvsTdujetKzLkVEJK0p0YmISFpTovuuMakOoJrR9tibtsfetD32pu1RDekanYiIpDWV6EREJK0p0YmISFpToguV96682sTM2pnZBDObY2azzOwnqY6pOjCzTDP73MzeSHUsqWZmTczsZTObG+4nx6Y6plQys5+Fx8qXZva8meWmOib5lhIdFX5XXm1SCPzC3Q8F+gLX1fLtUeInBE/1EXgQeNvdDwGOpBZvFzNrA9wI9Hb3w4FMgkccSjWhRBeoyLvyag13X+Xun4XftxCcxL7zWqTaJHzb/WnAE6mOJdXMrBFwPPA3AHcvcPeNqY0q5bKAumaWBdQj6oH1klpKdIEKve+uNjKzjkBP4OPURpJyDwC3AMWpDqQa6AzkA0+FVblPmFn9VAeVKu6+AhgJLAVWAZvc/Z3URiWRlOgCFXrfXW0TvirpFeCn7r451fGkipmdDqx192mpjqWayAKOAka7e09gG1Brr2uHb1Q5E+gEHADUN7MLUxuVRFKiC5T7rrzaxsyyCZLcs+7+aqrjSbH+wBAzW0xQrf09M/tnakNKqeXAcncvKeW/TJD4aqvvA1+7e7677wZeBfqlOCaJoEQXKPddebWJmRnB9Zc57v5/qY4n1dz9Nndv6+4dCfaN99y91v5id/fVwDIzOzjsNRiYncKQUm0p0NfM6oXHzmBq8c051VEy3zBebbl7oZmVvCsvE3jS3WelOKxU6g9cBMw0sy/Cfr9y9zdTGJNULzcAz4Y/DBcBl6Y4npRx94/N7GXgM4I7lj9HjwKrVvQIMBERSWuquhQRkbSmRCciImlNiU5ERNKaEp2IiKQ1JToREUlrSnQiCWJmRWb2RfgE+9fNrEk54zcxs2uTFZ9IbaVEJ5I4O9y9R/gE+/XAdeWM3wRQohOpYkp0IlXjI8IHg5tZAzMbb2afmdlMMyt5M8afgC5hKfC+cNybzexTM5thZr9PUewiaUVPRhFJsPD9hoMJX2MD7ATOdvfNZtYCmGpmYwkehHy4u/cIpzsJ6Erw2igDxprZ8e7+QdJXQiSNKNGJJE7d8JFpHYFpwLthfwP+aGbHE7zmpw3QOsb0J4Wfz8PuBgSJT4lOZB8o0Ykkzg5372FmjYE3CK7R/RW4AGgJ9HL33eFbEHJjTG/APe7+WLICFqkNdI1OJMHcfRNwI3BT+LqjxgTvs9ttZicAHcJRtwANIyYdB1wWvgcQM2tjZq2SGLpIWlKJTqQKuPvnZjad4LU+zwKvm1ke8AUwNxznGzObbGZfAm+5+81mdijwUfC2F7YCFwJrU7ISImlCby8QEZG0pqpLERFJa0p0IiKS1pToREQkrSnRiYhIWlOiExGRtKZEJyIiaU2JTkRE0tr/A+HczIwdm/FFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test1\n",
    "#Restnet20 on full House numbers train dataset without data augmentation\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = prepare_house_numbers_data(subtract_pixel_mean=True)\n",
    "teacher, init, model_type = choose_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "student = choose_noised_resnet_model(input_shape=x_train.shape[1:], n=3,version=1)\n",
    "accuracy_supervised, teacher_path = train_model(init, teacher, x_train, y_train, x_test, y_test, batch_size=32, num_classes=10, epochs=100, \n",
    "                                  data_augmentation=False, model_name= 'keras_housenumbers_trained_teacher_nda.h5')\n",
    "\n",
    "rate9=np.array([0.05,0.1,0.2,0.5,0.75,1,2.5,5,7.5,9])\n",
    "accuracy_9=stns_full_dataset(x_train,y_train,x_test,y_test,rate9,teacher, student,teacher_path,accuracy_supervised, data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
